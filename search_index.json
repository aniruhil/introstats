[["index.html", "Data Analysis for Leadership &amp; Public Affairs: An Introduction Chapter 1 Preface 1.1 Data Analysis and Public Affairs 1.2 The chapters that follow 1.3 Keys to Learning Data Analysis 1.4 Acknowledgements", " Data Analysis for Leadership &amp; Public Affairs: An Introduction Anirudh V. S. Ruhil Updated 2022-10-01 Chapter 1 Preface As a typical graduate student with the usual financial woes I would struggle to find ways to get my hands on the textbooks I needed for my courses without paying full cost. Old editions, used copies, photocopies, Interlibrary Loan services, you name it, I tried it. Once I became an instructor, a different problem surfaced: The annual hunt for a text that covered the material I needed to teach in a clear and thorough manner, albeit, at a reasonable price. Unfortunately, statistics textbooks written for public administration/public affairs students are not only few and far in between but also consistently expensive. This has been an exasperating situation for over two decades with no signs of a correction in the offing. It was thus less serendipity and more growing frustration with the state of affairs that led to the creation of this text. Given that in this age of crushing student debt the world is replete with open-source software for data analysis and desktop publishing, why not curate a textbook tailored not only to my instructional needs but also to my students’ pocketbooks? Leaning on the wonderful resources created by the teams at RStudio, R, Jared Lander (his wonderful “R for Everyone: Advanced Analytics and Graphics” showed me what a quality, aesthetically pleasing product might resemble), and the thousands of R users across the globe ever willing to share their coding knowledge, I have assembled my course material into a work that I hope is useful beyond just the pocketbook. My goal is to continually improve the quality and effectiveness of this book but that cannot be achieved without your suggestions for substantive, technical and stylistic improvements, so feel free to share these with me. 1.1 Data Analysis and Public Affairs In the early decades of the Twentieth century the field of public administration was a leader, as much in the area of intellectual thinking about governance as in granular research on governmental processes and outcomes. Since the 1960s, however, public administration has been seen as a poorer cousin of political science, once its twin, public administration’s cache dwindling in the social and behavioral sciences, largely because of the feeling that the scholarship it produces is less rigorous than that of other disciplines. There may be some truth to this notion but engaging in intellectual debates and recovering scholarly bragging rights are not our goals. Rather, the task before us to fill a more crucial gap in the public affairs world – preparing data savvy graduates. Not an easy task you say? Truer words were never spoken! Why is that? Most of you, like thousands of your peers across the nation, no doubt groaned when you saw that one of your required courses was a research methods class. Maybe you remembered the mandatory statistics course you took as an undergraduate and that familiar dread swept over you in a flash. A false fear of mathematics certainly gets in the way of understanding statistics. Beyond that, however, is the matter of understanding statistical concepts by seeing these concepts in action, something you probably bypassed as an undergraduate. And then, of course, is the all important learning exercise of applying these concepts to real-world data. Well, in this text I try to get you beyond your fear of numbers, help you see statistics in action, and force you to work with real-world data married to real-world questions. In other words, this is a very applied course in data analysis, one in which you will learn about how to use data in a way best suited to answer a specific question. Maybe the question is about weighing the evidence in a racial bias in hiring lawsuit your city is facing. Or your agency is curious to know if the public information campaign it has been working on and broadcasting to promote healthy living is having any (or no) impact on the health of the citizens. Maybe you work for a an economic development agency and need to track trends in unemployment rates. Whatever the question before us, invariably there are data that can be used to find reasonable answers. To get at these answers, however, you need to know three things: How do I gather the data? How should I analyze the data I gathered? What are the strengths and limitations of my analysis? These will be our guiding questions throughout this book. Seems fairly easy but it does require hard work that involves a lot of hands-on practice. After all, data analysis requires us to understand the syntax of statistics, its vocabulary, and in all cases, a crowd of Greek symbols squeezed into what seems to be a mysterious mathematical formula. 1.2 The chapters that follow The chapters that follow are sequenced in a way designed to promote learning. We start in Chapter 2 with the fundamentals, learning about samples and how they differ from populations, how the different ways we measure some attribute or phenomenon (for example, how the way we measure a survey respondent’s sex differs from how we measure hours of physical activity she/he spends per week) has implications for the analysis that can be done, the tricky business of establishing cause-and-effect, and other such fundamental principles. In Chapter 3 we move on to understanding the many interesting ways in which we can explore patterns in our data, both graphically and with simple tables. In the process we will also learn best practices in data visualization – how should you build an effective graph? Issues of human cognition and visual perception are more important than we tend to recognize. Chapter 4 revolves around measures of central tendency and variability. That is, we learn about the three ways we can measure and discuss the average, the typical – The mean, the median, and the mode – and variability around these averages – the range, the interquartile range, the variance, and the standard deviation. We also fold into our data visualization toolbox a very powerful graphic called the box-plot that relies on the five-number summary to tell us a lot about the shape of our distribution. Probability theory, one of the most notorious subjects in statistics, is our focus in Chapter 5. It is a tricky subject but unless we wrestle with the nuances of probability theory everything else that follows will make little to no sense. It is as powerful as it is difficult, but that is why game shows rely on it and Jeopardy champions and poker stars need to master it. It is also the source for understanding why such events as the winning pick-3 number in NYC on the first anniversary of the 9/11 terrorist attacks being 9-1-1 was not a rare event. If you understand probability you can be the life of the party by correctly predicting that at least two people in a gathering of 30 share the same birthday. Chapter 6 extends probability theory to probability distributions, both discrete and continuous, and Chapter 7 spans the theory of sampling distributions. Stalwarts like the standard error, the Central Limit Theorem, confidence intervals, and *the Student’s \\(t\\) distribution grace the stage. In Chapter 8 we discuss the logic of of hypothesis testing, a method of formalizing and testing our suspicions about whether a program has had an impact, whether there is a gender bias in hiring, etc. This is a tightly specified method, with little room for doing things differently. One of the biggest surprise many students encounter occurs in this chapter; no matter the strength of the evidence there is always a possibility that we could be drawing the wrong conclusion from our data analysis. This chapter also introduces you to some hypothesis tests commonly refered to as t tests or difference of means tests. Chapters 9 and 10 lead us farther through the world of inferential statistics, the process of analyzing the sample data in hand and extrapolating our conclusions to the population the sample represents. We start quite simply, looking at how to determine if the differences between two groups are statistically significant. Since often the world cannot be broken up into two groups (men and women, for example) but instead must be studied as it exists in reality (for example, the fact that the Ohio Department of Education classifies public school districts into eight mutually exclusive categories), we also learn how to analyze categorical outcomes for multiple groups in a coherent manner. In Chapter 11 we move on to the mother of most statistical analyses today – regression analysis. This is the most exciting and useful portion of the course because we are finally able to accommodate real world complexity in our calculations. For example, if you wanted to predict the number of highway fatalities on a particular stretch of I70, you would have to account for many things (visibility, traffic density, traffic speed, road conditions, driver intoxication, time of day, and so on). Indeed, much of the hubub about predictive analytics, health analytics, data mining, etc. revolves around one form or another of regression analysis. 1.3 Keys to Learning Data Analysis I have already emphasized that statistics has its own language, and in that sense learning statistics is no different from learning a foreign language. You cannot master a foreign language simply by cracking open a book for an hour a week or going to the weekly class. If it were that simple we would all be linguists, but we are not. The ones who master a foreign language (or at least learn enough to impress the servers at their neighborhood Spanish/Italian/French restaurant) are those who practice as much as they can. That is the approach I recommend to you if you want to learn data analysis. To encourage practice I have included a number of practice problems that conclude each chapter. These are designed to reinforce learning and I expect you to try and solve them. Answer keys are provided with fully worked-out calculations so that if you do make a mistake you can see where and how you went wrong. You should do a few problems before you tackle the assignment for the week; this puts you in the best position to not only complete the assignment correctly but also to do so with minimal frustration and time. Each chapter also has a few worked examples per key concept/calculation so there is no shortage of learning-by-doing opportunities. 1.4 Acknowledgements I want to acknowledge the following students for providing critical feedback: Viktoria Viktorova Marinova, Jody G. Dzuranin, Chelsa Morahan, Grace Kroeger, Eleonora Mocanu, Sharif Wahab, George Mance, Bethany Blinsky, Shayne Mae R. Lopez, Ross Bay, Marla Gaskill, Audrey Parker, Sheila Easter, Theresa Meyer, Ebrima Bandeh, and Mohammad Sakhi Hassany, Lyda Ngin, Abigail Wilson, Amanda White, and Madeline Yeatts. Jody G. Dzuranin deserves special mention for her meticulous proof-reading of the text; she has earned the sobriquet of Eagle-eyed Jody. Update: Junia (Dya) Purwandani just pipped Jody and now owns that title: Bermata Elang Dya. "],["intro.html", "Chapter 2 Introduction to Core Concepts 2.1 Statistics versus Statistic 2.2 Useful Research Designs 2.3 Elements of a Data-set 2.4 Variable Types 2.5 Cross-sectional, Time-Series, and Panel Data 2.6 Levels of Measurement 2.7 The Tricky Business of Cause-and-Effect 2.8 Chapter 2 Practice Problems", " Chapter 2 Introduction to Core Concepts 2.1 Statistics versus Statistic Statistics is a broad and lively field of study that involves methods to (a) accurately describe patterns observed in data, and (b) to draw inferences about the population the data were gathered from. Thus, for example, the pollsters who track public opinion polls at Gallup ask at regular intervals: “Do you approve or disapprove of the way [insert incumbent President’s name here] is handling his job as president?” Obviously it would be unfeasible to ask every adult in the United States of America this question. Instead, based on statistical theory and meticulous calculations that you will come to learn, Gallup works as follows: Gallup interviews [approximately 500] U.S. adults aged 18 and older living in all 50 states and the District of Columbia using a dual-frame design, which includes both landline and cellphone numbers. Gallup samples landline and cellphone numbers using random-digit-dial methods. Gallup chooses landline respondents at random within each household based on which member had the next birthday. Each sample of national adults includes a minimum quota of 60% cellphone respondents and 40% landline respondents, with additional minimum quotas by time zone within region. Gallup conducts interviews in Spanish for respondents who are primarily Spanish-speaking. Source What Gallup is working with is a subset of all U.S. adults aged 18 and older and living in the 50 states plus Washington DC. As they gather the data, they may be doing so to study the trend in presidential approval (see graph below). FIGURE 2.1: Presidential Approval over Time Why does Gallup do these surveys of a few hundred or a thousand individuals? Not because they are idly curious about what a sample of 500 U.S. adults think. Gallup wants to be able to say this is what the U.S. adult population thinks about the incumbent president’s job performance! In other words, Gallup is using a sample to say something useful about the population. Sample: the subset of cases drawn for analysis from the population Population: the universe (or set) of all elements of interest in a particular study Drawing conclusions about the population from analyzing the sample is the process of making an inference, and the processes and rules involved in doing so are called inferential statistics. It will rarely be the case that you will have access to the entire population. The rare instances when that might happen could include cases where you have access to all patients receiving care in the hospital group you work for, all individuals insured with your insurance agency, all individuals receiving services from your agency, all drivers registered with the department of motor vehicles, all homeowners in a specific location, and so on. Even in these instances, however, you can only access the population if the law and agency protocols permit you to do so and have legitimate, authorized reasons for accessing the population data. Some population data are in the public domain and easily accessed: For example, data about all public school districts in Ohio, data on all voters registered to vote in a particular location, data for the nation’s cities, counties, states, territories, congressional districts, census tracts, and so on. These data are publicly available because there are no legal restrictions on releasing these data. Most of us, however, end up relying on samples. If statistics is a field of study, a statistic is the result of applying a computational algorithm to a set of data. For example, when the U.S. Census Bureau calculates Median household income, they are generating a statistic. Likewise, when the Gallup polls and tells us what percent of likely voters will vote for Candidate A, they too are generating a statistic. If you are working with a pilot group of your agency’s service recipients and the pilot is testing whether a particular policy or service program is having the intended effect (for example, improving the service recipients’ financial literacy), and you conclude that 10% of those who receive this training will improve their financial literacy, you have generated a “statistic”. 2.1.1 Parameters versus Estimates Statistics revolves around drawing as accurate as possible estimates of some unknown attribute of the population from the sample at hand. These unknown population attributes/features are called parameters while their sample counterparts are called estimates. Thus, for example, if I had access to the household income of every household in Franklin County, Ohio, I could calculate average household income, and this would be a parameter. However, if I could only get my hands on incomes of say 100 households in Franklin County, Ohio, randomly selected for analysis, and then I calculate average household income for these 100 households, I would have an estimate. In general, we hope (and ensure as best as we can via statistical theory) that our sample estimates equal the population parameters they represent. This equality may not always hold and so every time we generate an estimate we also generate a measure of the uncertainty around the estimate, a measure that tells us how much our estimates may have drifted from their corresponding parameters. We will make these measures of uncertainty more precise as we go along but for now you can cast your mind to a phrase you may have encountered – the “margin of error”. For example, on October 20, 2017 a Gallup poll found that Americans’ approval of the way Congress was doing its job had hit 13% (i.e., only 13% thought Congress was doing a decent job while 87% thought otherwise). At the very bottom of this story Gallup noted that “[f]or results based on the total sample of national adults, the margin of sampling error is \\(\\pm 4\\) percentage points at the 95% confidence level”. We will learn all about this quantity called the margin of error but for now understand the essence of what that \\(\\pm 4\\) is telling us: If we could have asked every adult American whether they approved of Congress, the percent likely to say “yes” could have ranged between \\(13 - 4 = 9\\%\\) to \\(13 + 4 = 17\\%\\). So this margin of error is telling us something about the uncertainty around the estimate of 13%. If no such uncertainty estimate were provided to us, we would have no way of knowing how much faith to place in anything the sample is telling us. Estimates are quantities of interest calculated from the sample whereas parameters are their population counterpart, for example, the proportion of women in the sample versus the proportion of women in the population. 2.1.2 Sampling Error and Bias Samples can mislead of course, and by sheer chance at that. When this happens our (sample) estimates will \\(\\neq\\) their corresponding (population) parameters, leading to sampling error. Sampling error is difficult to eradicate completely but can be minimized up to a point. What we should worry about far more than sampling error is bias. Bias can arise in many ways, with some of the more common sources of bias being measurement bias (you have a systematic error in how you are measuring something but don’t realize it); sampling bias (survey non-response and/or the way you are drawing your sample is generating bias); model misspecification (you do not fully understand the phenomenon you are studying and thus end up with a flawed statistical model that will yield biased estimates because of variables you forgot to include, mis-characterized relationships between variables, and so on). Measurement error is a particularly interesting feature of working with samples. For example, say I am working with a “healthy living” group that is working to help folks live healthier lives by eating better and exercising more. I am tasked with studying the program and then releasing a final report on how well (or poorly) the program worked. I could measure participants’ attendance at organized events, their online studying of food and exercise modules we are providing to them via the web and smart phones/tablets, and of course we have their height, weight, and maybe some other data recorded when they enrolled in the program. Unfortunately, however, we can only record their eating and exercise patterns based on what they tell us! People don’t keep very accurate logs of food intake and exercise, so often when they are completing surveys they are doing their best to recall these behaviors over the last week. In recalling the recent past, there will be some mistakes, that is to be expected. Some may overestimate how much they ate and/or exercised while others will underestimate how much they ate and/or exercised. If these overestimates and underestimates were purely random, then they would cancel each other out and we would have no measurement error. However, we do know that self-reported dietary intake measures are not random and hence could generate bias. 2.1.3 Key Elements of Random Sampling At minimum two things must hold for a random sample … Every unit of analysis (i.e., the entity that is to be analyzed) in the population must have the same chance of being selected into the sample, and Every unit of analysis in the population must be sampled independently of all other units of analysis If you violate (1) you end up with biased estimates and if you violate (2) you will have imprecise estimates. Examples of these violations are aplenty. For instance, imagine I carry out a landline survey of health behaviors and outcomes. How many of you have a landline? Not many I would suspect. So what is the consequence of my using a landline survey? All cellphone-only households have a zero chance of being selected into my sample. If the cellphone-only population were no different from the population that has kept a landline there would be no problem. But we know that isn’t the case. If you are curious, see New Considerations for Survey Researchers When Planning and Conducting RDD Telephone Surveys in the U.S. With Respondents Reached via Cell Phone Numbers and Growing Cell-Phone Population and Noncoverage Bias in Traditional Random Digit Dial Telephone Health Surveys. I could also violate (2) if I am conducting a survey of Freshman’s opinions about healthy food options in a university’s dining halls and end up talking only to people seated together, even if I pick a few tables to talk to. Why is this a problem? Because it is likely that these students are friends/roommates who maybe also share a common major, etc., and most likely come from similar socioeconomic and demographic backgrounds, have shared tastes and preferences, and so on. Biased estimates will result if every unit in the population does not have the same chance of being drawn into the sample. Imprecise estimates will result if the units are not sampled independently of each other. 2.2 Useful Research Designs All research starts with a research design, a well-crafted and clearly articulated framework that outlines what data will be gathered in order to address the research questions motivating the research. There are several research designs that could be used, each with their own strengths and weaknesses but the key quantitative designs you are likely to encounter are listed below. 2.2.1 Experimental Experiments are the gold standard of quantitative research designs because, much like an experiment in a physics of chemistry lab, they allow the researcher to control for a host of factors and then introduce a stimulus to test the impact of this stimulus on the outcome of interest. For example, political psychologists who study the impacts of campaign advertising on voters’ choices often expose a randomly chosen group of subjects either to a positive campaign advertisements or to a negative campaign advertisements about two candidates running for the same elected office. The study’s purpose might be to figure out whether negative advertising reinforces stereotypes, whether it makes people less likely to vote at all (even for their own party’s candidate), and so on. Because the two groups are similar in every possible way and were randomly assigned to either of the campaign advertisement types, any impacts we see of the tone of the campaign advertisement can be reliably attributed to the advertisement itself and not to someone’s sex or educational attainment or party preferences, etc. Take another example. Say I am interested in figuring out whether private schools are really better at educating students than traditional public schools. Ideally, I would have a large random sample of genetically identical twins and the freedom to assign one twin in each pair to a public school and the other to a private school. I could then follow their learning for a year and see which group – private or public – has better academic performance on average. Why would this be a powerful research design? Because they are twins, their home environments are the same (same parental income, interest in child’s education, social circle, access to computers, internet, extra-curricular activities, and the like). An informed reader would at this point object to this example and point out that identical twins’ genes are not identical so couldn’t that be responsible for differential learning? Yes, it might but that is why we restricted our sample to genetically identical twins! Experiments draw their power from their ability to rule out all other likely influences on the outcome and in doing so yield a clean estimate of the treatment’s (say, attending a private versus a public school, being given Drug A versus Drug B, etc.) impact on an outcome (academic learning, reducing joint inflammation, and so on). However, they are difficult to carry out in most social and behavioral science settings in the real world because they cost a lot, logistics get in the way (you probably won’t get enough parents to agree to separate the twins) and because of legal/ethical requirements that circumscribe all research with human subjects. Central to these requirements is the notion of informed consent. Informed consent is the process by which researchers working with human participants describe their research project and obtain the subjects’ consent to participate in the research based on the subjects’ understanding of the project’s methods and goals. The subject must have understood his/her rights, the purpose of the study, the procedures to be undergone, and the potential risks and benefits of participation. It is strictly voluntary and consent must be secured for all human subjects research including diagnostic, therapeutic, interventional, social and behavioral studies, and for research conducted domestically or abroad. Vulnerable populations (i.e. prisoners, children, pregnant women, etc.) receive extra protections, and the legal rights of subjects are neither waived nor are the researchers, the study’s sponsor, the researchers’ employer, etc. exempt from any liability for negligence. There is a fascinatingly unfortunate history that has led to this sacrosanct principle of informed consent, even though some would argue it holds back what we could learn; for the curious among you, see Seven Creepy Experiments That Could Teach Us So Much (If They Weren’t So Wrong). Indeed, informed consent is such a crucial aspect of our work that your first assignment will require you to complete the Group 2: Social and Behavioral Investigators and Key Personnel training via Collaborative Institutional Training Initiative (CITI Program). 2.2.2 Natural Experiments The fundamental reason that experimental research sets the bar for all other research designs is because of the researchers’ ability to completely randomize what treatment a subject encounters. This ensures that there is very likely no other explanation for differences in outcomes other than the differences in the treatments themselves. Natural experiments come close to mimicking this complete randomization attribute of experimental designs although the randomization is not being carried out by the researchers. Here is a classic example that comes to us from Thad Dunning: An interesting social-scientific example comes from a study of how land titles influence the socio-economic development of poor communities. In 1981, urban squatters organized by the Catholic Church in Argentina occupied open land in the province of Buenos Aires, dividing the land into parcels that were allocated to individual families. A 1984 law, adopted after the return to democracy in 1983, expropriated this land with the intention of transferring titles to the squatters. However, some of the original landowners challenged the expropriation in court, leading to long delays in the transfer of titles to some of the squatters. By contrast, for other squatters, titles were granted immediately. The legal action therefore created a (treatment) group of squatters to whom titles were granted promptly and a (control) group to whom titles were not granted. The authors of the study find subsequent differences across the two groups in standard social development indicators: average housing investment, household structure, and educational attainment of children. On the other hand, the authors do not find a difference in access to credit markets, which contradicts a well-known theory that the poor will use titled property to collateralize debt. They also find a positive effect of property rights on self-perceptions of individual efficacy. For instance, squatters who were granted land titles—for reasons over which they apparently had no control— disproportionately agreed with statements that people get ahead in life due to hard work. Is this a valid natural experiment? The key claim is that land titles were assigned to the squatters as-if at random, and the authors present various kinds of evidence to support this assertion. In 1981, for example, the eventual expropriation of land by the state and the transfer of titles to squatters could not have been predicted. Moreover, there was little basis for successful prediction by squatters or the Catholic Church organizers of which particular parcels would eventually have their titles transferred in 1984. Titled and untitled parcels sat side-by-side in the occupied area, and the parcels had similar characteristics, such as distance from polluted creeks. The authors also show that the squatters’ characteristics, such as age and sex, were statistically unrelated to whether they received titles—as should be the case if titles were assigned at random. Finally, the government offered equivalent compensation—based on the size of the lot—to the original owners in both groups, suggesting that the value of the parcels does not explain which owners challenged expropriation and which did not. On the basis of extensive interviews and other qualitative fieldwork, the authors argue convincingly that idiosyncratic factors explain some owners’ decisions to challenge expropriation, and that these factors were unrelated to the characteristics of squatters or their parcels. The authors thus present compelling evidence for the equivalence of treated and untreated units. Along with qualitative evidence on the process by which the squatting took place, this evidence helps bolster the assertion that assignment is as-if random. Of course, assignment was not randomized, so the possibility of unobserved confounders cannot be entirely ruled out. Yet the argument for as-good-as-random assignment appears compelling. Two more examples follow, both from the following blog that also lists several other examples The Dutch Famine Study The Dutch “hunger winter” took place towards the end of World War 2 in the German-occupied Netherlands from late 1944 until the liberation of the area by the Allies in May 1945. During this time official food rations dropped to as low as 500 calories per day and around 20,000 people died of starvation. This experience of a modern advanced country experiencing a short, sharp famine is quite unusual, and in this case the West of the country was strongly affected but not the North or South. This has allowed many researchers to examine the effects of the famine on population health by comparing the outcomes of people from different regions. Of particular interest are the outcomes of the children who were in utero at the time of the famine, as they may have suffered developmental impairments with potentially lifelong repercussions. The Impact of the Mariel Boatlift on the Miami Labor Market While a majority of economists agree that the average American citizen would be better off if more low-skilled and high-skilled immigrants were allowed to enter the US each year, there is considerably more opposition to immigration among the general public in Britain, America and continental Europe. One frequently raised concern, aside from issues of social cohesion, is that the arrival of immigrants might negatively disrupt the economy and make it more difficult for local workers to secure good jobs. David Card examined this issue by looking at the labor market effects of the “Mariel boatlift” (wiki), the name given to the arrival of 125,000 Cubans into Florida during April - September 1980 (this is the impetus for Tony Montana’s move to America at the start of Scarface). This sudden arrival of relatively unskilled young men increased the size of the Miami labor force by 7%. By comparing Miami with other cities which were not affected by the Mariel boatlift, Card showed that the arrival of the Mariel workers did not notably affect the wages and employment rates of the existing unskilled workers living in Miami. In a nutshell, it is as if some naturally occurring phenomenon or event led to an “as-if” randomization of units into different groups, and the resulting groupings can then be studied for differences in the outcomes of interest. Some interesting and recent examples of research utilizing natural experiments include work around Hurricane Katrina and the 9/11 terror attacks. 2.2.3 Quasi-Experiments More often than not researchers can neither carry out an experiment nor access a natural experiment. For instance, consider schooling in America. Some parents choose to send their children to a private school, others send their children to a parochial school, some may place their children in a charter school, but most will send their children to a public school in their district. Suppose I am interested in figuring out whether a child’s academic performance tends to be better in one school type than in another. I have not randomized children across school types, and most likely parental and other attributes (wealth, opinions, school alternatives available, child’s current performance, the public school’s performance, and so on) have led parents to select a school for their child. This sort of a deterministic sorting that constructs the different groups of school type is what we call non-random assignment. If I really want to pursue my study, I would have to control for as many of these differences, as many of these attributes as possible if I want to have a reasonable chance of learning how school type influences learning. I might draw as large a random sample as I can and then use appropriate statistical techniques to see how learning differs by school type; I have a quasi-experimental research design. Take another example. Say a city decides to replace some traffic lights at major intersections with traffic circles (aka roundabouts) because they believe traffic circles will reduce accidents that otherwise occur at these intersections. Researchers could judge whether traffic circles have had any impact by gathering and analyzing data from the pre-traffic circle period to the post-traffic circle period. If traffic circles have had an impact, on average we should see fewer accidents in the post-traffic circle period. Note that in both these examples the researcher has had no influence; she/he could neither determine where or when the traffic circle is constructed nor ensure that the drivers using these roads are the same. After all, it is quite likely that some drivers who otherwise took these routes are now avoiding these intersections because they see them as more dangerous or are uncomfortable navigating traffic circles. Consequently, anybody analyzing these data would have to control for as many factors as possible – traffic density, traffic composition (for e.g., are commercial trucks still using these intersections or not), average speed approaching the intersection, and so on. Unless one could do so, it would be impossible to say with any degree of certainty that any reduction in traffic is due to the traffic circle. Experiments are the gold standard for research because you can randomize units into groups, for example, a random subset of kids to a private school and the remainder to a public school, and then seeing who has a better academic performance at the end of a year or two or 10. In quasi-experiments you cannot randomize and hence have to do a lot more work to get around the limitations imposed by the possibility that the kids who go to a private school are fundamentally different from those who go to a public school (perhaps these kids are poorer, racial/ethnic minorities, and so on). 2.3 Elements of a Data-set While data are the facts and figures collected, analyzed and summarized, a data-set comprises all data collected in the course of a particular study. Take, for instance, one of the many data-sets compiled by the Appalachian Regional Commission1. A snippet of this data-set is shown in Table 2.1. TABLE 2.1: A Typical Dataset County State Unemployment % Percapita Income Poverty % Blount County Alabama 6.5 23440 17.3 Calhoun County Alabama 8.4 23420 21.7 Chambers County Alabama 8.2 21494 23.9 Cherokee County Alabama 6.8 22310 21.0 In the language of data analysis you might hear folks talk about observations and variables. By observations they mean the units that make up each specific row of the data-set. Here we have one county per row so the counties are the observations, the units. If we were surveying adults then each adult would be a unit. A variable is an attribute that differs across the observations. Here we have four variables – the state a county is in, unemployment rate, percapita income, and poverty rate. If some attribute does not differ across the observations, then it is really not a variable, is it? Since it does not vary it provides no useful information for studying any outcomes. For example, if in a sample gathered to study adult women’s reproductive health decisions we included a column called sex but the entry for each woman in the sample read “Female”, this would be wasted space. You know all the units are women so you cannot ask if some behavior or choice varies by the individual’s sex; sex is a constant in this study, not a variable. 2.4 Variable Types When you are staring at a data-set you are likely to see different types of variables. Let us see this variety in the context of a specific example, that displayed in Table 2.2 below. TABLE 2.2: Proficiency Data for Ohio’s School Districts dirn district grade subject Advanced Plus Advanced 45187 Ada Exempted Village 3rd Grade Reading NA 30.5 45187 Ada Exempted Village 3rd Grade Mathematics NA 37.3 61903 Adams County Ohio Valley Local 3rd Grade Reading NA 18.0 61903 Adams County Ohio Valley Local 3rd Grade Mathematics NA 24.8 49494 Adena Local 3rd Grade Reading NA 15.3 49494 Adena Local 3rd Grade Mathematics NA 25.0 43489 Akron City 3rd Grade Reading NA 11.6 43489 Akron City 3rd Grade Mathematics 0.1 14.6 45906 Alexander Local 3rd Grade Reading NA 18.9 45906 Alexander Local 3rd Grade Mathematics NA 14.3 You see a variable called dirn – a unique identifier for each school district. This is a numeric variable but the actual numbers don’t mean anything other than identifying a particular district. Your social security number is another such numeric variable. In contrast, the columns labeled “Advanced Plus” and “Advanced” contain true numeric values that represent the percent of students deemed Advanced Plus and the percent of students deemed Advanced, respectively. These numbers mean something. In that sense we can refer to these two proficiency levels as a quantitative variable. Other examples might be median household income, number of highway fatalities on Interstate 95, number of arrests, your height, weight, age, and so on. The variable district is a string (also called a character) variable in that contains only text, the name of each school district. The other two variables contain a combination of numbers and text, for example, “3rd Grade”, (grade) and just text, for example, “Reading” (subject ). These two variables and dirn are usually referred to as factors or categorical variables that identify categories. If we wanted to recognize broad data types we might as well realize that we can either slot a variable as a true numeric/quantitative variable or then as a qualitative/categorical variable. Note too that Table 2.2 shows some cells with the string “NA”, an indicator for missing values; for some reason we do not have information on the percent of students Advanced Plus in a particular subject for some districts but this information is available for other districts (for instance, Akron City, 3rd Grade, Mathematics). It is not uncommon to have missing data and when you do, the data-set will either show missing values with “NA”, “.”, “-9999” or some such entry2. Data analysts often speak in terms of dependent and independent variables, or then in terms of response and explanatory variables. When they speak in these terms what they are referring to is the fact that the dependent/response variable is the outcome of interest, and that they believe this outcome can be explained/predicted by the explanatory/independent variable. For example, I want to study literacy in early childhood so I gather a random sample of children who attend a nearby child development center, measure their literacy, then provide different amounts of materials designed to improve literacy to the sampled children, and at the end of three months I measure their literacy again. In this study, literacy level is the “dependent” variable while the amount of literacy material received is the “independent” variable; how literate a child is can be explained/predicted by how much literacy development material was provided to a child. Dependent or response variables are the outcomes of interest in a study. Independent or explanatory variables are the variables we believe influence the outcome. 2.5 Cross-sectional, Time-Series, and Panel Data These are three, very general types of data structures but by no means the only ones data analysts run into. However, these are the structures we are most likely to see and so a few words about each (with an example) are called for. 2.5.1 Cross-sectional Data Cross-sectional data refer to a structure where we have measurements taken at a single point in time. For example, the U.S. Census Bureau conducts a decennial census, the Census of Population and Housing every 10 years. The result is a snapshot of socioeconomic, demographic and other conditions at a single point in time. Other examples might be a one-time public opinion survey, a financial audit of all state agencies conducted in 2017, a study of all first-time enrollees in two/four year degree granting institutions in Fall 2016, and so on. The data shown in Table 2.1 and Table 2.2 are examples of cross-sectional data. 2.5.2 Time-Series Data Time-series data are measurements taken for a single unit over multiple time points. The Presidential Approval data we saw earlier (Figure 2.1) is a classic example of a time series; we are taking the pulse of a single unit (the nation) over multiple time periods. Other common examples would be the interest rate in the U.S., the value of the S&amp;P 500 measured daily for a given year, tracking your organization’s clients’ aggregate satisfaction by month/quarter/year. 2.5.3 Panel Data If you take multiple measurements over time for the same cross-sections you have what we call panel data. For example, I survey the same individuals or organizations every year. Or the Census Bureau’s American Community Survey measures a wealth of information for census states, counties, places, etc. annually, releasing the ACS 1-year data and the ACS 5-year data, respectively. There used to be a 3-year ACS data series as well but this was stopped because of budget cuts effective 2015. Read more about it here. Table 2.3 is an example of panel data gathered for 10 firms followed from 1935 to 1954. TABLE 2.3: An Example of Panel Data firm year inv value capital 1 1935 317.6 3078.5 2.8 1 1936 391.8 4661.7 52.6 1 1937 410.6 5387.1 156.9 1 1938 257.7 2792.2 209.2 1 1939 330.8 4313.2 203.4 1 1940 461.2 4643.9 207.2 1 1941 512.0 4551.2 255.2 1 1942 448.0 3244.1 303.7 1 1943 499.6 4053.7 264.1 1 1944 547.5 4379.3 201.6 1 1945 561.2 4840.9 265.0 1 1946 688.1 4900.9 402.2 1 1947 568.9 3526.5 761.5 1 1948 529.2 3254.7 922.4 1 1949 555.1 3700.2 1020.1 1 1950 642.9 3755.6 1099.0 1 1951 755.9 4833.0 1207.7 1 1952 891.2 4924.9 1430.5 1 1953 1304.4 6241.7 1777.3 1 1954 1486.7 5593.6 2226.3 2 1935 209.9 1362.4 53.8 2 1936 355.3 1807.1 50.5 2 1937 469.9 2676.3 118.1 2 1938 262.3 1801.9 260.2 2 1939 230.4 1957.3 312.7 2 1940 361.6 2202.9 254.2 2 1941 472.8 2380.5 261.4 2 1942 445.6 2168.6 298.7 Often people ask if one type of data is best and the answer is yes. Think about it this way. If I want to understand public opinion on some issue, I could just run a survey once. All this would tell me is what people think at that point in time. This tells me nothing about what I would have discovered about public opinion if I had run the survey the year before or waited for a year and then run the survey. If I aggregate these data and run the same survey every year, I would learn how the average person’s opinion varies over time. However, aggregation would lead to my ignoring subtle differences in the opinion of specific types of individuals (for example, minority groups, men versus women, those with less education versus the more educated, and so on). If, on the other hand, I survey the same individuals every year I can not only study how public opinion changes over time but also how different groups felt about an issue at a specific point in time, if and how people change their opinions, and so on. This is the ultimate strength of panel data – the ability both to study something at a given point in time and to study change over time. Many people think about cross-sectional data as a static photograph while panel data allow you to draw a moving picture. What data you can gather will be determined by your needs, the amount of money available for data collection, and time. Otherwise everybody would be gathering panel data and then tossing away portions of the data they don’t need. However, my experience has taught me to always gather more data than I need because it easier to set aside data you do not need than having to go back and gather data bypassed because you didn’t anticipate needing it. As with knowledge, so with data: More is always better than less! 2.6 Levels of Measurement When we set out to measure some aspect, some attribute or quality of an observation, we run into four commonly encountered mutually exclusive levels of measurement. 2.6.1 Nominal With the nominal level of measurement the best we can do is say this is school district x, that is district y, this student is male, that student is female, this person is White, that person is an Asian, and so on. This is the simplest level of measurement that distinguishes between observations in terms of some attribute but these differences have no hierarchy (i.e., we are unable to say Male is better than Female, Mathematics is above Reading, etc). We have three nominal variables in Table 2.2 – dirn, district and subject. Other examples of nominal variables include an individual’s sex, the color of your eyes, whether you are left-handed or right-handed, etc. 2.6.2 Ordinal With the ordinal level of measurement we are able to assign some hierarchy to the data as, for example, in the meaning that 5th Grade is above 4th Grade which is, in turn, above 3rd Grade (i.e., \\(5^{th} \\text{ Grade} &gt; 4^{th} \\text{ Grade} &gt; 3^{rd} \\text{ Grade}\\)). In Table 2.2 grade is an ordinal variable. Other examples would be a college student’s standing (Freshman/Sophomore/Junior/Senior), your opinion about gluten-free bread (Dislike/Like a Little/Like a Lot), income categories (Poor/Middle/Rich), and so on. 2.6.3 Interval/Ratio The interval/ratio level of measurement applies to numeric variables where the numerical values mean something. For instance, the percent of students at a specific proficiency level is a numeric variable. If I see Advanced proficiency values \\(10.2\\) for District A and \\(20.4\\) for District B then District B has twice as many students at Advanced proficiency levels than District A; this is a ratio. There is also a \\(10.2\\) percent gap in Advanced proficiency between District A and B; this is a statement of numerical difference. Both statements are true and possible, depending upon what we want to say. Notice too that it is possible for a District to have no student at Advanced proficiency; this would be the smallest value possible, a value of \\(0\\). However, what if we were measuring something like the maximum temperature in your hometown today versus what it was yesterday. You would be measuring this in degrees Fahrenheit, and maybe it was \\(80^{0}F\\) today and it was \\(60^{0}F\\) yesterday. You couldn’t say today is \\(1.33\\) times hotter than yesterday. All you could say is that it is \\(20\\) degrees warmer today than it was yesterday, period. Why can’t we speak in terms of ratios with temperature in Fahrenheit but we could with proficiency levels? We can’t because the Fahrenheit temperature scale is a numeric scale but an arbitrary one where \\(0^{0}F\\) does not mean there is no temperature. However, if Advanced proficiency is \\(0\\) it means nobody is at this level. So we arrive at a distinction – ratio levels of measurement for numeric variables that have a true zero where zero means a complete absence of whatever is being measured versus an interval level of measurement for numeric variables that do not have a true zero.3 Other examples of ratio levels of measurement would be your income, distance of your college town from your hometown, years of formal schooling, number of highway fatalities, number of black swans you have seen, number of children in a family, and so on. Some examples of interval levels of measurement would be a student’s score on a standardized test like the GRE or SAT or ACT, a feeling thermometer that asks you to rate your feeling towards your Senator with a \\(0\\) indicating you are very cold towards this individual and \\(100\\) indicating you feel very warmly towards this individual. Nominal variables are qualitative/categorical variables that measure some non-quantifiable attribute that has no order to it, for example an individual’s sex or race/ethnicity. Ordinal variables are qualitative/categorical variables that measure some non-quantifiable attribute that has an order to it, for example a student’s status (freshman/sophomore/junior/senior). Interval variables are quantitative variables that measure some numeric attribute but there is no natural zero point that reflects complete absence of the attribute being measured. For example, 0 degrees Fahrenheit does not mean no temperature. Ratio variables are quantitative variables that measure some numeric attribute and have a natural zero point. For example, an income of 0 means the individual has no income. Both ratio and interval levels of measurement may yield discrete or continuous variables. Discrete variables would be numerical variables with values that are finite and hence countable. For example, I can count the number of children in a family, the number of cars owned by a family, the number of traffic fatalities occurring in a particular week, and so on. Continuous variables are numerical variables with values that are infinite and hence uncountable. For example, time can be measured in a number of ways, each measuring the same concept (your age, for instance) in a finer and finer way. Technically, we can define these two types of numerical variables most simply as “discrete” if there is no intermediate value possible between two successive values versus “continuous” if any number of values is possible between two successive values. Applying this logic to the number of children we recognize that a family cannot have 1.5 children, it is either 1 or 2, nothing in between. However, the youngest and the next youngest student in a classroom may have values of 18 years and one month and 19 years and two months but we do know there exist many people with ages between these two students. Before we close our discussion of measurement levels, note that there is a hierarchy to levels of measurement: \\(ratio &gt; interval &gt; ordinal &gt; nominal\\). Ideally you would measure everything at the ratio level but this is not always possible. The best we can do with measuring sex, for example, is record an individual’s sex, that is it; there is no way to come up with granular numerical values of sex. The reason why the ratio level trumps the other three is because you can convert ratio levels into any of the other three by suitable alterations as, for example, by asking survey respondents to report their age (in years) and then collapsing age into categories of 18-24, 25-34, 35-44, 45-54, 55-64, and 65 or older (you just created an ordinal variable). But, if you had asked for age categories to begin with you would never be able to extract each individual’s actual age. Note also that in some fields they refer to numerical variables as “continuous” while others will refer to them as “measurement” variables, and categorical variables might be called “factors”. 2.7 The Tricky Business of Cause-and-Effect Among the many things I want to caution you against is thinking in terms of cause-and-effect. People often see correlations and think \\(x\\) must cause \\(y\\), or they see a simple table that show the driver’s race (black versus white) in the rows and whether the driver was subject to a traffic stop (stopped/not stopped)in the columns and think race causes a traffic stop. Indeed, you do this in your own life, many times a day, thinking something happened because of \\(x\\) or that if you do such-and-such some outcome will occur. At a base level, causality is around us and we have become programmed to understand our world in terms of causality and make predictions based on causality. The problem is, cause-and-effect is a difficult thing to demonstrate in general and especially so in the social and behavioral sciences. Although we cannot here cover the granular details and epidemiological debates that surround the study of causality, we can agree upon three commonly used rules to demonstrate cause-and-effect. The presumed cause must precede – in time – the claimed effect. This is a question of the temporal order of the cause (\\(x)\\) and the effect (\\(y\\)). Take the example of depression and drinking; those who are depressed are often found to have high levels of alcohol consumption but does that necessarily mean depression causes alcoholism? Couldn’t it be that alcoholism causes depression since alcohol is a depressant? We need to prove that depressed people drank a little or not at all before depression set in and then drank a lot during the episodes of depression in order to satisfy this rule. We also need to rule out all possible rival explanations for the effect. By this we mean there must be no other logical explanation for the effect we see. Unless we do so we don’t really know if the presumed cause is the sole driver of the effect. Ruling out all possible rival explanations is difficult because the real world is made up of interconnected complex processes and phenomenons, and because our knowledge of how these processes and phenomenons interact tends to be less than perfect. The cause and the effect must co-vary. That is, we must be able to demonstrate a relationship between \\(x\\) and \\(y\\) as, for example, that if depressed then alcoholic and if not depressed then not alcoholic. When all three rules are satisfied, more often than not we would have done a reasonable job of demonstrating cause-and-effect. You might wonder why I am emphasizing the tricky nature of demonstrating causality; I do not want you to make the mistake that millions commit daily, of thinking correlation equals causation, of failing to look for plausible rival explanations, of failing to check to see if there is indeed a measurable relationship between the cause and the effect. Cause must precede effect All possible rival explanations must be ruled out Cause and effect must covary 2.8 Chapter 2 Practice Problems Problem 1 In a browser, open up the Collaborative Institutional Training Initiative (CITI Program) website. Create an account by clicking on the Register button. Enter all information asked for: your institution = Ohio University, first-name and last-name, your OU email address and then a second email address, a username and a password, skip the continuing education (CE) credits portion unless you are looking to pickup some credits but if you do this you may have to pay. Make sure you select Human subjects training in Step 7 and then Group 2: Social and Behavioral Investigators and Key Personnel. Make sure you Finalize registration on the next screen. The very next screen will show the Group 2: Social and Behavioral Investigators and Key Personnel course and current status. You are now ready to take this course. Once you complete the course and are deemed to have passed you will need to save your certificate as a that you will submit as a part of your first assignment. Hold on to this in case you need it in the near future. Problem 2 Identify whether the following variables are numeric or categorical, and the level of measurement. The color of an individual’s eyes An adult individual’s age Number of children in a household An undergraduate student’s standing in the college (Freshman/Sophomore/Junior/Senior) School type (public/private/parochial/charter) A student’s SAT or ACT score Household income Your monthly cellphone bill An attitudinal scale where individuals are asked to rate how they feel about the Supreme Court of the United States (0 = Very Cold, 50 = Neutral, 100 = Very Warm) An attitudinal scale where individuals are asked to rate how they feel about the United States Congress (-100 = Very Cold, 0 = Neutral, 100 = Very Warm) Number of highway fatalities on a specific one mile stretch of US 33E between Athens and Nelsonville, Ohio Whether an individual voted or not in the last general election An individual’s race/ethnicity Your nationality Problem 3 Open up EPA’s Fuel Economy data. These data are the result of vehicle testing done at the Environmental Protection Agency’s National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. You will need to carefully read the accompanying document that explains what and how some attribute is being measured; the document is available here. Identify whether the following variables are numeric or categorical, and their level of measurement. fuelType1 charge120 cylinders drive year ghgScore highway08 model trany youSaveSpend Problem 4 The next set of questions deal with this data-set that consists of responses of graduate students in the social sciences enrolled in STA 6126 in a recent term at the University of Florida. The variables are: GE gender (m, f) AG age in years HI high school GPA (on a four-point scale) CO college GPA (on a four-point scale) DH distance (in miles) of the campus from your home town DR distance (in miles) of the classroom from your current residence TV average number of hours per week that you watch TV SP average number of hours per week that you participate in sports or have other physical exercise NE * number of times a week you read a newspaper AH number of people you know who have died from AIDS or who are HIV+ VE whether you are a vegetarian (yes, no) PA political affiliation (d = Democrat, r = Republican, i = independent) PI political ideology (1 = very liberal, 2 = liberal, 3 = slightly liberal, 4 = moderate, 5 = slightly conservative, 6 = conservative, 7 = very conservative) RE how often you attend religious services (0 = never, 1 = occasionally, 2 = most weeks, 3 = every week) AB opinion about whether abortion should be legal in the first three months of pregnancy (yes, no) AA support affirmative action (yes, no) LD belief in life after death (yes, no) Download these data and read into your software package (SPSS, Excel, etc.). Make sure you label all the variables and their values (if the variable is categorical). For example, the variable HI must be labeled high school GPA (on a four-point scale) and the values of political affiliation labeled Democrat/Independent/Republican rather than d/i/r, and so on. For each variable in the data-set, identify whether it is numeric or categorical. Also identify whether it is a ratio or interval level of measurement (if numeric), and nominal or ordinal (if categorical). Problem 5 Why are our best and most experienced employees leaving prematurely? The data available here includes information on several current and former employees of an anonymous organization. Fields in the data-set include: satisfaction_level = Level of satisfaction (0-1) last_evaluation = Evaluation of employee performance (0-1) number_project = Number of projects completed while at work average_monthly_hours = Average monthly hours at workplace time_spend_company = Number of years spent in the company Work_accident = Whether the employee had a workplace accident left = Whether the employee left the workplace or not (1 or 0) promotion_last_5years = Whether the employee was promoted in the last five years sales = Department in which they work for salary = Relative level of salary (low med high) Identify whether each variable is numeric or categorical, and whether the level of measurement is nominal/ordinal/interval/ratio. Problem 6 Say you are interested in studying if and how a tutoring program for first-generation college students (i.e., the first in their family to attend college) helps these students complete their program of study. What type of research design would you have to work with – experimental? Quasi-experimental? A natural experiment? Why? Would you prefer to use cross-sectional data, time-series data, or panel data and why? What variables would you want to include in your data-set and why? What would be the unit of analysis? Problem 7 What conditions lead to a biased estimate versus an imprecise estimate? Explain with reference to an original example. Problem 8 What are the three commonly accepted conditions that must be met to demonstrate causality? The Appalachian Regional Commission (ARC) is a regional economic development agency that represents a partnership of federal, state, and local government. Established by an act of Congress in 1965, ARC is composed of the governors of the 13 Appalachian states and a federal co-chair, who is appointed by the president. Local participation is provided through multi-county local development districts.↩︎ These data show the percent of students with a specific proficiency level, by grade and subject, for Ohio’s public school districts. There are additional proficiency levels not shown in the table: Accelerated, Proficient, Basic, and Limited. These and other data can be found here.↩︎ The Celsius temperature scale is also an arbitrary scale. This is why for scientific purposes the Kelvin scale is used. Wikipedia describes it thus: “The Kelvin scale is an absolute, thermodynamic temperature scale using as its null point absolute zero, the temperature at which all thermal motion ceases in the classical description of thermodynamics.”↩︎ "],["dataviz.html", "Chapter 3 Visualizing Data 3.1 Visualizing Nominal/Ordinal Data 3.2 Visualizing Interval/Ratio Data 3.3 Some Essential Rules for Good Visualizations 3.4 Chapter 3 Practice Problems", " Chapter 3 Visualizing Data One of the first tasks of data analysis should be to look at our data. Certainly, you should open up the dataset and eyeball it to make sure everything looks okay, that the file isn’t mysteriously corrupted, etc., but that is not what I mean when I say “look”. Nor am I talking about data visualization in the sense of the masterpieces of stalwarts such as Alberto Cairo, Mona Chalabi, Andy Kirk, Robert Kossara, Giorgia Lupi, David McCandless, Cole Nussbaumer Knaflic, Randy Olson, Lisa Charlotte Rost, John Schwabish, Nathan Yau, Stephanie Evergreen, Martin Wattenberg, Fernanda Viégas, and many others. We are not going to create visuals that rival these artists’ stunning works. Rather, our goal will be simpler, to use appropriate tables and graphics when analyzing and describing our data, both so that we understand what the data are telling us and can communicate the resulting story to our audience. To accomplish these tasks we will obey simple rules that help us make wise decisions. 3.1 Visualizing Nominal/Ordinal Data Nominal -&gt; no hierarchy to the levels, values of the categorical variable (A = B = C = …) Ordinal -&gt; some hierarchy to the levels, values of the categorical variable (A &gt; B &gt; C &gt; …) 3.1.1 Bar-charts and Frequency Tables If a variable is measured in nominal or ordinal terms either (i) a bar-chart or (ii) a frequency table are very effective displays of how the variable is distributed: What value seems to be most common? Are high values as likely as low values? You can see both these visuals below, drawn from data about the relative abundance of different species. First, the frequency table. FIGURE 3.1: Factors associated with human-killing tigers in Chitwan National Park, Nepal TABLE 3.1: Factors associated with human-killing tigers in Chitwan National Park, Nepal Activity Frequency Proportion Percentage Disturbing tiger kill 5 0.06 6 Fishing 8 0.09 9 Forest products 11 0.12 12 Fuelwood/timber 5 0.06 6 Grass/fodder 44 0.50 50 Herding 7 0.08 8 Sleeping in house 3 0.03 3 Toilet 2 0.02 2 Walking 3 0.03 3 Total 88 1.00 100 Notice how the frequency table is constructed. You have each human activity listed, followed by the frequency (the number of times someone was killed by a tiger while engaging in each activity), then this column with this frequency converted into a proportion, and finally the proportion converted into a percentage. None of this should be a mystery to you: \\[proportion = \\dfrac{frequency}{Total}\\] \\[percentage = \\left(\\dfrac{frequency}{Total}\\right) \\times 100 = proportion \\times 100\\] The last row in the table shows you the total frequency (we have a total of 88 humans killed), the total proportion (which must sum to \\(1\\)), and the total percentage (which should sum to \\(100\\)). I love frequency tables such as these because they show all the data and the story; most kills occurred while the individuals were cutting grass for cattle fodder, followed by while they were out gathering other forest products, and least of all while they were going to the toilet4. We could improve upon this table and bar-chart by organizing it in such a way that the most dangerous activity is listed/plotted first. This would have the added benefit of quickly drawing the reader’s/viewer’s eyes to the most dangerous activity. What if the variable was an ordinal level variable, say something like an individual’s frequency of praying? TABLE 3.2: Frequency of Prayer Response Category Frequency Never 196 Once a week or less 226 A few times a week 264 Once a day 346 Several times a day 456 Total 1488 FIGURE 3.2: Frequency of Prayer We would be facing the same options, a bar-chart or a frequency table. However, we would have to be cautious here in making sure both the table and the bar-chart categories are logically assigned. That is, notice the Response Category; we start with the option “Never” and each category that follows corresponds to a category that reflects praying more often than the preceding category. This order would have to be maintained so that we would be unable to arrange the table or bar-chart in ascending/descending order of the “Frequency” column. If we made that mistake we would be destroying the natural order that exists in the ordinal variable we have before us. You might be wondering, what about pie charts? Why not use those with nominal/ordinal data? There are two camps, those who hate them and those who think they may be useful. If you are interested, read What do you mean I’m not supposed to use Pie Charts?! but I for one do not use them since I find them less useful than bar-charts. 3.1.2 Contingency Tables and Bar-charts Frequency tables and bar-charts are also useful when you have two nominal/ordinal variables to work with and are interested in exploring difference between the two or more groups reflected in one variable versus whatever is being measured by the second variable. Let us use a specific example, one where we ask if religiosity differs between Liberals, Moderates, and Conservatives. First the table. TABLE 3.3: Political Ideology and Religiosity Liberal Moderate Conservative Total Never 62 46 36 144 Once a week or less 53 63 52 168 A few times a week 53 69 76 198 Once a day 76 89 107 272 Several times a day 69 99 176 344 Total 313 366 447 1126 If you look at Table 3.3, you see the frequencies reported in what we call a contingency table or a crosstabulation – where the distributions of two nominal/ordinal variables are jointly displayed. Reading such a table should be simple. In brief, You see how many Liberals said they prayed “Never”, “Once a week or less”, “A few times a week”, “Once a day”, or “Several times a day”. What pattern is evident from this table? Of the 447 conservatives we see most of them saying they pray several times a day, followed by once a day, and the fewest saying they never pray. The pattern is similar for liberals and moderates, although the differences between the numbers responding “Never” and “Several times a day” is small for liberals than it is for conservatives. The story could be helped a great deal if we calculated percentages for these frequencies. We have two choices when calculating these percentages, we could calculate these as row percentages where we ask “what percent of those who said Never were Liberal, Moderate, and Conservative, respectively?” We could then repeat this for the other categories of religiosity. The result is shown in Table 3.4. This table shows quite clearly that 51.16% of those who say they pray several times a day tend to be Conservative. Likewise, most (43.06%) of those who say they never pray tend to be Liberal. TABLE 3.4: Political Ideology and Religiosity: Row Percentages Liberal Moderate Conservative Never 43.06 31.94 25.00 Once a week or less 31.55 37.50 30.95 A few times a week 26.77 34.85 38.38 Once a day 27.94 32.72 39.34 Several times a day 20.06 28.78 51.16 If we calculated column percentages we would be able to answer such questions as: “What percent of Liberals said they never pray, pray once a week or less, a few times a week, once a day, several times a day?” The same could then be asked of Moderates and Conservatives, respectively. If we used the column percentages shown in Table 3.5 it would be obvious that Moderates and Conservatives tend to be more religious than Liberals. The essential takeaway here is that how you calculate the percentages (row versus column) depends upon what story you want to highlight, the question you want to ask and answer. TABLE 3.5: Political Ideology and Religiosity: Column Percentages Liberal Moderate Conservative Never 19.81 12.57 8.05 Once a week or less 16.93 17.21 11.63 A few times a week 16.93 18.85 17.00 Once a day 24.28 24.32 23.94 Several times a day 22.04 27.05 39.37 Graphing these tables is easily done as well, and very effective; see Figure 3.3. FIGURE 3.3: Political Ideology and Religiosity Categorical variables (one or two) -&gt; bar-chart If you have cross-tabulations, choose between stacked versus dodged bar-charts 3.2 Visualizing Interval/Ratio Data We have several visuals we could draw with numeric data that are either interval or ratio levels of measurement. Let us see these first before we look at the one frequency table that could be used with interval/ratio data. 3.2.1 The Histogram A histogram is used with a single numeric variable and looks like a bar-chart except there are no gaps between consecutive bars unless there are missing data. The example that follows use a popular dataset known as hsb2, which contains information about 200 randomly selected students from a national survey of high school seniors called the High School and Beyond survey. The variables in this dataset include: id = student id female = (0/1) race = ethnicity (1=hispanic 2=asian 3=african-amer 4=white) ses = (1=low 2=middle 3=high) schtyp = type of school (1=public 2=private) prog = type of program (1=general 2=academic 3=vocational) read = standardized reading score write = standardized writing score math = standardized math score science = standardized science score socst = standardized social studies score I’ll use read (Reading scores on a standardized test) to plot a histogram. Notice a few things about the histogram in Figure 3.4. The height of the bars, representing how often a particular score occurs, varies a great deal. A few students have done poorly while a few have done very well, but the rest are distributed over the middle range of test scores. FIGURE 3.4: Histogram of Reading Scores We could farther break this histogram apart by asking if male and female students (female), private versus public students (schtyp), or students of different races/ethnicities (race) perform differently on the reading test. FIGURE 3.5: Histogram of Reading Scores by Various Groups These plots are hard to read for a number of reasons. First, with just \\(n = 200\\) students in all we have more students in some groups than in others and as a result the histograms look very thin for the groups with fewer data points, making it difficult to tease out any patterns. Second, within any group there are too many different test scores so we don’t see a clear pattern at all. To fix these problems we construct groups of scores, turning what is a numeric variable into an ordinal variable. Let us build this by first creating a grouped frequency table and then plotting this table as a histogram. Histogram’s bins must be chosen with some care 3.2.2 Grouped Frequency Tables Because numeric variables take on too many values it is often easier to see their distribution by grouping the numeric values. For example, we could start by seeing what are the lowest and highest reading scores. These turn out to be 28 and 76, respectively. We can build the groups as follows: Calculate difference between maximum and minimum values, which turns out to be \\(76 - 28 = 48\\) Decide how many groups we want. Good practice suggests no fewer than 4/5 and no more than 6/7. Say we go with 5 groups. Divide the gap between the maximum and minimum values by the desired number of groups: \\(= \\dfrac{48}{5} = 9.6\\) and round up to the nearest whole number \\(= 10\\). This tells us how wide each group should be. The groups could thus be \\(28-38, 38-48, 48-58, 58-68, 68-78\\). Notice that these groups span, start to finish, all the values of reading scores. But we’ll have to decide which group to include 38, 48, 58, and 68 in. Should 38 go in 28-38 or in 38-48? The choice doesn’t matter so long as we are consistent. Let us choose a rule that says include 38 in 38-48, 48 in 48-58, 58 in 58-68, and 68 in 68-78. Using this rule we now find each reading score and drop it into its group. Then we calculate how many scores fall in each group, creating our Frequency column. TABLE 3.6: Grouped Frequency Table of Reading Scores Grouped Scores Frequency 28-38 14 38-48 68 48-58 62 58-68 36 68-78 20 Total 200 Now it is easy to see that the largest number of students (68) appear to fall in the 38-48 group of reading scores while the smallest frequency (14) occurs in the 28-38 group. Let us add to this table a percentage column. TABLE 3.7: Grouped Frequency Table of Reading Scores with Percentages Grouped Scores Frequency Percentage 28-38 14 7 38-48 68 34 48-58 62 31 58-68 36 18 68-78 20 10 Total 200 100 The percentages make it even easier to see how the distribution breaks down; 34 percent of the students have scores in the 38-48 range while only 10 percent scored in the highest bracket (68-78). We can also break this down by the three groups we used earlier. FIGURE 3.6: Histogram of Grouped Reading Scores by Specific Groups Histograms were once quite popular but there is a better visual for looking at our numeric variables, one called the box-plot that we’ll see in the next chapter. I am not a huge fan of histograms because grouping decisions influence the story being told. My advice would be to use grouped frequency tables instead of histograms to present a summary overview of your numeric data. Histograms of grouped frequencies of numerical variables can be useful for summary depictions of the distribution 3.2.3 Scatterplots With two numeric variables, a scatterplot comes in very handy if we want to explore how one variable might be related to another. For example, we may want to ask whether students who score high on the reading test also tend to score high on the mathematics test. This could be visually explored via a scatter-plot, as shown below. The goal should be to look for a pattern: Does one variable increase as the other increases or does one variable decrease as the other increases? Or does there seem to be no relationship at all? FIGURE 3.7: Scatterplot of Reading Scores and Mathematics Scores Quite clearly, students who do well in Reading also tend to do well in Mathematics. You can see this by virtue of the upward, left-to-right tilt of the cloud of points. What about Science scores and Mathematics scores? Is there any relationship between doing well in Science and doing well in Mathematics? FIGURE 3.8: Scatterplot of Science Scores and Mathematics Scores Of course, just like everything else we could break this down by any nominal/ordinal variable. The visual below shows you the breakouts by the student’s sex. FIGURE 3.9: Scatterplot of Science Scores and Mathematics Scores, by Sex Work best with two numerical variables since they show the pattern of association between the two variables 3.2.4 Line Graphs If we have time-series data, such as the Presidential Approval data we saw earlier, then a line graph works well because it shows you how the outcome/phenomenon varies over time. Let us take another example, median household incomes. Say I am curious about trends in median household incomes in Ohio, Pennsylvania, and West Virginia. FIGURE 3.10: Trends in Real Median Household Incomes in OH, PA, and WV These plots don’t work well just for financial or election data, they are ideally suited for any phenomenon that is measured over time. For example, the size of the immigrant population over the years, and even the number of lynx pelts reported in Canada per year from 1752 to 1819. FIGURE 3.11: Number of Legal Permanent Residents in the USA FIGURE 3.12: Number of lynx pelts reported in Canada per year from 1752 to 1819 Line graphs are the default choice for showing trends – patterns over time 3.2.5 Polar Charts These charts are helpful for visualizing data that might have otherwise been explored via a bar-chart. For example, say you want to look at the miles per gallon given by a number of different automobiles. We could use a bar-chart, as shown below: FIGURE 3.13: Miles per Gallon Quite obviously, the Toyota Corolla has the best fuel economy, followed by the Fiat 128, while the Cadillac Fleetwood is tied with the Lincoln Continental for the worst fuel economy. A polar chart would present the same information as seen in the bar-chart, albeit in a more aesthetically pleasing manner. FIGURE 3.14: Polar Chart of Miles per Gallon 3.3 Some Essential Rules for Good Visualizations There are several rules, some favored by this expert or that, but regardless of the source of the rule, its merit is not in doubt. So here are the ones I try to follow (although I do break these at times, some times by design and other times unintentionally). It all starts of course with Edward Tufte’s maxims: show the data, tell the truth, help the viewer think about the information rather than the design, encourage the eye to compare the data, make large data sets coherent. A more specific listing of the rules I try to keep in mind follows: Do not include anything in your visualization that is not very informative. Give titles and subtitles where needed and make these stand on their own. At the same time, do not clutter your charts and tables with too much information otherwise the reader gets lost. Use colors wisely. Is this visual to be printed on a color printer? Is it part of a presentation in a poorly lit room? Choose bright colors that stand out from each each other and yet are visible on a printed page or on the projection screen in the room. Remember, a fair share of men seeing the visual could be color-blind so use color palettes designed for color-blind individuals. If this is news to you, read How a dog sees a rainbow, and 12 other images that explain how we see color. Pay attention to what is being plotted: Use sequential colors for ordered data values (that go from low to high or vice-versa); qualitative colors nominal data, and; diverging colors if the goal is to put equal emphasis on mid-range critical values and extremes at both ends of the data range but emphasize the middle with light colors and the low/high extremes with dark colors that have contrasting hues. The visualization is not your personal Mona Lisa; it must be effective for the target audience and help you tell the story. Do not get carried away in creating it. Use a table if a table would be more effective than a graph but remember, while tables are useful for audiences that will want to see more (not less of the actual data you have) graphs are favored by those who want a quick take-away. Always have your y-axis (or x-axis if relevant) starting at zero. If you don’t do this you can misrepresent the data. If you are forced to truncate the axis, point this out to the reader so that they can interpret dips and swells in line charts or differences between heights of frequency bars with care. Combine multiple graphs/tables into a single figure if you can so long as it does not lead to information overload. Go back and look at the scatterplots and line charts and try to visualize what these might look like if the y-axis had been forced to start at \\(0\\); they would certainly look different. For illustration purposes I have let the plotting software pick the starting coordinates. For non-technical audiences you should round up all percentages/proportions to no more than one decimal place and ideally to no decimal place unless doing so distorts the picture. For technical audiences, stay with two or four decimal places. Above all, start with pencil and paper and consider all alternate visualizations possible by drawing a rough sketch of what the finished product should look like. Consider using a choropleth map if geographic variation is one of the key narratives. Be prepared to alter your visual if it does not work; we are all hesitant to delete a page or a graphic and start from scratch but this locks you into something that isn’t working to begin with and you will be stuck in a rut. Avoid jargon like the plague. We often think we sound intelligent when we use jargon but this distracts from the oral/written presentation. Think of your audience and write and present in a manner that will resonate with them. Show as much data as you can; it vastly improves the effectiveness of the visual narration. 3.4 Chapter 3 Practice Problems Problem 1 Download the monthly Great Lakes water level dataset SPSS format from here and Excel format from here. Construct an appropriate chart to display the data for Lake Superior. Be sure to label the x- and y-axis, and to title the chart. Note that water level is in meters. Problem 2 Download the number of births per 10,000 of 23 year old women, U.S., 1917-1975 SPSS format from here and Excel format from here. Construct an appropriate chart to display the trend in the data. Be sure to label the x- and y-axis, and to title the chart. Problem 3 Download the winning speed (in kilometers per hour) for several men’s track and field distances world meets over the 1900 - 2012 period SPSS format from here and Excel format from here. Construct an appropriate chart to display the speeds for the 100 meter dash. Be sure to label the x- and y-axis, and to title the chart. Note that the data are monthly and replicate the speed from the preceding month if the fastest speed was not eclipsed. Problem 4 Use this data-set used in Practice Problem 4 in the preceding Chapter, noting these details of each variable. Construct a frequency table for belief in life after death, showing both the frequencies and the relative frequencies (as percentages). Based on the table, what do most people seem to believe? Report the percentage for your answer. Construct an appropriate chart for these data, making sure to label all axis and providing a title. Problem 5 Construct an appropriate chart that shows the relationship between high school GPA and college GPA. Label both axis and title the chart. What does this plot show? Are the two positively/negatively related? How strong would you guess is the relationship? Problem 6 Construct a contingency table of vegetarianism against belief in life after death. What percent of vegetarians believe in life after death? What percent of those who believe in life after death are vegetarians? Use an appropriate chart to show the relationship between vegetarianism and belief in life after death. Label everything. Problem 7 Construct a grouped frequency table with five groups of the variable age. Report both the frequencies and the relative frequencies (as percentages). Plot the relative frequencies using an appropriate chart. Label everything as usual. What is the modal age group? The next set of questions revolve around the 2016 Boston Marathon race results available here. The dataset contains the following variables: Bib = the runner’s bib number Name = the runner’s name Age = the runner’s age (in years) M/F = the runner’s gender (M = Male; F = Female) City = the runner’s home city State = the runner’s home state Country = the runner’s home country finishtime = the runner’s time (in seconds) to the finish line Problem 8 What countries had the largest and second-largest number of runners in the race? Problem 9 What was the distribution of the runners’ gender? Use a suitable chart to reflect the distribution. Problem 10 Construct a grouped frequency table of the runners’ age, using the following groupings – 18-25; 25-32; 32-39; 39-46; 46-53; 53-60; 60-67; 67-74; 74-81; 81-88. Also construct a grouped histogram. What is the modal age group? Problem 11 Draw a scatter-plot of runners’ age and finish times. Does this show any relationship? If it does, what sort of a relationship? Problem 12 Using a reasonable grouping structure, construct country-specific histograms of finish times for runners from each the following countries – AUS, BRA, CAN, CHN, FRA, GBR, GER, ITA, JPN, MEX, and USA. Are finish times skewed for each country? Is the direction of the skew similar? What country seems to have the least skew? What country seems to have the most skew? Use this data-set, also used in Practice Problem 5 in the preceding Chapter, to answer the following questions. Problem 13 Were employees who had a workplace accident more likely to leave than employees who did not have an accident? Briefly explain your conclusion with appropriate charts/tables. Problem 14 Are low salary employees more likely to leave than medium/high salary employees? Why do you conclude as you do? Briefly explain your reasoning with appropriate charts/tables. Problem 15 Construct an appropriate chart that shows the distribution of the number of years employees spent in the company. What patterns do you see in this chart? Download the 2020 County Health Rankings data SPSS format from here, CSV format from here and the accompanying analytic codebook Problem 16 You should see the following measures (measurename) Adult obesity Children in poverty High school graduation Preventable hospital stays Unemployment rate Looking at pairs of variables in turn, briefly (20 words or less) state whether you would expect a positive/negative/no relationship between the variables in each pair and why you deduce as you do? Problem 17 Construct scatterplots of all pairs of the five variables. Problem 18 How do the scatterplots you constructed in the preceding problem stack up against the expectations you listed in Problem 16? Which plots surprised you and how? Be brief. Indoor plumbing is lacking in many still developing countries, particularly in the rural areas. In some of these countries, particularly India, tigers aren’t the most dangerous animals↩︎ "],["meansd.html", "Chapter 4 Central Tendency and Dispersion 4.1 Central Tendency 4.2 Dispersion (aka Variability) 4.3 Properties of the Mean and Variance/Standard Deviation 4.4 The Empirical Rule 4.5 Z-scores 4.6 The Coefficient of Variation 4.7 Symmetric, Skewed and Bi-modal Distributions 4.8 The Five-number Summary and the Box-plot 4.9 The Power of Summary Statistics Married to Graphical Explorations 4.10 Outliers, Outliers, What do I do with them? 4.11 Beware the flawed rule 4.12 Chapter 4 Practice Problems", " Chapter 4 Central Tendency and Dispersion While graphical displays of our data are useful descriptors of how a variable is distributed, numerical summaries of our data are far more concise and descriptive. But what would be good descriptors? A simple rule tends to work well in most situations. In particular, think about this: I tell you that the typical homeless person has been on the streets for 14 months. I know this because I have access to all homeless persons and have documented how long they have been on the streets. I then give you a random sample of homeless persons, blindfold you, and ask you to pick one homeless person from your sample. When you do so I ask you to guess how long this individual has been on the streets. What would be your best guess? Well, ideally it should be 14 months because that is the average duration in the population. Of course, you could say 14 months and that might be exactly how long this person has been on the street. I don’t tell you whether your guess was correct or not. Instead, I ask you to tell me how many months \\(\\pm\\) from the 14 months that is true of the population might you be if your guess was wrong? You would be unable to answer this without a blind guess unless you knew how much the duration of homelessness varies in the population. This is where knowing something about what is most likely, most typical, the average, and how much variation there is around this typical, average, most likely feature is essential to statistics. In this chapter we will focus on mastering two concepts – how to calculate the “average”, and how to calculate “variability” around this average. The former is central tendency and the latter is dispersion. 4.1 Central Tendency Central tendency is a statistical measure that defines the center of a distribution and is most typical, most representative, of the scores that comprise the distribution of the variable of interest. There are three measures of central tendency – the mean, the median, and the mode. Each of these has strengths and weaknesses and is ideally suited to a specific situation with respect to a given variable. Let us see what these measures are and when they are most useful. Before we do so, however, a brief tutorial on some symbols and mathematical operators you will start seeing. Chief among these is the summation operator \\(\\sum\\). This Greek symbol calls us to add everything that follows. For example, if I did \\(\\sum(x_i)\\) where \\(i = 1, 2, 3, 4\\) then we are being asked to sum each value of \\(x\\). The subscript \\(i\\) distinguishes between \\(x_i = 1, x_i = 2, x_i = 3, \\text{ and } x_i = 4\\) so we know which x-value we are using in the calculation. In a nutshell, then, \\(\\sum(x_i) = \\sum(x_1 + x_2 + x_3 + x_4) = \\sum(1 + 2 + 3 + 4) = 10\\). You will also see the same thing expressed alternatively thus: \\[\\sum^{n}_{i=1}{x_i}, i=1,2,3,\\ldots,n\\] which basically means the same thing as above: Add each observation \\(i\\) of the total \\(n\\) observations of \\(x\\). In other words, \\[\\sum^{n}_{i=1}{x_i} = x_1 + x_2 + x_3 + \\ldots + x_n\\] Note also that \\[\\sum^n_{i=1}(x_i - 1) = \\sum((x_1 - 1) + (x_2 - 1) + (x_3 - 1) + (x_4 - 1)) = \\sum(0 + 1 + 2 + 3) = 6\\] \\[\\sum^n_{i=1}(x_i^2) = \\sum((x_1^2 + x_2^2 + x_3^2 + x_4^2)) = \\sum((1 + 4 + 9 + 16)) = 30\\] \\[(\\sum^n_{i=1}x_i)^2 = (\\sum(x_1 + x_2 + x_3 + x_4))^2 = (\\sum(1 + 2 + 3 + 4))^2 = (10)^2 = 100\\] \\[\\sum^n_{i=1}(x_i + y_i) = \\sum^n_{i=1}x_i + \\sum^n_{i=1}y_i\\] \\[\\sum^n_{i=1}kx_i = k\\sum^n_{i=1}x_i\\] \\[\\sum^n_{i=1}k = kn\\] We will start seeing the summation sign with increasing frequency as we progress through the course. You may be wondering: Do I need to know how to do this? The short answer is no, you don’t, because it is not completely necessary to get a basic grasp of the concepts we must wrestle with. However, if you are curious, math-oriented, or just want a deeper understanding because you think you may want to take a few more statistics courses, then yes, try to understand how formulas work. If you crack this nut you will be surprised at the whole new world that opens up before you. 4.1.1 Mean The arithmetic mean (also known as the arithmetic average) is computed by adding up the scores in the distribution and dividing this sum by the sample size. In plain words, and using the summation operator, the arithmetic mean of \\(x_i\\) is calculated as \\[\\dfrac{\\sum^n_{i=1}x_i}{n} = \\dfrac{x_1 + x_2 + x_3 + \\ldots + x_n}{n}\\] While the underlying calculations are the same, we do draw a distinction between the population mean and the sample mean, using different symbols when calculating each. The formulas are given below in (4.1) and (4.2), respectively: \\[\\begin{equation} \\tag{4.1} \\text{Population Mean } = \\mu = \\dfrac{\\sum^N_{i=1}x_i}{N} \\end{equation}\\] \\[\\begin{equation} \\tag{4.2} \\text{Sample Mean } = \\bar{x} = \\dfrac{\\sum^n_{i=1}x_i}{n} \\end{equation}\\] Notice the important difference – the population mean is symbolized by \\(\\mu\\) (pronounced mu or myoo), and the total number of observations (aka the population size) is symbolized by uppercase N. In contrast, the sample mean is symbolized by \\(\\bar{x}\\) (pronounced \\(x-bar\\)) and the total number of observations (aka the sample size) is symbolized by lowercase n. Here is a trivial example, that shows you the starting bi-weekly salary of 12, randomly selected graduates of a university’s public affairs school. TABLE 4.1: Starting biweekly salaries Bi-weekly Salary 1 2710 2 2755 3 2850 4 2880 5 2880 6 2890 7 2920 8 2940 9 2950 10 3050 11 3130 12 3325 Since this is a sample, the sample mean could be calculated as: \\[\\bar{x} = \\dfrac{\\sum^n_{i=1}x_i}{n}\\] \\[= \\dfrac{x_{1} + x_{2} + \\cdots + x_{12}}{n}\\] \\[= \\dfrac{2,850 + 2,950 + \\cdots + 2,920}{12}\\] \\[= \\dfrac{35,280}{12}\\] \\[\\therefore \\bar{x} = 2,940\\] The interpretation of this sample mean is rather simple. Think of it as follows: If I ask you to make a random pick of a graduate from this school and ask you what his/her bi-weekly salary is likely to be, your best guess should be $2,940. Why? Because your sample told you that this is the average salary, the typical salary. Of course, this does not mean everybody will have this salary or that the next student you randomly pick has this salary. The table and common sense should tell you that salaries vary, the person you select may be making more or less than your sample mean. However, guessing any other salary would be a wild guess. Instead, knowing the sample mean gives you the basis to make a more informed guess that is more likely to be correct than any other number you pick out of thin air. Even if you are wrong, you will not be very far from the truth. This accuracy is driven by statistical theory that we will cover shortly. Before we do that though, we need to understand two other measures of central tendency. TABLE 4.2: Properties of the Arithmetic Mean Observation x (x - 2) (2 * x) (x / 2) 1 6 4 12 3.0 2 3 1 6 1.5 3 5 3 10 2.5 4 3 1 6 1.5 5 4 2 8 2.0 6 5 3 10 2.5 Total 26 14 52 13.0 4.1.2 Median The median is the middle-value that occurs when the data are arranged in an ascending or descending order, and is commonly denoted by the symbol \\(Md\\). Since central tendency is all about finding the “center” of the values of a variable \\(x\\), the median tends to be intuitive for most folks and calculating it is equally simple. Arrange the values of \\(x\\) either in ascending order or in descending order. If the number of data points in the population or sample is an odd number, the median observation can be identified as \\(Md = \\dfrac{N + 1}{2}\\) for a population and \\(Md = \\dfrac{n + 1}{2}\\) for a sample. See Table 4.3 for an example. If the number of data points in the population or sample is aneven number, the median observation can be identified as the average of the middle two values of \\(x\\). See Table 4.4 for an example. TABLE 4.3: Median Salary when n or N is an odd number Bi-weekly Salary 1 2710 2 2755 3 2850 4 2880 5 2890 6 2920 7 2940 8 2950 9 3050 10 3130 11 3325 The median observation (sometimes also called the median position) is the \\(6^{th}\\) observation and the median salary is thus $2,920. TABLE 4.4: Median Salary when n or N is an even number Bi-weekly Salary 1 2710 2 2755 3 2850 4 2880 5 2880 6 2890 7 2920 8 2940 9 2950 10 3050 11 3130 12 3325 Now the median observation lies between the \\(6^{th}\\) and \\(7^{th}\\) observations and the hence median salary is \\(Md = \\dfrac{2,890 + 2,920}{2} = \\dfrac{5,810}{2} = \\$2,905\\). With even-numbered samples/populations you could end up with a median value that is actually not seen in the data-set since we average the middle values. Don’t be surprised if this happens. 4.1.3 Mode The mode or modal value refers to the value of the variable \\(x\\) that occurs most frequently in the data-set. The mode makes little sense for numerical variables (interval or ratio) and instead works best with categorical variables (nominal or ordinal). The table below shows you an example where 50 randomly sampled residents of Manhattan (New York City, NY) were asked about how they commute to work on a typical working day of the week. TABLE 4.5: Modal Transportation Choice Transportation.Choice Frequency Walking 19 Bus/Train 8 Taxi 5 Bicycle 13 Drove alone/Car-pooled 5 Total 50 Most of the respondents \\((19)\\) out of \\((50)\\) said they walk to work and hence the modal transportation choice is walking to work. Just as in the case of the arithmetic mean, the implication should be clear: If you picked a randomly chosen resident of Manhattan and I ask you how do you think they commute to work your best guess ought to be that they walk. You could be wrong, but based on the sample of 50 residents you have seen, walking would be the most educated guess you could make. 4.1.4 Some Features of Measures of Central Tendency If we have categorical data then now we know that the mode would be the ideal measure to describe the average value. However, what if we have numerical data? Should we choose the mean or the median? We choose between the mean and median depending upon whether we have extremely unusual observations or not. For example, imagine I give a 10-point pop-quiz in a seminar class of five students and the scores are \\(x = 1, 3, 5, 7, 9\\) leading to an average score of \\(5\\). We see that \\(5\\) is also the median. Intuitively, \\(5\\) makes perfect sense as the average score given the data. However, what if the scores were \\(x = 1, 1, 1, 2, 10\\), resulting in a mean of 3. The median score is clearly \\(1\\). In this latter situation, the mean has been inflated by that one student who got a perfect score but most people did very poorly. Intuitively then, saying the average score was a 3 would be misleading given that four out of five students scored below the average! So what would be a good measure of the average score here? The answer is, the median. If you did this and said the average student scored \\(1\\) you would be describing the scores more accurately than if you opted for the mean of \\(3\\). This reflects an important principle we should follow when describing the data in terms of means and medians. With unusually large or small scores – compared to the rest of the data – the median better represents the average than does the mean. Note that the mean will be inflated (i.e., pulled upwards) if the unusual data point occurs on the high side of the variable’s values, and the mean will be deflated (i.e., pulled downwards) if the unusual data point occurs on the low side of the variable’s values. The median is also useful when you have open-ended data or incomplete data. For instance, say you give kindergarten kids some task to complete and are timing them. Some of them do not finish the task so you cannot record a time value for these kids. Maybe the times (in minutes) look as follows: \\(x = 7, 8, 6, 9, 1, 3, 12, 18, NA, NA\\). If you had to calculate the average, what could you do? You may be tempted to discard the data for all kids who did not finish the task and calculate the mean time based on all other kids’ data this would lead to a mean time of \\(8\\) minutes. But this would lead to a flawed conclusion. After all, it would be important to recognize the truth that some kids couldn’t finish the task. How then could you calculate the average? You could arrange the data in increasing order of time taken to complete the task, putting all the kids who did not finish at the very high end of the times, and then calculate the median time! If you did this you would have: \\(x = 1, 3, 6, 7, 8, 9, 12, 18, NA, NA\\) and the median time would be \\(8.5\\) minutes, a more accurate portrayal of average time on task. The mean has two things going for it though; it uses all data points (assuming no incomplete or missing data) and is used in most statistical calculations. This is why we often fall back on the mean as the best measure of the average value of a variable.5 4.2 Dispersion (aka Variability) Knowing something about the average is useful but by itself the average is of limited value. Rather, what you want to also know is how much do the data values vary? Why does this matter? Think about this in the context of your exam scores. Let us say that when I give you your exams back I also tell everyone that the mean exam score was \\(90\\) (out of a \\(100\\)) on Exam I. Your score was a \\(96\\) and you are relieved because you did better than the average student in the class. But you shouldn’t rejoice just yet. Instead you should ask: How much did the exam scores vary for the class? I tell you average variability was \\(2\\) points, which means most scores fell in the \\(88 \\text{ and } 92\\) range. Well then, you did much better than most people, maybe you were the top performer. However, if I tell you the average variability was \\(6\\) then you know most scores fell between \\(84 \\text{ and } 96\\) and now you don’t feel so good any more. Knowing variability tells you something useful about the variable you are studying. If the variable shows little variability then any conclusions you make will have good accuracy when you extrapolate from your sample to the population (unless you made a mess of gathering your sample data to start with). However, if you see a lot of variability in your data then your conclusions are likely to have a lot of uncertainty when applied to the population from which you drew your sample. I would thus go as far as to say that variability is the linchpin for all data analysis, and the reason you will see a specific measure of variability figuring prominently in most statistical calculations. Let us see what that measure is by working our way to it, starting with a crude measure of variability. 4.2.1 Range One of the crudest measures of variability is the range, computed as \\(Range = x_{max} - x_{min}\\) where \\(x_{max}\\) is the maximum value of variable \\(x\\) and \\(x_{min}\\) is the minimum value of variable \\(x\\). Intuitively, if the smallest and largest values are very close together, there cannot be much variability in \\(x\\) but if there is a huge gap between the lowest and highest values then there must be quite a bit of variability. Let us take two examples to demonstrate each scenario, respectively. \\(x = 1, 2, 3, 4, 10\\) and so range is \\(10-1 = 9 \\ldots\\) high variability \\(y = 1, 2, 3, 4, 5\\) and so range is \\(5-1 = 4 \\ldots\\) low variability Simple enough, and yet fallible. See the following two variables, \\(x\\) and \\(y\\). \\(x = 1, 3, 5, 7, 10\\) and so range is \\(10-1 = 9\\) \\(y = 1, 1, 1, 1, 10\\) and so range is \\(10-1 = 9\\) The range is identical for \\(x\\) and \\(y\\) but quite obviously the scores hardly vary in \\(y\\)! This is an example of the limitation of the range – it can mislead because it only relies on two data points, even though your sample size may be 10 or 100 or even 10,000. In fact, the range is being driven by the smallest and largest values and that would be as much an issue here as in the case of the mean. Consequently, the range is of very limited use in practice. 4.2.2 Quartiles and Interquartile Range If focusing only on the ends of the data (i.e., \\(x_{min} \\text{ and } x_{max}\\)) is not a good way to measure variability, then one alternative may be to ignore the low and high values and focus just on the middle of the distribution, say the middle \\(50\\) percent of the distribution. This approach, called the interquartile range (IQR) is a standard way of measuring variability and used quite often. To calculate the IQR, though, we need first to identify the values that would give us the middle \\(50\\) percent of our data. 4.2.2.1 Calculating the Quartiles Just as the median halved the data, the quartiles quarter the data into four sections of \\(25\\) percent. We refer to these technically as the first quartile (\\(Q_1\\)): \\(25\\) percent of the data fall below \\(Q_1\\), the second quartile (\\(Q_2\\)): also the median, \\(50\\) percent of the data fall below \\(Q_2\\), and the third quartile (\\(Q_3\\)): \\(75\\) percent of the data fall below \\(Q_3\\) Note that \\(Q_1\\) is also called the \\(25^{th}\\) percentile, \\(Q_2\\) is called the \\(50^{th}\\) percentile, and \\(Q_3\\) is called the \\(75^{th}\\) percentile. Note too that we will be using software to perform the calculations, quartiles can be calculated by hand as shown below: \\[\\begin{equation} \\tag{4.3} i = \\left(\\dfrac{p}{100}\\right) \\times n \\end{equation}\\] where \\(i\\) is the observation number and \\(p\\) is the percentile we want. If, for example, I want the \\(25^{th}\\) percentile, then this is given by \\(i = \\left(\\dfrac{25}{100}\\right) \\times n\\). Similarly, the \\(50^{th}\\) percentile (our median) is calculated as \\(i = \\left(\\dfrac{50}{100}\\right) \\times n\\) and the \\(75^{th}\\) percentile as \\(i = \\left(\\dfrac{75}{100}\\right) \\times n\\).6 Let us calculate \\(Q_1\\) and \\(Q_3\\) for our salary data when we had \\(n=11\\), an odd numbered sample size. In this case, \\(Q_1 = i = \\left(\\frac{p}{100}\\right) \\times n = \\left(\\frac{25}{100}\\right) \\times 11 \\approx 3\\) or the \\(3^{rd}\\) salary. Likewise, \\(Q_3 = i = \\left(\\frac{p}{100}\\right) \\times n = \\left(\\frac{75}{100}\\right) \\times 11 \\approx 9\\) or the \\(9^{th}\\) salary. The actual salary values of these two observations turn out to be \\(Q_1 =\\) $2,850 and \\(Q_3\\) = $3,050; \\(25\\) percent of our sample had salaries less than $2,850 and \\(75\\) percent of our sample had salaries below $3,050. What if we have an even-numbered data-set? Recall that the median was between the \\(6^{th}\\) and the \\(7^{th}\\) values, which effectively gave us six salaries below the median and six salaries above the median. Consequently, the first quartile would be \\(Q_1 = \\dfrac{3^{rd} \\text{ salary } + 4^{th}\\text{ salary }}{2} = \\dfrac{2850 + 2880}{2} = 2865\\). Similarly, the third quartile would be \\(Q_3 = \\dfrac{9^{th} \\text{ salary } + 10^{th}\\text{ salary }}{2} = \\dfrac{2950 + 3050}{2} = 3000\\). If we had to depict the fences marking the quartiles and the Median, it would look as follows: \\[2710, 2755, 2850, \\vert|Q1\\vert|, 2880, 2880, 2890, \\vert|Median\\vert|, 2920, 2940, 2950, \\vert|Q3\\vert|, 3050, 3130, 3325\\] Again, do not be thrown off by the fact that the quartiles seem to be illusory (i.e., values that are not seen in the sample). Also note that different statistical software will calculate the quartiles marginally differently depending upon how the software has been programmed. 4.2.2.2 Calculating the Interquartile Range The interquartile range (IQR) is given by \\(Q_3 - Q_1 \\cdots\\) the spread in the middle \\(50\\) percent of our data values. In the salary example we just worked with, \\(IQR = Q_3 - Q_1 = 3050 - 2850 = 200\\). So the spread in the middle \\(50\\) percent of our data values is $200. For the salary data with \\(n = 12\\), the \\(IQR = Q_3 - Q_1 = 3000 - 2865 = 135\\). The IQR is used in visualizing our data with a more powerful graphic than what we have seen in Chapter 3. We will cover this graphic in Chapter 5, once we have covered some more necessary ground. Before we move on, note that whereas the range was weak in its reliance upon just two data points, the IQR is only marginally better because it relies upon just the middle \\(50\\) percent of our data to give us a sense of variability. After all, the IQR is discarding half of our data! This is a waste and will lead to an incomplete sense of variability in our data. At this point, unless you have dozed off, you should be expecting me to say that a better measure of variability would be one that takes all data into account. You bet. That is precisely what we will do next. 4.2.3 Variance and Standard Deviation As I had mentioned at the very outset of this chapter, variability is not calculated in a vacuum but with reference to the average value of the variable. Say we are using the mean as the average. How could we calculate average variability, average distance of a data value, around the mean? One way to do this might be to calculate how far each data value is from the mean. Then take these distances, add them up, and divide the resulting total by the number of data points we have (whether \\(N \\text{ or } n\\)). Let us try this simple approach with the following sample \\(x = 1, 3, 4, 6, 22, 24\\). Note that the mean is \\(10\\). TABLE 4.6: Distance from the Mean: Part I x Mean of x (x - Mean of x) 1 10 -9 3 10 -7 4 10 -6 6 10 -4 22 10 12 24 10 14 Total 60 0 Ah! The total of the distances is \\(0\\), which makes it impossible to calculate the average distance from the mean. Can we work around it? Sure, we could if we squared each distance so that all negative values were transformed into positive values. The result is shown below. TABLE 4.7: Distance from the Mean: Part II x Mean of x (x - Mean of x) (x - Mean of x) Squared 1 10 -9 81 3 10 -7 49 4 10 -6 36 6 10 -4 16 22 10 12 144 24 10 14 196 Total NA 0 522 Now the average distance would be \\(\\dfrac{522}{6} = 87\\), but interpreting this would be difficult since this is average distance in squared units. This isn’t an intractable problem though since just as we squared the distances, we could also take the square root to get average distance in meaningful units of the original metric. That is, we could calculate \\(\\sqrt{\\dfrac{522}{6}} = \\sqrt{87} = 9.32379\\)! That is, average variability is 9.32379. Congratulations! You have just calculated the variance and the standard deviation. Formally, for a variable \\(x\\), \\[\\begin{equation} \\tag{4.4} \\text{Population Variance } = \\sigma^2 = \\dfrac{\\sum(x_i - \\mu)^2}{N} \\end{equation}\\] \\[\\begin{equation} \\tag{4.5} \\text{Population Standard Deviation } = \\sigma = \\sqrt{\\dfrac{\\sum(x_i - \\mu)^2}{N}} \\end{equation}\\] While the above formulas work for population data, we have to make a slight modification to these formulas when we are working with sample data. In brief, the formulas become: \\[\\begin{equation} \\tag{4.6} \\text{Sample Variance } = s^2 = \\dfrac{\\sum(x_i - \\bar{x})^2}{n-1} \\end{equation}\\] \\[\\begin{equation} \\tag{4.7} \\text{Sample Standard Deviation } = s = \\sqrt{\\dfrac{\\sum(x_i - \\bar{x})^2}{n-1}} \\end{equation}\\] The key part of the modified formula is not the symbols \\(s^2 \\text{ and } s\\) replacing \\(\\sigma^2 \\text{ and } \\sigma\\); it is the fact that we divide by a smaller number \\(\\cdots (n-1) \\text{ instead of } (N)\\). Why this change? 4.2.4 Why \\(n-1\\)? Consider these data: \\(x = 1, 5, 4, 10\\). What is the mean? \\(\\bar{x} = 5\\). What is the sample size? \\(n = 4\\). Fair enough. Now let me give you the following data: \\(x = 1, 5, 4, ?\\) along with the information that although the fourth data point is not provided to you, \\(\\bar{x} = 5\\). Can you tell me what the missing data value must be? Yes you can. How? \\[\\bar{x} = \\dfrac{\\sum^n_{i=1}}{n} = \\dfrac{1 + 5 + 4 + ?}{4} = 5\\] \\[1 + 5 + 4 + ? = 5 \\times 4 \\] \\[10 + ? = 20\\] \\[? = 20 - 10 = 10\\] So the missing number must be \\(10\\). You could have figured out whatever was the missing number in a similar fashion even if I had provided you with \\(x = 1, ?, 4, 10\\) or \\(x = ?, 5, 4, 10\\) or \\(x = 1, 5, ?, 10\\). Why? Because the mean is calculated by summing all values of \\(x\\) and then dividing by the sample size. When we do this, we know that one number in our data-set will always be locked down to a specific value, it cannot just be any random value. Since we calculate the variance by using the mean, we are forcibly limiting our estimate of variability. Consequently, we make a specific adjustment, that of dividing the squared distances by a smaller number in order to make a correction and obtain a slightly larger estimate of variability. Another way of thinking about this is as follows. I give you three values in a sample: \\(x = 4, 6, 8\\). I also tell you to calculate the variance in two situations: (a) You are told that the population mean is \\(\\mu = 3\\) and (b) You are not given this information and so must calculate the sample mean in order to calculate variance without adjusting the denominator. The calculations are shown below. x (x - Popn Mean) (x - Popn Mean) Squared (x - Sample Mean) (x - Sample Mean) Squared 8 5 25 2 4 4 1 1 -2 4 6 3 9 0 0 Total 9 35 0 8 If I calculate the variance when I am given \\(\\mu=3\\) I get \\(\\dfrac{35}{3} = 11.66667\\) but when I calculate variance by using the sample mean \\(\\bar{x} = 6\\) without adjusting the denominator I get an estimated variance of \\(\\dfrac{8}{3} = 2.666667\\), quite a bit smaller than what I got earlier. However, if I make the adjustment we are asked to make: \\(\\dfrac{8}{3-1} = \\dfrac{8}{2}=4\\) I get a much bigger estimate of variance. Thus, it matters whether we know the population mean or we do not. If we know the population mean then our estimated variance is closer to the “true variance” but if we don’t know the population mean and must use the sample for calculating both the mean and the variance, then we need to adjust the formula to come closer to the “true variance”. Note that the adjustment of course doesn’t get us completely to \\(11.66\\), we are still a big distance away but nevertheless \\(4\\) is closer than \\(2.66\\). If none of the two explanations help, think of the adjustment as necessary because no sample will ever fully capture all the variability in the population. Hence we need to make an adjustment to avoid ending up with an estimate of the variance that is guaranteed to be smaller than the true variance in the population. Variance and Standard Deviation for the Salary Data Before we move on, here is the calculated variance and standard deviation for our salary data. TABLE 4.8: Variance and Standard Deviation for the Salary Data Salary (Salary - Mean) (Salary - Mean) Squared 2710 -230 52900 2755 -185 34225 2850 -90 8100 2880 -60 3600 2880 -60 3600 2890 -50 2500 2920 -20 400 2940 0 0 2950 10 100 3050 110 12100 3130 190 36100 3325 385 148225 Total 0 301850 \\[\\text{Sample mean: }\\bar{x}=2940 \\ldots\\] \\[\\Sigma(x_{i} - \\bar{x}) = 0\\] \\[\\Sigma(x_{i} - \\bar{x})^{2} = 301850\\] \\[\\text{Sample Variance: }s^{2} = \\dfrac{301850}{(12 - 1)} = \\$27440.91 \\ldots\\] \\[\\text{Sample Standard Deviation: }s = \\sqrt{27440.91} = \\$165.63 \\ldots\\] 4.3 Properties of the Mean and Variance/Standard Deviation The mean has some useful properties that come in handy when we have to manipulate data. For instance, say we have the following scores \\(x = 2, 4, 6, 8, 10\\). Let us calculate the mean and standard deviation of these scores. These turn out to be \\(\\bar{x} = 6\\) and \\(s_{x} = 3.162278\\). Now let us divide each of these numbers by \\(2\\) to create a new variable, \\(x^{*} = \\frac{x}{2}\\). Turns out \\(x^{*} = 1, 2, 3, 4, 5\\). If you calculate the mean and standard deviation for \\(x^{*}\\) you will find these to be \\(\\bar{x^{*}} = 3\\) and \\(s_{x^{*}} = 1.581139\\), respectively. Compare these to \\(\\bar{x}\\) and \\(s_{x}\\) and note the difference. What happened here? Well, if you divide each \\(x_i\\) by a constant \\(k\\), this is the same as dividing the original mean by \\(k\\) and the original standard deviation by \\(k\\). Likewise, if you multiply \\(x_i\\) by \\(k\\) you will see the new mean and standard deviation equal \\(\\bar{x} \\times k\\) and \\(s \\times k\\), respectively. What if you add a constant to each \\(x_i\\)? Say we add 2 to each of the original \\(x\\) values to create a new variable \\(x^{**} = x + 2\\). This will yield \\(x^{**} = 4, 6, 8, 10, 12\\) and the mean and standard deviation will be \\(8\\) and \\(3.162278\\), respectively. Note that the mean has increased by \\(2\\), the same value added to each \\(x_i\\) but the standard deviation is unchanged and is still \\(3.162278\\). What if you subtracted a constant \\(k\\) from each \\(x_i\\)? Say we subtract 1 from each of the original \\(x_i\\) to create \\(x^{***} = x - 1\\). This will yield \\(x^{***} = 1, 3, 5, 7, 9\\), and the new mean will be \\(5\\) but the standard deviation will still be \\(3.162278\\). TABLE 4.9: Properties of the Mean and the Standard Deviation x x / 2 2x x + 2 x - 1 1 2.00 1.00 4.00 4.00 1.00 2 4.00 2.00 8.00 6.00 3.00 3 6.00 3.00 12.00 8.00 5.00 4 8.00 4.00 16.00 10.00 7.00 5 10.00 5.00 20.00 12.00 9.00 Mean 6.00 3.00 12.00 8.00 6.00 Standard Deviation 3.16 1.58 6.32 3.16 3.16 In brief, then, Changing the value of any observation changes the mean Adding or subtracting a constant \\(k\\) from all observations is equivalent to adding or subtracting the constant \\(k\\) from the original mean and leaves the standard deviation and variance unchanged Multiplying or dividing a constant \\(k\\) from all observations is equivalent to multiplying or dividing the original mean by the constant \\(k\\) and the original variance and standard deviation will also be multiplied or divided by \\(k\\) 4.4 The Empirical Rule There is a rule, the empirical rule that says for a bell-shaped distribution, approximately 68% of the data lie within one standard deviation of the mean, that is, in the interval with endpoints \\(\\bar{x} \\pm s\\) for samples and \\(\\mu \\pm \\sigma\\) for populations; approximately 95% of the data lie within two standard deviations of the mean, that is, in the interval with endpoints \\(\\bar{x} \\pm 2s\\) for samples and \\(\\mu \\pm 2\\sigma\\) for populations; and approximately 99.7% of the data lie within three standard deviations of the mean, that is, in the interval with endpoints \\(\\bar{x} \\pm 3s\\) for samples and \\(\\mu \\pm 3\\sigma\\) for populations. The empirical rule does not apply to data that are asymmetric and in fact even with bell-shaped distributions the percentages 68%, 95% and 99.7% are only approximate. There is also a more general theorem, called Chebyshev's Theorem that says at least \\((1 - \\dfrac{1}{z^{2}})\\) of the data values must lie within \\(z\\) standard deviations of the mean, where \\(z\\) is any value greater than 1. In particular, approximately 75% of the data points will fall within \\(\\bar{x} \\pm 2s\\), approximately 89% of the data points will fall within \\(\\bar{x} \\pm 3s\\), and approximately 94% of the data points will fall within \\(\\bar{x} \\pm 4s\\) (and of course similar spans around the population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\). Why are these rules important? They are useful because we know the approximate range within which a specific amount of the data values must fall if the population from which the sample is being drawn is bell-shaped (in which case the empirical rule comes into force) or if we don’t know the shape of the population (in which case Chebyshev’s Theorem comes into force). 4.5 Z-scores Combining means and standard deviations helps us in many ways, one of these being the ability to compare what seem to be apples and oranges. For instance, let us say that in December 2016, total snowfall in Boston (MA) was 39 inches, and in Caribou (ME) it was 129 inches. The typical mean and standard deviation for each city are given in the table below. Question is: Which city had more unusual snowfall?7 TABLE 4.10: Comparing Snowfall in Disparate Locations Place Mean Std.Dev. December.2016 Caribou (ME) 110 30 125 Boston (MA) 24 5 39 We can answer the question posed to us if we standardize the snowfall in each city relative to its average and the typical deviation from its average. That is, we calculate what is called a z-score where \\(z=\\dfrac{x-\\mu}{\\sigma}\\). The resulting z-score allows us to identify the relative location of an observation in a data set by telling us how many standard deviation units above or below the Mean a particular value \\(x_{i}\\) falls. The farther away it is, whether above or below, the more unusual the snowfall would have been. As it turns out, \\(z_{Boston} = \\dfrac{39 - 24}{5} = \\dfrac{15}{5} = +3.0\\) and \\(z_{Caribou} = \\dfrac{125 - 110}{30} = \\dfrac{15}{30} = +0.5\\). TABLE 4.11: Z-scores for Comparing Snowfall in Disparate Locations Place Mean Std.Dev. December.2016 z.score Caribou (ME) 110 30 125 +0.5 Boston (MA) 24 5 39 +3.0 That is, Boston had the more unusual snowfall relative to Caribou. Note that if you only focused on the relative means you would have thought both places got 15 inches more than they usually get so they were both hit equally hard. But that approach ignores the fact that their averages and variabilities differ. Once you account for the typical pattern in each city you realize Boston had the worse month of December. 4.6 The Coefficient of Variation Combining the mean and standard deviation also comes in handy when we are trying to figure out which of two variables has more variability, but the variables are not measured on the same metric. In order to compare variability across differently measured variables we rely on the coefficient of variation, a measure of how large the standard deviation is relative to the mean. \\[\\begin{equation} \\tag{4.8} \\text{Coefficient of Variation } = CV = \\left(\\dfrac{\\text{Standard Deviation }}{\\text{Mean }}\\right) \\times 100 \\end{equation}\\] For example, say the average amount of spending a family does in a typical weekend is $200, and this spending has a standard deviation of $100. On average, the typical family has \\(3\\) members but family size tends to vary as well, with a standard deviation of \\(2\\). Is there more variability in family size or in family spending? The coefficient of variation will help us out here. \\(CV_{spending} = \\left( \\dfrac{100}{200}\\right) \\times 100\\) = 50% \\(CV_{size} = \\left( \\dfrac{2}{3}\\right) \\times 100\\) = 66.67% Evidently family size tends to vary far more than does family spending in a typical weekend. 4.7 Symmetric, Skewed and Bi-modal Distributions All data follow a shape and you will often see references to the shape of a distribution, how peaked and spreadout the values of a variable are. A distribution is said to be symmetric if you could carve it in the middle and fold each half perfectly onto the other half. The bell-shaped distribution (see below) is one such distribution but not the only one that is symmetric. FIGURE 4.1: A Bell-Shaped Distribution FIGURE 4.2: A Uniform Distribution In contrast, we may have skewed distributions that have tails that extend more to the right (positively-skewed) or to the left (negatively-skewed), as shown below. FIGURE 4.3: Skewed Distributions A skew means you have unusual data points on the right (positively-skewed) or on the left (negatively-skewed). These patterns are evident from histograms as well, though the direction of the skew isn’t always clear. FIGURE 4.4: Skewed Histograms The peak in each of the preceding graphs reflects the modal value of the variable – the value of \\(x\\) that occurs most frequently. Most distributions may have a single modal value but at times there may be two or more modes. A bimodal distribution is shown below: FIGURE 4.5: A Bimodal Distribution We could, and do calculate skewness as \\[\\begin{equation} \\tag{4.9} skewness = \\dfrac{1}{n}\\sum\\left( \\dfrac{x_i - \\bar{x}}{s} \\right)^3 \\end{equation}\\] where \\(\\bar{x}\\) is the mean, \\(s\\) is the standard deviation, and \\(n\\) is the sample size. In normal and other symmetric distributions skewness \\(=0\\). Negative values of skewness indicate negatively-skewed data while positive values of skewness indicate positively-skewed data. Remember: By skewed left, we mean that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail. Be careful though; with multimodal data the sign of the skewness can be affected. For the preceding plots of skewed distributions, skewness is calculated to be -0.8043625 and 0.7655996, respectively. Often you will see estimates of skewness being accompanied by a measure of kurtosis (the Greek word for bulging) defined as \\[\\begin{equation} \\tag{4.10} \\dfrac{n(n+1)(n-1)}{(n-2)(n-3)} \\dfrac{\\sum \\left(x_i - \\bar{x}\\right)^4}{\\left(\\sum(x_i - \\bar{x})^2 \\right)^2} \\end{equation}\\] The normal distribution has kurtosis \\(=0\\) and is said to be mesokurtic. A negative kurtosis would be indicative of a thin-tailed distribution, also called a platykurtic distribution while a positive kurtosis would be indicative of a fat-tailed distribution, also called a leptokurtic distribution. Again, for the same data for which we calculated skewness, measures of kurtosis would be 0.5144074 and 0.6626601, respectively. 4.8 The Five-number Summary and the Box-plot The five-number summary is composed of the following values: \\[\\text{Minimum value }, Q_1, Md, Q_3, \\text{ Maximum value}\\] Looking at just these five values tells us a lot about the shape of our data. In fact, this five-number summary is used in a very powerful graphic called the box-plot. The box-plot relies on the five-number summary and also identifies extremely unusual values. An example is given below. Here we are looking at the distribution of reading scores of male/female students as reflected in the hsb2 data we saw earlier. FIGURE 4.6: Box-plot of Reading Scores by Sex The darker line in the middle of the box is the median, while the edges of the box are the first quartile \\((Q_1)\\) and the third quartile \\((Q_3)\\), respectively. The lines (called the whiskers) extend leftwards to the minimum value and rightwards to the maximum value. What do these plots tell us? That reading scores are higher for Male students as compared to Female students. Notice that the line in the middle of the box is shifted to the right for Male students. The portion of the box to the right of the medians is larger than the portion of the box to the left. If we look at the five-number summary we will be able to more easily figure out the skew so let us do that. For Males: \\(Min = 31.0; Q_1 = 44.5, Md = 52.0, Q_3 = 63.0, Max = 76.0\\) For Females: \\(Min = 28.0; Q_1 = 44.0, Md = 50.0, Q_3 = 57.0, Max = 76.0\\) Quite clearly, median reading scores are higher for Male students. Now on to understanding how to decipher the skew. Look at the distance between \\(Min\\) and \\(Md\\) for each sex. For Male students, the distance is \\(21\\) and for Female students it is \\(22\\). If you then look at the distance between the \\(Md\\) and \\(Max\\) for each group, you see that for Males it is \\(24\\) and for Females it is \\(26\\). For both groups, the distance between \\(Md\\) and \\(Max\\) is greater than the distance between \\(Min\\) and \\(Md\\), indicating the the distribution is more stretched to the right; hence both Male and Female students’ reading scores are positively-skewed. These box-plots show no outliers (unusual data values) but the next set do (see below). Here we have broken out science scores by the student’s socioeconomic status. Notice the two isolated dots for the High SES group. FIGURE 4.7: Box-plots of Science Scores by Socioeconomic Status In technical terms we define an outlier as any data point that is more than \\(1.5 \\times IQR\\) below \\(Q_1\\) or more than \\(1.5 \\times IQR\\) above \\(Q_3\\). For the High SES group we have the following values of the five number summary: \\[Min = 26.00, Q_1 = 50.00, Md = 58.00, Q_3 = 62.50, Max = 69.00\\] Given these values we know \\(IQR = Q_3 - Q_1 = 62.50 - 50.00 = 12.50\\) and thus \\(1.5 \\times IQR = 1.5 \\times 12.50 = 18.75\\). So anything that is more than 18.75 points below/above \\(Q_1\\)/\\(Q_3\\) will be an outlier. The boundary values then are \\(50.00 - 18.75 = 31.25\\) so any science score smaller than \\(31.25\\) will be an outlier. As it turns out we do have two scores below this – \\(26, \\text{ and } 31\\); hence the two outliers shown. There are no other outliers for the High SES group, and certainly none for the Low and Middle SES groups. Outliers may be present on both sides of a distribution as well as shown in the graph below. FIGURE 4.8: Ohio School District Performance by District Type 4.9 The Power of Summary Statistics Married to Graphical Explorations One of the golden lessons I want you to take away from Chapters 3 and 4 is the maxim that relying only on summary statistics (mean, median, standard deviation, etc.) without graphically exploring the data can lead to terrible mistakes. A classic example comes to us from a stylized example presented by Francis Anscombe in 1973. Anscombe wanted to make the point that if you don’t visualize your data and look for (and deal with) outliers as well, instead rely simply on summary statistics, you could be easily misled. Here is the example in all its glory. The data-set has 11 observations for four pairs of variables, \\(x_i, y_i\\), shown below. The last row in the table is each variable’s mean. TABLE 4.12: Anscombe’s Quartet x1 x2 x3 x4 y1 y2 y3 y4 10 10 10 8 8.04 9.14 7.46 6.58 8 8 8 8 6.95 8.14 6.77 5.76 13 13 13 8 7.58 8.74 12.74 7.71 9 9 9 8 8.81 8.77 7.11 8.84 11 11 11 8 8.33 9.26 7.81 8.47 14 14 14 8 9.96 8.10 8.84 7.04 6 6 6 8 7.24 6.13 6.08 5.25 4 4 4 19 4.26 3.10 5.39 12.50 12 12 12 8 10.84 9.13 8.15 5.56 7 7 7 8 4.82 7.26 6.42 7.91 5 5 5 8 5.68 4.74 5.73 6.89 9 9 9 9 7.50 7.50 7.50 7.50 Note the similarity in the means for the \\(x\\) and the \\(y\\) variables, respectively. If you calculate the correlation between each pair, you will find this to be \\(0.8164205\\) for each pair of \\(x\\) and \\(y\\). So looking just at these numerical summaries it seems the pairs are similar if not downright identical. But if we plot each pair of \\(x\\) and \\(y\\) variables, do the distributions still look similar? FIGURE 4.9: Anscombe’s Quartet This is precisely why you should always start by graphing your data, all of it, even if you only intend to analyze specific variables, before doing any sort of numerical calculations of means, correlations, variances, etc., leave alone proceeding to do any statistical test. If you fail to follow this mantra, be prepared to be embarrassed. 4.10 Outliers, Outliers, What do I do with them? If you encounter outliers, the overriding question has to be the next step: Do you delete them or do you keep them? Deleting data points is never a good idea unless you have good reason to do so. The only good reason might be one of measurement error, in that the data were entered incorrectly or you have good reason to believe whoever recorded the measurement did so with error. If you see a skewed distribution with outliers, and there is no good reason to believe this is because of measurement error, then try a transformation. Try \\(x^* = x^3\\) to reduce extreme negative skewness Try \\(x^* = x^2\\) to reduce negative skewness Try \\(x^* = x^{\\frac{1}{2}}\\) to reduce positive skewness Try \\(x^* = -\\dfrac{1}{\\sqrt{x}}\\) to reduce extreme positive skewness Try \\(x^* = -\\dfrac{1}{x}\\) to reduce even more extreme positive skewness If \\(x\\) can assume a value of \\(0\\), add a small constant \\(c\\), such that \\(x^* = x + c\\). Folks often use \\(c = 0.01\\) or even \\(c = 1\\). Then try options (1) through (5) depending upon the direction and strength of the skewness You will also see the natural logarithm used to transform data. It only works if \\(x\\) is strictly \\(\\geq 1\\) so if \\(x \\text{ is } \\geq 0\\), be sure to add a constant to \\(x\\) before trying the natural logarithm. The reason many shy away from it is because it is difficult to interpret the results. An easy fix to this problem is to use the antilogarithm, i.e., \\(e^{x^*}\\). This same rule, of flipping the final result back into the original metric also works for the other transformation discussed here. Specifically, If you used \\(x^* = x^3\\) to reduce extreme negative skewness, convert the final answer via \\((x^*)^\\frac{1}{3}\\) If you used \\(x^* = x^2\\) to reduce negative skewness, convert the final answer via \\((x^*)^\\frac{1}{2}\\) If you used \\(x^* = x^{\\frac{1}{2}}\\) to reduce positive skewness, convert the final answer via \\((x^*)^2\\) If you used \\(x^* = -\\dfrac{1}{\\sqrt{x}}\\) to reduce extreme positive skewness, convert the final answer via \\(\\dfrac{1}{(x^*)^2}\\) If you used \\(x^* = -\\dfrac{1}{x}\\) to reduce even more extreme positive skewness, convert the final answer via \\(\\dfrac{1}{-x^*}\\) 4.11 Beware the flawed rule Many textbooks tend to indicate that the mean is always pulled away from the median (some also extend this to the mode) in skewed distributions with a left-skewed distribution with \\(mean &lt; median\\) and a right-skewed distribution with \\(mean &gt; median\\). This is not always true for discrete variables, and the rule can fail with continuous variables that are distributed with one long tail but the other tail is heavy. Bimodal and multimodal distributions of continuous variables are equally susceptible. In brief: Do not rely on this rule of thumb. 4.12 Chapter 4 Practice Problems Problem 1 Working with the Boston Marathon (2016) data used in the previous chapter, calculate the following: Mean finish time Median finish time Variance of finish times Standard Deviation of finish times Range of finish times The first and the third quartiles of finish times The interquartile range of finish times The skewness of finish times Problem 2 Construct gender-specific box-plots of runners’ times. Are the distributions skewed or symmetric for each gender? If skewed, in what direction? Are there outliers in each gender’s distribution? On which side(s) of the distribution? Problem 3 What is the five-number summary of finish times? Problem 4 Calculate the statistics listed in Problem 1 for each gender. Do the estimates of skewness you calculated here support your conclusions about skewness from your work in Problem 2? Problem 5 What is the coefficient of variation for male and female runners’ finish times, respectively? What do these estimates tell you about finish times for male and female runners, respectively? Problem 6 Ignoring gender differences, if you see skewed distributions in finish times with outliers, try to transform the data to reduce the skewness. What transformations work? Explain why you chose the transformation(s) you did and show (via a box-plot and estimates of skewness) that the transformation(s) worked. Problem 7 Using the data-set employed in Problem 5 of Chapter 2, calculate and report the average number of hours worked by low/medium/high salary employees. Explain why you chose the measure of average that you did. Also display the distribution of hours worked for each salary group via an appropriate graphic. What patterns do you see in this graphic? Problem 8 Load the EPA data used for Practice Problem 3 in Chapter 2 and answer the following questions. (a) Calculate the average annual fuel cost in 2017. Note that the variable is fuelCost08 and is based on 15,000 miles, 55% city driving, and the price of fuel used by the vehicle. (b) Of all the vehicles in the data-set, for the year 2017, what is the most common drive axle type? Note that the variable is drive. (c) Of all the vehicles in the data-set, for the year 2017, what is the most common vehicle size class? Note that the variable is VClass. (d) For the year 2017, construct a grouped frequency table &amp; histogram of youSaveSpend. Note that this variable measures the vehicle-related savings/spending over 5 years compared to an average car (in US$). Savings are positive; a greater amount spent yields a negative number. It would be remiss of us not to recognize the existence of a very passionate and vigorous Null Hypothesis Significance Testing (NHST) controversy. While the details of the controversy are too advanced for our present purposes, here are some articles you should read: The Truth Wears Off: Is there something wrong with the scientific method? and Do We Really Need the S-word?↩︎ You will also see these formulas listed as \\(\\dfrac{\\left(n + 1\\right)}{4}\\) for the position of \\(Q_1\\) and \\(\\dfrac{3\\left(n + 1\\right)}{4}\\) for the position of \\(Q_3\\).↩︎ Unusual compared to what seems to be typical for the city, that is.↩︎ "],["prob.html", "Chapter 5 Probability 5.1 Basic Concepts and Terminology of Probability Theory 5.2 Counting Rules with Permutations and Combinations 5.3 Assigning Probabilities to Events 5.4 The Complement of an Event 5.5 Mutually Exclusive Events 5.6 The Addition Rule for Mutually Exclusive Events 5.7 Addition Rule for Non-Mutually Exclusive Events 5.8 Independent Events and the Multiplication Rule 5.9 Decision Trees 5.10 Conditional Probability 5.11 Dependent Events 5.12 Bayes’ Theorem 5.13 Key Things to Remember 5.14 Chapter 5 Practice Problems", " Chapter 5 Probability Probability theory is one of the hardest fields to master. Most people often get tripped up by what should be simple calculations, thinking that the answer for how often something will happen has to have a fantastically rare probability of occurrence. For example, on the first anniversary of 9/11, the winning pick-three number in the second draw was 9-1-1. This lit up the news of course because it seemed to be an astonishing coincidence. As it turns out, the probability of this happening was very high, exactly \\(\\dfrac{1}{500}\\). Several pages have been devoted to the Monty Hall Problem wherein the contestant sees three doors, two with a goat behind them and one with a car, and picks one. The host opens one of the other doors that turns out to have a goat behind it. The contestant is given a choice, should he/she stick with the door they chose or should he/she switch their choice to the unopened door? Probability also comes in handy with party tricks: With 23 people in a room there is a 50-50 chance of two people having the same birthday! In a more serious vein, NASA engineers knew that the probability of a Challenger disaster on flight day was unacceptably high (13%) but most folks never understood this before the shuttle launch was given the green light. How probability works its magic in any of these examples would be elementary if you understood the core principles of probability theory. However, remember, we are not looking to become probability theorists. Not at all. Our task is simpler, to grasp the essence of some probability mechanics so that we can build a bridge between the samples we gather and the population our samples should represent. 5.1 Basic Concepts and Terminology of Probability Theory In statistics, we think of every study as an experiment. Not in the sense of a laboratory experiment but in terms of a real-world experiment that nature is carrying out without revealing to us all the truths. For example, a district tries to boost literacy by spending a lot of money on programs that claim to have every third grader proficient in English language arts. The district has just carried out an experiment whose outcomes are unknown (i.e., will the program work, worsen the situation, or have no impact?). A prison implements a prisoner rehabilitation program with wrap-around skills; will it have the desired effect? Patients battling Parkinson’s disease are given a new drug designed to control tremors; will it work as designed? Experiments surround us, every day of our lives and with consequences big and small. All experiments have outcomes that we can identify a priori (before the experiment is conducted). For example, a drug either makes the patient better (outcome 1), worse (outcome 2), or has no impact (outcome 3). If we could list all outcomes likely in an experiment we would have the sample space denoted by the symbol \\(S\\). Let us run a few simple experiments. I toss a coin knowing it can come up Heads or Tails. Only two outcomes are likely in this experiment and so the sample space can be written as \\(S = \\{Heads \\text{, } Tails\\} = \\{H,T\\}\\) I roll a six-sided dice knowing it can come up with the face showing one of the following numbers: \\(S = \\{1,2,3,4,5,6\\}\\). A state trooper pulls over a driver she thinks is driving while intoxicated and administers the field sobriety test. The driver is either sober or above the legal limit. That is, \\(S=\\{Sober \\text{, } Drunk\\}\\) A politician runs for elected office. She either wins or loses; \\(S=\\{Win \\text{, } Lose\\}\\)8 Each outcome of an experiment is a sample point, with all likely outcomes together making up the sample space. One or more sample points of the sample space could make up an event. For example, we could define event \\(A\\) as seeing an odd-number show up on the dice when it is rolled once. That is, \\(A=\\{1, 3, 5\\}\\). Or we could define an event \\(B\\) as having a perfect square show up on the face of the dice, i.e., \\(B=\\{1, 4\\}\\). TABLE 5.1: Some Experiments and their Outcomes Experiment Outcomes Coin Toss (Head, Tail) Sales Call (Sale, No Sale) Roll a Die ({1,2,3,4,5,6}) Product Test (Defective Not defective) Health Campaign (Behavior modified Not) Green Eyes (Yes, No) Each outcome occurs with some probability. For example, we all recognize that if I have a fair coin that I toss, it could either land Heads or Tails. Since it is a fair coin, not biased towards Heads or Tails, only one of two likely outcomes could occur in a single toss. Therefore, there is a 50:50 chance of seeing a Heads or a Tails in a coin toss. However, you could flip a coin ten times and see more (or all) of Heads or Tails. How does that make for a 50:50 chance then? It does not unless you think in a very specific way. In particular, in statistics, we define probability as a numerical measure of the likelihood that an event will occur. It is, essentially, the proportion of times the event would occur if we repeated the random trial under identical conditions an infinite number of times. See the example below where I toss a coin 10 times, 100 times, 1,000 times, and then 10,000,000 (ten million) times, counting in each case the proportions of Heads and Tails, respectively. FIGURE 5.1: Simulated Coin Flips It is only in the case of 10 million coin flips that I approach anything close to a 50:50 split between the proportion of Heads and the proportion of Tails. Even here the split actually is 50.02% Heads and 49.97% Tails. What is the point of this demonstration? That probability of any outcome only makes sense in the sense of an infinite number of identical trials/experiments. In the coming chapters I will keep pointing out how to see your single sample “as if” you had tossed it back into the population and then drawn a random sample of exactly the same size, done your analysis, tossed the sample back, drawn another random sample of exactly the same size, and on and on until all unique samples of identical size had been drawn and analyzed. Before we move on, though, notice that we can define probability as a proportion: the ratio of the number of times an outcome occurs to the sample space. By definition, the probability of any event must lie between 0 and 1, i.e., \\(0 \\le P(Event) \\le 1\\). If an event has a probability of 1 then it always occurs. Similarly if an event has a probability of 0 it never occurs. Many outcomes tend to have well-defined probabilities. For instance, if I toss a fair coin once, I know a priori that \\(P(Heads)=P(Tails)=\\dfrac{1}{2}=0.50\\). Similarly, if it is a very tight political contest between three candidates and no polls are available to forecast the outcome, I might as well assume that \\(P(\\text{Candidate A wins})=P(\\text{Candidate B wins})==P(\\text{Candidate C wins})\\) so that the probability of Candidate A or B or C winning is \\(\\dfrac{1}{3}=0.33\\). 5.2 Counting Rules with Permutations and Combinations Calculating the number of likely outcomes is easy when you have a simple experiment such as flipping a coin once. But what if you were flipping 10 coins 10 times each? Counting rules simplify these calculations a great deal. Consider tossing one coin twice, and knowing that the possible outcomes are four in total: \\(S=\\{H,H\\},\\{H,T\\},\\{T,H\\},\\{T,T\\}\\). Generally, in a multi-step experiment with \\(k\\) sequential steps with \\(n_{1}\\) outcomes in Step 1, \\(n_{2}\\) outcomes in Step 2, \\(n_{3}\\) outcomes in Step 3, \\(\\ldots\\) and \\(n_{k}\\) outcomes in Step \\(k\\), the total number of experimental outcomes is given by: \\[(n_{1})(n_{2})(n_{3})\\cdots(n_{k})\\] For example, tossing one coin twice yields \\((n_{1})(n_{2})=(2)(2)=4\\) outcomes while tossing one coin six times yields \\((n_{1})(n_{2})(n_{3})(n_{4})(n_{5})(n_{6})=(2)(2)(2)(2)(2)(2)=64\\) outcomes. In all of these cases of coin flips we have said nothing about whether the order matters at all; we do not care whether Heads or Tails shows up first. Sometimes you may have to select \\(n\\) objects from a larger pool of \\(N\\) objects. In these situations, we can calculate the number of experimental outcomes possible via \\[ C^{N}_{n} = \\left(^{N}_{n}\\right) = \\frac{N!}{n!(N-n)!} \\] You may not remember this from the good old high school days but \\(!\\) stands for factorial such that \\(N! = N(N-1)(N-2)\\cdots(2)(1)\\) and \\(n! = n(n-1)(n-2)\\cdots(2)(1)\\). Note that \\(0! = 1\\), \\(3! = 3 \\times 2 \\times 1 = 6\\), and \\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\). How is this useful? For instance, I work in the quality control department of a manufacturing plant that makes noise cancelling headphones. My job is to randomly select two headphones out of a batch of five and check for defects. In how many ways can I select two headphones from a batch of five headphones? Let us use the formula: \\[ C^{N}_{n} = \\left(^{N}_{n}\\right) = \\frac{N!}{n!(N-n)!} \\] \\[C^{5}_{2} == \\frac{5!}{2!(5 - 2)!} = \\frac{5 \\times 4 \\times \\cancel{3} \\times \\cancel{2} \\times \\cancel{1}}{(2 \\times 1) (\\cancel{3} \\times \\cancel{2} \\times \\cancel{1})} = \\frac{5 \\times 4}{2 \\times 1} = \\frac{20}{2} = 10\\] You could label the headphones A, B, C, D, and E, and then count the number of combinations: AB, AC, AD, AE, BC, BD, BE, CD, CE, and DE until no unique sample is left; this is the sample space. This would be a very tedious and potentially error-prone method of manually calculating the result if either \\(n\\) or \\(N\\) were large; the formula helps ease our task a great deal. Before we proceed any farther, note that we draw no distinction here between whether A is the first headphone or the second or the third. That is, AB counts the same as BA and hence BA does not show up as an outcome. These situations, where the order of the outcomes is of no interest are called combinations but if the order matters, then we have permutations. Here is an example of permutations in action. Assume now that we do want to draw distinction between cases such as AB versus BA, AC versus CA, and so on. In how many ways could we gather two headphones out of five? The formula is slightly different here: \\[ P_{n}^{N} = n!\\left(^{N}_{n}\\right) = \\frac{N!}{(N-n)!} \\] \\[ P^{5}_{2} = \\frac{5!}{(5-2)!} = \\frac{5!}{3!} = \\frac{(5)(4)(3)(2)(1)}{(3)(2)(1)} = \\frac{(5)(4)\\cancel{(3)}\\cancel{(2)}\\cancel{(1)}}{\\cancel{(3)}\\cancel{(2)}\\cancel{(1)}} = 20 \\] Spelling these out would give us AB, BA, AC, CA, AD, DA, AE, EA, BC, CB, BD, DB, BE, EB, CD, DC, CE, EC, DE, and ED; this is the Sample Space. 5.3 Assigning Probabilities to Events Thus far we’ve seen how to use counting rules to establish the Sample Space. Now let us think about the probabilities associated with each outcome. We can assign probabilities to outcomes so long as we use two rules: For an event \\(E_{i}\\), \\(0\\leq P(E_{i})\\leq 1\\) Given \\(n\\) outcomes, \\(P(E_{1})+P(E_{2})+P(E_{3})+\\cdots + P(E_{n}) = 1\\) For example, in tossing a fair coin, \\(P(H) = 0.5; P(T) = 0.5\\). Therefore \\(P(H)+P(T)=1\\). Likewise, in tossing a fair dice, \\(P(1) = \\frac{1}{6}; P(2) = \\frac{1}{6}; \\cdots P(6) = \\frac{1}{6}\\). Therefore, \\(P(1)+P(2)+\\cdots+P(6)=1\\). This method of assigning probabilities is known as the classical method, i.e., all outcomes are equally likely. Assume a clerk in the student health center at a university tracks how many patients are on the waiting list at 9 AM on 20 successive days. These data are given below: TABLE 5.2: Example of the Relative Frequency Method No. Waiting No. of Days Proportion 0 2 2/20 = 0.10 1 5 5/20 = 0.25 2 6 6/20 = 0.30 3 4 4/20 = 0.20 4 3 3/20 = 0.15 Total 20 20/20 = 1.00 It seems, based on his observations, that on any given day he should expect 2 patients on the waiting list since this outcome has the highest relative frequency of \\(0.30\\). I want you to note how the data and calculating frequencies and relative frequencies has lead to this conclusion; they have enabled us to answer what would be most likely if we picked a day at random. Here is another example, where a company is looking to build a bridge over an expanse of water and gathering data on how long the two phases – design versus construction – have taken for similar sized bridges built in the recent past. These data are shown below: TABLE 5.3: Bridge-building and Relative Frequencies Design Construction Time/Phase Total Time Frequency P(Time) 2 6 (2,6) 8 6 6/40 = 0.15 2 7 (2,7) 9 6 6/40 = 0.15 2 8 (2,8) 10 2 2/40 = 0.05 3 6 (3,6) 9 4 4/40 = 0.10 3 7 (3,7) 10 8 8/40 = 0.20 3 8 (3,8) 11 2 2/40 = 0.05 4 6 (4,6) 10 2 2/40 = 0.05 4 7 (4,7) 11 4 4/40 = 0.10 4 8 (4,8) 12 6 6/40 = 0.15 Note that most projects seem to have taken three months for design and seven months for construction; hence the company feels this is the most likely outcome for its own project. In some instances, we may have no hard data to go by, only our own subjective perceptions (right or wrong) of what might happen. Here, for instance, we have a couple bidding on a house. They each have their own estimates of whether their bid will be accepted or denied. TABLE 5.4: Subjective Probabilities Person P(Acceptance) P(Rejection) Sum Pat 0.8 0.2 1 Chris 0.6 0.4 1 Subjective probabilities are used more often than not in international security. Today we might think of it in terms of a drone strike on a potentially high value target based on hazy intelligence but a classic example comes from the infamous Christmas bombing of Hanoi. Here is an extract from Politico though you should read the original piece in its entirety to appreciate the value of what has come to be known as “The Madman Theory.” In the summer of 1969, Nixon knew he needed to do something bold about Vietnam. He’d been elected with a promise to end the war, but months later, the conflict continued to consume his presidency, and peace was nowhere in sight. When the Paris peace talks had collapsed earlier that summer, the North Vietnamese had declared that they’d sit silently “until the chairs rot.” Nixon and Kissinger sought to restart the negotiations by pushing the Soviets to lean on the North Vietnamese. And so, Nixon turned to what came to be known as the “Madman Theory” —- a game-theory based approach he had witnessed as Dwight Eisenhower’s vice president that was meant to raise uncertainty in the Soviet mind about whether Nixon would launch his nuclear weapons if provoked. The White House needed to convince the Soviets that Nixon would resort to anything — including a nuclear attack — to get peace in Vietnam. As Defense Secretary Melvin Laird said later, “He never [publicly] used the term ‘madman,’ but he wanted adversaries to have the feeling that you could never put your finger on what he might do next. Nixon got this from Ike, who always felt this way.” Where game-theory comes into the story is in the indisputable fact that reasonable individuals disagreed whether the North Vietnamese would return to the negotiating table and sue for peace after their sudden withdrawal from the talks on December 13, 1972. Some felt that the Russians would bring the North Vietnamese back to the table but others, including Nixon and Kissinger, thought otherwise. Consequently, on December 18, Operation Linebacker II began, lasting for two weeks and involving 741 B-52 sorties that rained 20,000 tons of munitions on Hanoi and Haiphong, knocking out almost all of the electricity supply grid and killing 1,600 civilians. Regardless of who held more accurate subjective beliefs about the North Vietnamese’s intentions, by December 29 they had agreed to resume the talks and the Vietnam War ended soon thereafter. Thus far we have covered a good bit of ground, putting into place several rules of probability theory, but not really applied them to concrete problems. Let us do so, starting with a simple example. 5.3.1 Example 1: Venture Capital Funding Of 2,374 venture capital awards disbursed nationwide in 2016, 1,434 went to CA, 390 to MA, 217 to NY, and 112 to CO. Some 22% went to companies in early stages and 55% to expanding companies. What is P(Company from CA)? \\(P(California)=\\frac{1434}{2374}=0.60\\) What is P(Company from other state)? Number that went to other than these 4 states \\(=2374 - \\{1434+390+217+112)=221\\). So, P(Other states) \\(=\\frac{221}{2374} = 0.09\\). What is P(Company not in early stage)? P(Not in early stage) \\(=1-0.22 = 0.78\\) P(How many MA companies were in early stages, assuming early stage companies were evenly distributed across states)? Approximate number of MA companies in early stages \\(=(0.22)(390) \\approx 86\\) If the total amount was $32.4 million, how much went to CO? Assume fund amounts were distributed in proportion to the relative distribution of awards. Then, \\(=\\frac{112}{2374}(\\$32.4 \\text{ billion}) = \\$1.53 \\text{ billion}\\) 5.3.2 Example 2: Powerball Powerball played twice per week in 23 states, VI, and DC. Player must buy $1 ticket and pick five numbers from \\(1 \\ldots 53\\) and one Powerball number from \\(1 \\ldots 42\\). Lottery officials draw (i) 5 White balls out of drum with 53 White balls, and (ii) 1 Red ball from drum with 42 Red balls. The winner must match 5 numbers on White balls (any order) and the number on the Red Powerball. In Augusts 2001, 4 winners shared $295 million by matching \\(8, 17, 22, 42, 47\\) plus Powerball number \\(21\\). Minor prizes of $100,000 given if 5 White ball numbers matched. In how many ways can five numbers be selected? \\[C^{53}_{5}=\\dfrac{53!}{5!(53-5)!}=\\dfrac{53!}{5!(48)!} = \\dfrac{(53)(52)(51)(50)(49)}{(5)(4)(3)(2)(1)}=2,869,685\\] What is P(winning $100,000)? \\(=\\dfrac{1}{2,869,685}\\) What are the odds of picking the Red Powerball? \\[C^{42}_{1}=\\dfrac{42!}{1!(42-1)!}=\\dfrac{42!}{41!} = 42; \\therefore \\mbox {Odds of picking Red ball} = \\dfrac{1}{42}\\] What are the odds of winning the Powerball jackpot? \\(P(A) \\mbox{and} P(B) = P(A) \\times P(B) = \\left(\\dfrac{1}{2869685}\\right)\\times\\left(\\dfrac{1}{42}\\right)=\\dfrac{1}{120,526,770}\\) 5.3.3 Example 3: Rolling Two Dice Two dice are rolled and we are interested in the sum of face values showing on the 2 dice. Possible outcomes are (1,1), (1,2), (1,3), (1,4), (1,5), (1,6) … (6,1), (6,2), (6,3), (6,4), (6,5), (6,6). In other words, \\(C^{6}_{1} \\times C^{6}_{1} =(6)(6)=36\\) outcomes are likely. TABLE 5.5: Sum of Two Dice 1 2 3 4 5 6 1 2 3 4 5 6 7 2 3 4 5 6 7 8 3 4 5 6 7 8 9 4 5 6 7 8 9 10 5 6 7 8 9 10 11 6 7 8 9 10 11 12 Note that the first row with columns 1, 2, 3, 4, 5, and 6 are the numbers that show up on a roll of Dice 1 and the first column with rows 1, 2, 3, 4, 5, and 6 are the numbers that show up on a roll of Dice 2. What is P(value of 7)? \\(P(7)=\\dfrac{\\{(1,6), (6,1), (2,5), (5,2), (3,4), (4,3)\\}}{36} = \\dfrac{6}{36}=\\dfrac{1}{6}\\) What is P(value \\(\\geq 9\\))? \\(P(\\geq 9)=\\dfrac{10}{36}=\\dfrac{5}{18}\\) Will the sum of the two dice show even values more often than odd values? No because P(Odd) = P(Even) = \\(\\dfrac{18}{36} = \\dfrac{1}{2}\\) How did you assign probabilities? Classical, because each outcome has an identical probability of occurring. 5.3.4 Example 4: Fortune 500 Companies The table below shows the registration of Fortune 500 companies in some states. TABLE 5.6: Headquarters of Fortune 500 Companies State Headquartered No. of Fortune 500 Cos. Proportion New York 56 0.112 California 53 0.106 Texas 43 0.086 Illinois 37 0.074 Ohio 28 0.056 Pennsylvania 28 0.056 If I pick a company at random, What is P(NY)? \\(P(NY)=\\frac{56}{500}=0.112\\) What is P(TX)? \\(P(TX)=\\frac{43}{500}=0.086\\) What is P(in any of these six states)? \\(=\\frac{(56+53+43+37+28+28)}{500}=\\frac{245}{500}=0.49\\) 5.4 The Complement of an Event Given an event A, its complement is defined as the event consisting of all sample points that do not belong to (i.e., are not in) event A, and is denoted as \\(A^c\\). \\[\\begin{eqnarray*} P(A) + P(A^c) = 1 \\\\ \\therefore P(A) = 1 - P(A^c) \\\\ \\therefore P(A^c) = 1 - P(A) \\end{eqnarray*}\\] Toss a coin once: \\(P(H) = 0.5; P(H^c) = 1- P(H) = 1-0.5 = 0.5\\) Roll two dice: \\(P(\\text{value } \\geq 9) = \\dfrac{5}{18}\\) \\(P(\\text{value } &lt; 9) = 1 - P(\\text{value } \\geq 9) = 1 - \\dfrac{5}{18} = \\dfrac{13}{18}\\) Why is this notion of the complement of an event useful? Because given that the probabilities of the event happening and not happening must sum to unity i.e., \\((1)\\), given one probability we can always calculate the other. 5.5 Mutually Exclusive Events Two events are mutually exclusive if both cannot occur simultaneously. That is, if event A occurs then event B cannot occur, and vice-versa. Say I toss a coin once. It can only come up Heads or Tails. Let, for example, \\[\\begin{eqnarray*} P(A) = Heads; P(B) = Tails \\\\ P(A \\text{ and } B) = 0 \\\\ \\ldots \\text{ because there is no overlap ... A and B are mutually exclusive} \\end{eqnarray*}\\] Similarly, say I roll a dice once. Let, for example, \\[\\begin{eqnarray*} P(A) = \\{1, 3, 5\\}; P(B) = \\{2, 4, 6\\} \\\\ P(A \\text{ and } B) = 0 \\\\ \\ldots \\text{ because there is no overlap ... A and B are mutually exclusive} \\end{eqnarray*}\\] What if I again roll a dice once and this time \\[\\begin{eqnarray*} P(A) = \\{2, 4, 6\\}; P(B) = \\{1, 4\\} \\\\ P(A \\text{ and } B) \\neq 0 \\\\ \\ldots \\text{ because there is an overlap ... A and B are not mutually exclusive} \\end{eqnarray*}\\] Knowing that two events are mutually exclusive (or not) is critical in order to calculate the probability of some outcome that involves more than one event. You can see this rule in action vis-a-vis the addition rules for mutually exclusive versus non-mutually exclusive events. 5.6 The Addition Rule for Mutually Exclusive Events For two mutually exclusive events, A and B, the probability that either A or B occurs is given by \\(P(A \\text{ or } B) = P(A) + P(B)\\). The rule also extends to more than 2 events so long as they are all mutually exclusive events. A dice is rolled once. What is \\(P(3 \\text{ or more})\\)? \\[\\begin{eqnarray*} = P(3) + P(4) + P(5) + P(6) \\\\ = \\dfrac{1}{6} + \\dfrac{1}{6} + \\dfrac{1}{6} + \\dfrac{1}{6} \\\\ = \\dfrac{4}{6} \\\\ = \\dfrac{2}{3} \\end{eqnarray*}\\] What is \\(P(\\text{not rolling 3 or more})\\)? … \\(1 - P(3 \\text{ or more}) = 1 - \\dfrac{2}{3} = \\dfrac{1}{3}\\) 5.7 Addition Rule for Non-Mutually Exclusive Events For non-mutually exclusive events we calculate the probability that event A or B occurs as \\(P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B)\\) Assume on a typical day in a plant, of 50 workers 5 complete the work late, 6 assemble a defective product, and 2 both complete the work late and produce a defective product Let L = Late completion. Then, \\(P(L)=\\dfrac{5}{50} =0.10\\) Let D = Defective product. Then, \\(P(D)=\\dfrac{6}{50} =0.12\\) \\(P(L \\text{ and } D) = \\dfrac{2}{50}=0.04\\) What is the probability that a randomly selected worker either is late or produces a defective product? \\(P(L \\text{ or } D)=P(L)+P(D)-P(L \\text{ and } D) = 0.10 + 0.12 - 0.04 = 0.18\\) 5.8 Independent Events and the Multiplication Rule We also take into consideration whether two or more events are independent or dependent, i.e., whether the probability of one event occurring is not at all influenced by whether the other event(s) has/have occurred or not or if it is conditioned by the other event(s) occurrence/non-occurrence. Specifically, two events, A and B, areindependent events if P(A) is not influenced by whether event B has occurred or not (and vice-versa). For instance, (1) rolling a 4, and rolling a 1 on a second roll of the same dice, and (2) picking the Ace of Spades from a fair deck of 52 cards, replacing it, and then choosing the Ace of Spades again are both independent events; what happens in the first step does not influence what happens in the second step. If two events A and B are independent, then the probability that both A and B occur is given by \\(P(A \\text{ and } B) = P(A) \\times P(B)\\). For example, for (1) above \\(P(A \\text{ and } B) = \\dfrac{1}{6} \\times \\dfrac{1}{6} = \\dfrac{1}{36}\\) for (2) above \\(P(A \\text{ and } B) = \\dfrac{1}{52} \\times \\dfrac{1}{52} = \\dfrac{1}{2704}\\) Here is a substantive example: Assume we are told that for a randomly chosen adult, \\(P(\\text{smoking})=0.17\\) and \\(P(\\text{high blood pressure}) = 0.22\\). If smoking and high blood pressure are independent, what is \\(P(\\text{smoking and high blood pressure})\\)? This would be \\(P(smoking \\text{ and } blood \\text{ } pressure) = P(A) \\times P(B) = 0.17 \\times 0.22 = 0.037\\). 5.9 Decision Trees Trees are handy ways to depict sequential events and their probabilities and often used to understand sequential outcomes. Game theorists, for example, put decision trees to good use. In the example below we see the case of a tree built with the question of two-child families. Specifically, the tree outlines for us the probabilities of a family having both kids of a given sex versus having one kid of each sex. What is \\(P(1 \\text{ Boy and } 1 \\text{ Girl})\\)? \\(=(0.249856 + 0.249856) = 0.499712\\) What is \\(P(\\text{at least } 1 \\text{ Girl})\\)? \\(=(1 - 0.262144) = 0.737856\\). This is also \\(1 - P( \\text{both are Boys})\\) What is \\(P(\\text{at least } 1 \\text{ Boy})\\)? \\(=(1 - 0.238144) = 0.761856\\). This is also \\(1 - P(\\text{both are Girls})\\) What is \\(P(\\text{both are of the same sex})\\)? \\(=(0.262144 + 0.2381444) = 0.5002884\\) 5.10 Conditional Probability The conditional probability of an event is the probability of that event occurring given that a condition is met (i.e., some other event is known to have occurred). Conditional probabilities are denoted as \\(P(A | B)\\) (i.e., the probability of A given that B has occurred) and \\(P(A | B) = \\dfrac{P(A \\text{ and } B)}{P(B)}\\). Similarly, we have \\(P(B | A) = \\dfrac{P(A \\text{ and } B)}{P(A)}\\), (i.e., the probability of B given that A has occurred). For example, let B be an event of getting a perfect square when a dice is rolled. Let A be the event that the number on the dice is an odd number. What is \\(P(B|A)\\)? The Sample Space is \\(S=\\{1,2,3,4,5,6\\}\\); \\(A =\\{1,3,5\\}\\); \\(B=\\{1,4\\}\\). Then, \\(P(A \\text{ and } B)=\\dfrac{1}{6}; P(A)=\\dfrac{1}{2}; P(B)=\\dfrac{1}{3}\\) and hence \\(P(B|A)=\\dfrac{P(A \\text{ and } B)}{P(A)}=\\dfrac{\\dfrac{1}{6}}{\\dfrac{1}{2}}=\\dfrac{1}{6} \\times \\dfrac{2}{1}=\\dfrac{2}{6}=\\dfrac{1}{3}\\) For dependent events, the multiplication rule that determines the probability that both A B occur becomes: \\[\\begin{array}{l} P(A \\text{ and } B) = P(A) \\times P(B | A), \\text{ and} \\\\ P(A \\text{ and } B) = P(B) \\times P(A | B) \\end{array}\\] Here is a specific example that pulls together various elements of basic probability theory we have covered thus far, and especially so the concept of conditional probability. 5.10.1 The Gender-bias Example Say we have a city police department and there have been claims that men and women are not promoted at the same rate. Let us assume that when candidates are eligible for promotion they are equally qualified, regardless of sex. You are hired to investigate whether this claim of a gender bias has any merit to it. You ask for data on all officers currently serving in the city and for each officer whether they were promoted or not. The data are shown below. TABLE 5.7: An Example of Conditional Probability Action Men Women Total Promoted 288 36 324 Denied Promotion 672 204 876 Total 960 240 1200 If Men and Women had the same probability of being promoted each group should have \\(P(A)=\\dfrac{324}{1200}= 0.27\\). This means we should have \\(P(A \\text{ and } M) = P(A \\text{ and } W) = 0.27\\). We can now ask: What is the probability that an officer is promoted given that the officer is a man? \\(P(A|M) = \\dfrac{P(A \\text{ and } M)}{P(M)} = \\dfrac{288/1200}{960/1200} = \\dfrac{288}{960} = 0.30\\). Similarly, we might ask: What is the probability of being denied a promotion given that the officer is a woman? \\(P(A^{c}|W) = \\dfrac{P(A^{c} \\text{ and } W)}{P(W)} = \\dfrac{204/1200}{240/1200} = \\dfrac{204}{240} = 0.85\\) TABLE 5.8: Probabilities for the preceding table Action Men Women Total Promoted P(.) = 0.24 P(.) = 0.03 P(.) = 0.27 Denied Promotion P(.) = 0.56 P(.) = 0.17 P(.) = 0.73 Total P(.) = 0.80 P(.) = 0.20 P(.) = 1.00 What would you conclude and report back to the city? Unfortunately, that female officers do not seem to be promoted at the same rate as are male officers. Notice how the notion of conditional probability and independent events allowed you to tackle this question! 5.11 Dependent Events Many events are not independent of one another; the odds of event A change if event B has occurred (and vice versa). Take the Nasonia’s fascinating behavior, for example. The jewel wasp Nasonia vitripennis is a parasite laying its eggs on the pupae of flies. The larval Nasonia hatch inside the pupal case, feed on the live host, and grow until they emerge as adults from the now dead, emaciated host. Emerging males and females, possibly brother and sister, mate on the spot. Nasonia females have a remarkable ability to manipulate the sex of the eggs that they lay; if they fertilize the egg with stored sperm the offspring will be a female. When a female finds a fresh host (i.e., not parasitized), she lays mainly female eggs and a few sons needed to fertilize all her daughters. If the female finds the host to be parasitized she produces a higher proportion of sons. Thus the state of the host (parasitized or not) and the sex of an egg are dependent events. Let the probability a host already has eggs be = 0.20. If it is a fresh host, the female lays a male egg with a probability of 0.05 and a female egg with a probability of 0.95. If the host already has eggs the female lays a male egg with a probability of 0.90 and a female egg with a probability of 0.10. What is the probability that an egg, chosen at random, is male? What is \\(P(\\text{egg is male})\\)? … \\(0.18 + 0.04 = 0.22\\) What is \\(P(\\text{egg is female})\\)? … \\(0.02 + 0.76 = 0.78\\) The Law of Total Probability stipulates that the total probability of an event \\(A\\) is given by \\(P(A) = \\left[P(A|B) \\times P(B)\\right] + \\left[ P(A|B^c) \\times P(B^c)\\right]\\) The Multiplication Rule: The probability of both of two events occurring is given by \\(P(A \\text{ and } B) = P(A) \\times P(B | A)\\). If the two events are independent then we know that \\(P(A \\text{ and } B) = P(A) \\times P(B)\\). So what is \\(P(\\text{egg is male})\\)? Well, we don’t know if the host is parasitized so … \\(P(\\text{egg is male}) = P(\\text{host parasitized}) \\times P(\\text{egg is male } | \\text{ host parasitized}) + P(\\text{host not parasitized}) \\times P(\\text{egg is male } | \\text{ host is not parasitized})\\) \\(P(\\text{egg is male}) = (0.20 \\times 0.90) + (0.80 \\times 0.05) = 0.22\\) What is \\(P(\\text{egg is female})\\)? We don’t know if the host is parasitized so … \\(P(\\text{egg is female}) = P(\\text{host parasitized}) \\times P(\\text{egg is female } | \\text{ host parasitized}) + P(\\text{host not parasitized}) \\times P(\\text{egg is female } | \\text{ host is not parasitized})\\) \\(P(\\text{egg is female}) = (0.20 \\times 0.10) + (0.80 \\times 0.95) = 0.78\\) 5.11.1 The Monty Hall Problem Suppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind each of the other doors, a goat. You pick a door, say No.1, and the host, who knows what’s behind the doors, opens another door, say No.3, which has a goat. He then says to you, “Do you want to pick door No.2?” Well, what should you do? Stay with your original pick or switch? The winning strategy is to always switch to the still closed door you didn’t originally pick. Why? How? TABLE 5.9: The Monty Hall Problem Scenarios Door 1 Door 2 Door 3 Host’s Action Scenario 1 Goat Car Goat Opens Door 3 Scenario 2 Car Goat Goat Opens Door 2 Scenario 3 Goat Goat Car Opens Door 2 Say you pick Door 1 and you know there are three possibilities here. Either the car is behind Door 1, it is behind Door 2, or it is behind Door 3. So the probability the car is behind Door 1 is \\(\\dfrac{1}{3}\\), behind Door 2 is \\(\\dfrac{1}{3}\\) and behind Door 3 is \\(\\dfrac{1}{3}\\). You can also think of this as there is a \\(\\dfrac{1}{3}\\) probability the prize is behind Door 1 and hence there is a \\(\\dfrac{2}{3}\\) probability that the prize is behind Door 2 or 3. That is, \\(P(\\text{Car behind Door 2 or Door 3}) = \\dfrac{2}{3}\\). Once the host opens up one of the doors you did not pick, say Door 3, and this door will always have a goat, \\(P(\\text{Car behind Door 2 or Door 3}) = \\dfrac{2}{3}\\) is still true BUT now you know the car is not behind Door 3 and hence \\(P(\\text{Car behind Door 2 or Door 3}) = P(\\text{Car behind Door 2}) = \\dfrac{2}{3}\\); you better switch! This puzzle has spawned a book and several pages of explanations. Try out this simulation here to convince yourself switching is the winning strategy. 5.12 Bayes’ Theorem Having established some basic rules of probability theory, we can now turn to one of the most exciting developments that occurred a long time ago but is being seen in a new light only in recent decades: Bayesian statistics. Who knew an eighteenth century Presbyterian minister could wreak such positive havoc! Indeed, his early insights have applications in such diverse areas as medical testing, ecology, Google’s self-driving cars, spam filters that protect your email, and even saving a fisherman’s life?. Let us appreciate Bayes’ Theorem with reference to an often used problem. Say I present you with the following information: 1% of women at age forty who participate in routine screening have breast cancer. 80% of women with breast cancer will get positive mammographies. 9.6% of women without breast cancer will also get positive mammographies. A 40-year old woman had a positive mammography in a routine screening. What is the probability that she actually has breast cancer? TABLE 5.10: Breast Cancer and Bayes Theorem Test Cancer No Positive 0.800 0.096 Negative ? ? TABLE 5.11: Breast Cancer and Bayes Theorem (Part 2) Test Result Will have cancer Will not have cancer Positive 0.800 0.096 Negative 0.200 0.964 Total 1.000 1.000 The question we are tasked with is this: If a woman has a positive test result, what is the probability that she has breast cancer? That is, what is \\(P(\\text{Breast Cancer } | +\\text{Mammography)}\\), i.e., \\(P(B | A)\\)? We might struggle if it weren’t for the clean logic that Bayes encapsulated into an elegant mathematical statement. For now, let us break the problem down based on the information given to us. Start by assuming a certain population size of 40-year old women who are tested, i.e., \\(N = 100,000\\). We know that 1% of these will have breast cancer, which amounts to \\(1,000\\) women. Some 80% of these women will have breast cancer, i.e, \\(0.80 \\times 1000 = 800\\), which means \\(200\\) women who have breast cancer will test negative. We are also told that 9.6% of women without breast cancer will also get a positive test result. This amounts to \\(0.096 \\times 99000 = 9,504\\). This means that \\(86,696\\) women will have a negative test and have no breast cancer. Entering these values into the table we can calculate the probability that a woman with a positive test result actually has breast cancer to be \\(\\dfrac{800}{10304} = 0.0776\\) or 7.76%. What about the probability that the woman has a negative test result and yet has breast cancer? This would be \\(\\dfrac{200}{89696} = 0.0022\\) or 0.22%. The table below is populated with the estimates. TABLE 5.12: Breast Cancer and Bayes Theorem (Part 3) Test Result Will have cancer Will not have cancer Row Total Probability Positive 800 9504 10304 800/10304 = 0.0776 Negative 200 89496 89696 200/89696 = 0.0022 Total 1000 99000 100000 Once could arrive at the same conclusion via applying Bayes’ Theorem, as shown below. \\[\\begin{eqnarray*} P(B | A) = \\dfrac{P(A | B) \\times P(B)}{\\left[ P(A | B) \\times P(B) \\right] + \\left[ P(A | B^c) \\times P(B^c) \\right]} \\\\ = \\dfrac{0.800 \\times 0.01}{\\left[ 0.800 \\times 0.01 \\right] + \\left[0.096 \\times 0.990 \\right]} = \\dfrac{0.008}{\\left[0.008 + 0.09504 \\right]} = \\dfrac{0.008}{0.10304} = 0.0776 \\end{eqnarray*}\\] \\(P(B|A) = \\dfrac{P(A|B) \\times P(B)}{P(A)}\\) What is \\(P(A|B)\\) … the probability that a woman has Breast Cancer and gets a positive mammography? \\(= 0.80\\)? What is \\(P(B)\\)? … this is 0.01 What is \\(P(A)\\)? … This is the probability of getting a positive mammography, which can happen as follows: when she has Breast Cancer … \\((0.80 \\times 0.01)\\), or when she doesn’t have Breast Cancer … \\((0.096 \\times 0.99)\\) \\[\\begin{eqnarray*} P(A) = (0.80 \\times 0.01) + (0.096 \\times 0.99) = 0.10304 \\\\ \\therefore P(B|A) = \\dfrac{P(A|B) \\times P(B)}{P(A)} \\\\ = \\dfrac{0.80 \\times 0.01}{0.10304} = \\dfrac{0.008}{0.10304} = 0.07763975 \\end{eqnarray*}\\] 5.12.1 Another Example Suppose a patient exhibits symptoms that make her physician concerned that she may have a particular disease. The disease is relatively rare in this population, with a prevalence of 0.2% (meaning it affects 2 out of every 1,000 persons). The physician recommends a screening test that costs $250 and requires a blood sample. Before agreeing to the screening test, the patient wants to know what will be learned from the test, specifically she wants to know the probability of disease, given a positive test result, i.e., P(Disease | Screen Positive). The physician reports that the screening test is widely used and has a reported sensitivity of 85%. In addition, the test comes back positive 8% of the time and negative 92% of the time. Source TABLE 5.13: Testing Positive and Bayes Theorem Test Result Sick Healthy Total Probability Positive 170 7830 8000 170/8000 = 0.02125 Negative 30 91970 92000 30/92000 = 0.000326087 Total 200 99800 100000 What is \\(P(Sick | +Test)\\)? We know that \\(P(Sick | +Test) = \\dfrac{P(+{Test} | S) \\times P(Sick)}{P(+{Test})}\\) We know that \\(P(+{Test} | Sick) = 0.85\\), that \\(P(Sick) = 0.002\\), and that \\(P(+{Test}) = 0.08\\) Therefore, \\(P(Sick | +Test) = \\dfrac{0.85 \\times 0.002}{0.08} = 0.021\\) … There is a 2.1% chance the patient is actually sick if the test comes back positive. What if the test comes back as a false positive … i.e., What is the chance the patient is actually healthy but the test is positive? This requires calculating \\(P(+Test | \\text{ Not Sick }) = \\dfrac{P(\\text{Healthy } | +{Test}) \\times P(+Test)}{P(Healthy)} = \\dfrac{1 - 0.021 \\times (0.08)}{1 - 0.002} = 0.078\\) or 7.8%; a healthy patient can still test positive for the disease 7.8% of the time. 5.12.2 Extending Bayes Rule Bayes theorem is not limited to simple \\(2 \\times 2\\) contingency tables, as demonstrated by the example below. An aircraft emergency locator transmitter (ELT) is a device designed to transmit a signal in the case of a crash. The Altigauge Manufacturing Company makes 80% of the ELTs, the Bryant Company makes 15% of them, and the Chartair Company makes the other 5%. The ELTs made by Altigauge have a 4% rate of defects, the Bryant ELTs have a 6% rate of defects, and the Chartair ELTs have a 9% rate of defects. What is \\(P(Altigauge|Defective)\\)? TABLE 5.14: Extending Bayes Theorem Manufacturer Defective Not Defective Total Probability Altigauge 320 7680 8000 320/455 = 0.7032967 Bryant 90 1410 1500 90/455 = 0.1978022 Chartair 45 455 500 45/455 = 0.0989011 Total 455 9545 10000 \\[\\begin{eqnarray*} P(A|D) = \\dfrac{P(D|A) \\times P(A)}{\\left[P(D|A) \\times P(A) \\right] + \\left[P(D|B) \\times P(B) \\right] + \\left[P(D|C) \\times P(C) \\right]} \\\\ = \\dfrac{0.80 \\times 0.04}{\\left[ 0.80 \\times 0.04 \\right] + \\left[0.15 \\times 0.06 \\right] + \\left[0.05 \\times 0.09 \\right]} \\\\ = \\dfrac{0.0320}{0.0320 + 0.0090 + 0.0045} = \\dfrac{0.0320}{0.0455} = 0.7032967 \\end{eqnarray*}\\] 5.13 Key Things to Remember The Addition Rule Non-Mutually Exclusive Events: \\(P(\\text{A or B}) = P(A) + P(B) - P(\\text{A and B})\\) Mutually Exclusive Events: \\(P(\\text{A or B}) = P(A) + P(B)\\) Multiplication Rule Independent Events: \\(P(\\text{A and B}) = P(A) \\times P(B)\\) Dependent Events: \\(P(\\text{A and B}) = P(A) \\times P(B | A)\\) and \\(P(\\text{B and A}) = P(B) \\times P(A | B)\\) A and B are mutually exclusive if \\(P(\\text{A and B}) = 0\\) A and B are independent if \\(P(A|B) = P(A)\\); \\(P(B|A) = P(B)\\) Total Probability: \\(P(B) = P(A)\\times P(B|A) + P(A^c)\\times P(B|A^c)\\) Bayes’ Rule: \\(P(A|B) = \\dfrac{P(A)\\times P(B|A)}{P(B)}\\); and \\(P(B|A) = \\dfrac{P(B)\\times P(A|B)}{P(A)}\\) 5.14 Chapter 5 Practice Problems Problem 1 In the first hour of a hunting trip, the probability that a pride of Serengeti lions will encounter a Cape buffalo is 0.035. If it encounters a buffalo, the probability that the pride successfully captures the buffalo is 0.40. What is the probability that the next one-hour of a hunt the pride will successfully capture a Cape buffalo? Problem 2 You graduated with a Biology degree and are interviewed for a lucrative job as a snake handler in a circus. As part of the audition you must pick up two rattlesnakes from a pit containing eight rattlesnakes, three of which have been defanged (so they cannot bite you) but the other five are still dangerous (they can bite you). You skipped the herpetology course while in school so you cannot identify the defanged snakes from the others. Luckily for you, you did take a course on probability. As you thank your stars you bend down and pick up one snake with your left hand and another with your right hand. What is the probability that you picked up no dangerous snakes? Assuming any of the five dangerous snakes will, if picked up, bite with a probability of 0.8. What is the chance that in picking up two snakes you will be bitten at least once? If you picked up only one snake and it did not bite you, what is the probability that it is defanged? Problem 3 Schrodinger’s cat lives under a constant threat of death from the random release of a deadly poisonous gas. The probability of a release of the poison on any given day is 0.01, and releases are independent across days. What is the probability that the cat will survive one day? What is the probability that the cat survives seven days? What is the probability that the cat survives a year (365 days)? What is the probability that the cat will die by the end of a year? Problem 4 You are the health inspector for your city and tasked with improving airport washroom hygiene in the city’s lone airport. You know that the probability of a man washing his hands after using the washroom is 0.74 and that of a woman is 0.83. Assume there are 40 men and 60 women waiting to use the washroom. What is the probability that the next person to use the washroom will wash his/her hands? Problem 5 Rapid HIV tests allow for quick diagnosis without expensive laboratory equipment but their accuracy is in doubt. In a population of 1517 tested individuals in a country, 4 had HIV but tested negative (false negative) with the rapid test, 166 had HIV and tested positive, 129 were HIV free but tested positive (false positive), and 1218 were HIV free and tested negative. What is the probability of a false positive? What was the false negative rate? If a randomly sampled individual from this population tests positive on a rapid test, what is the probability that he/she has HIV? Problem 6 Taking a group photograph is tricky because someone or the other usually seems to blink just as the photograph is taken. We know the probability of an individual blinking during a photo is 0.04. If you photograph one person, what is the probability that he/she will not blink? If you are photographing a group of 10 individuals, what is the probability that at least one person will blink? Problem 7 In Vancouver (British Columbia), the probability of rain on any given winter’s day is 0.58, on a spring day it is 0.38, on a summer day it is 0.25, and in the fall it is 0.53. What is the probability of rain on any given randomly chosen day in a year (365 days)? If I tell you that it rained on a particular day (but I don’t tell you the date), what is the probability that this was a winter’s day? Problem 8 A standard deck of cards has 52 cards, 13 of each suit Spades, Diamonds, Hearts, and Clubs. Each suite has 13 cards, an Ace, King, Queen, Jack, and nine numbered cards 2 through 10. Face cards are King, Queen, and Jack. If you draw one card, what is the probability that the card is a King? What is the probability of drawing a face card that is also a Spade? What is the probability of drawing a card without a number on it? What is the probability of drawing a red card? What is the probability of drawing an ace? What is the probability of drawing a red Ace? Are these events (“Ace” and “red”) mutually exclusive? Are they independent? List two events that are mutually exclusive for a single draw from a standard deck of cards. What is the probability of drawing a red King? What is the probability of drawing a face card in hearts? Are these two events mutually exclusive? Are they independent? Problem 9 A boy mentions that none of the 21 kids in his third-grade class has had a birthday since school began 56 days ago. Assume that in the population the probability of having a birthday on any given day is the same for every day of the year (365 days). What is the probability that 21 kids in such a class would not yet have a birthday in 56 days? Problem 10 The probability of getting a heads or tails on a single flip of a fair coin is 0.5, each. If you flip a coin twice, what is the probability of getting no heads? If you flip a coin twice, what is the probability of getting exactly two heads? If you flip a coin twice, what is the probability of getting exactly one tail? If you flip a coin twice, what is the probability of getting no tails? Problem 11 Two dice are rolled, find the probability that the sum of the two faces is equal to 1 equal to 4 less than 13 greater than 7 Problem 12 The blood groups of 200 people is distributed as follows: 50 have type A blood, 65 have B blood type, 70 have O blood type and 15 have type AB blood. If a person from this group is selected at random, what is the probability that this person has O blood type? Problem 13 A box is filled with candies in different colors. We have 40 white candies, 24 green ones, 12 red ones, 24 yellow ones and 20 blue ones. If we have selected one candy from the box without peeking into it, find the probability of getting a green or a red candy. Problem 14 You go to see the doctor about an ingrowing toenail. The doctor selects you at random to have a blood test for swine flu, which for the purposes of this exercise we will say is currently suspected to affect 1 in 10,000 people in Ohio. The test is 99% accurate, in the sense that the probability of a false positive is 1%. The probability of a false negative is zero. You test positive. What is the new probability that you have swine flu? Now imagine that you went to a friend’s wedding in Mexico recently, and (for the purposes of this exercise) it is known that 1 in 200 people who visited Mexico recently come back with swine flu. Given the same test result as above, what should your revised estimate be for the probability you have the disease? Problem 15 In a TV Game show, a contestant selects one of three doors; behind one of the doors there is a prize, and behind the other two there are no prizes. After the contestant selects a door, the game-show host opens one of the remaining doors, and reveals that there is no prize behind it. The host then asks the contestant whether they want to SWITCH their choice to the other unopened door, or STICK to their original choice. Is it probabilistically advantageous for the contestant to SWITCH doors, or is the probability of winning the prize the same whether they STICK or SWITCH? (Assume that the host selects a door to open, from those available, with equal probability). Problem 16 A diagnostic test has a probability 0.95 of giving a positive result when applied to a person suffering from a certain disease, and a probability 0.10 of giving a (false) positive when applied to a non-sufferer. It is estimated that 0.5 % of the population are sufferers. Suppose that the test is now administered to a person about whom we have no relevant information relating to the disease (apart from the fact that he/she comes from this population). Calculate the following probabilities: that the test result will be positive that, given a positive result, the person is a sufferer that, given a negative result, the person is a non-sufferer that the person will be misclassified Problem 17 On the first anniversary of the September 11, 2001 terrorist attack on the Twin Towers in New York City, the winning pick-3 number in the city was 911. This lottery involves choosing a three-digit sequence between 000 and 999, with the winning draw determined by numbered balls circulating in a machine. What is the probability of the winning number being 911 on September 11, 2002 if a single draw is held for the lottery? In reality there are two draws daily. What is the probability of the winning number being 911 on September 11, 2002 in two draws held for the lottery that day? Note that in each of these cases we haven’t said what will happen yet we have been able to list all possible outcomes that might occur. This is important because if we omit any likely outcome our sample space would be inaccurate.↩︎ "],["distributions.html", "Chapter 6 Probability Distributions 6.1 Random Variables 6.2 Probability Distributions 6.3 Continuous Probability Distributions 6.4 Chapter 6 Practice Problems", " Chapter 6 Probability Distributions In this chapter and the next we build a bridge between the one sample we almost always have to work with and the population from which our sample was drawn. This is a crucial stage of our statistics knowledge-building for one major reason. Specifically, it is always easy to guess (correctly) what the sample will look like if we know what the population looks like. For example, imagine I show you a jar filled with 100 black marbles. I then blindfold you and ask you to pick 20 marbles. What color are they? Of course, all 20 marbles are black since that is the population. I then show you another jar, this one with 60 black, 20 white, and 20 red marbles. I then blindfold you again and ask you to pick 20 marbles and guess their colors. You might guess that the sample will show the same mix as the population: You might have 12 black, 4 white, and 4 red marbles. You could have a different mix but 12 black, 4 white and 4 red would be a reasonable guess. Now I blindfold you, place a jar before you and ask you to pick 20 marbles without telling you the distribution of colored marbles in the jar. What would you expect in your sample? You have no clue and nor should you since the population is unknown. This is what you face in statistics all the time: A population beyond your purview, flying blind so to speak. How then can you really generalize from your sample to the population? Only by way of the theory of sampling distributions, by knowing how much drift to expect between your sample and the population it represents. Understanding the finer points of the theory of sampling distributions is our goal in this chapter. To ease our understanding we start by grasping what is meant by random variables that are discrete versus continuous, looking at some discrete versus continuous probability distributions, and then working our way to the concepts of a standard error, the central limit theorem, and confidence intervals. 6.1 Random Variables A random variable is a numerical description of the outcome of an experiment. A discrete random variable assumes discrete values while a continuous random variable may assume any value in an interval or collection of intervals. TABLE 6.1: Examples of Discrete Random Variables Random Variable Possible Values No. of defective iPhones (0, 1, 2, 3,…, 49) Sex of car buyer (Male, Female) No. of Mountain Lions seen (0, 1, 2, 3, …, 419 Gene length (in nucleotides) (60, …, 100000) TABLE 6.2: Examples of Continuous Random Variables Random Variable Possible Values Gene length (in nucleotides) 60 &lt;= x &lt;= 100000 Spending per week 0 &lt;= x &lt;= infinity Travel times to CMH 55.4 &lt;= x &lt;= 118.5 Undulation rates of gliding snakes 0 &lt;= x &lt;= 1.9 Petal length of the virginica Iris 1 &lt;= x &lt;= 6.7 6.2 Probability Distributions A probability distribution is a list of the probabilities of all mutually exclusive events (aka outcomes) of a random trial. While both discrete and continuous variables have probability distributions, discrete probability distributions are easier to understand so let us begin with them. 6.2.1 Discrete Probability Distributions Suppose we have an experiment whose outcome \\((x)\\) depends on chance (i.e., is a random variable) and the sample space of the experiment is, as usual, the set of all possible outcomes of the experiment. If the sample space is either finite or countably infinite, the random variable is said to be discrete. A probability distribution of a discrete random variable (\\(x\\)) describes how probabilities are distributed over the values of the random variable, and is denoted by \\(f(x)\\). Discrete probability functions must meet two conditions … The table below shows the number of persons waiting for service at a local office of the state’s Department of Motor Vehicles when the first employee walks up to open the counter. Let us assume that these data were gathered for a random stretch of 300 business days, and span two years. TABLE 6.3: An Example of a Discrete Probability Distribution No. waiting (x) Frequency (f) x * f(x) 0 54 0.18 1 117 0.39 2 72 0.24 3 42 0.14 4 12 0.04 5 3 0.01 Note that \\(\\sum f = 300\\) and \\(\\sum x*f(x) = 1.00\\) FIGURE 6.1: Frequency Distribution of Number Waiting If you scan the table or the bar chart it is obvious what you should expect if you were the individual manning the counter. On any given day you should expect to see 1 person waiting to be served. Why? Because this is what seems to happen most often (117 out of the 300 days you kept logs). What if you were rolling dice instead? Note, again, that the first row with columns 1, 2, 3, 4, 5, and 6 are the numbers that show up on a roll of Dice 1 and the first column with rows 1, 2, 3, 4, 5, and 6 are the numbers that show up on a roll of Dice 2. FIGURE 6.2: Rolling Two Dice You have seen Table 6.4 before so it should be easy to figure out what would be the most likely sum of the numbers showing up on the faces of two dice you roll: \\(7\\). Why? Because that happens most often. FIGURE 6.3: Sum from Rolling Two Dice TABLE 6.4: Sum of Two Dice 1 2 3 4 5 6 1 2 3 4 5 6 7 2 3 4 5 6 7 8 3 4 5 6 7 8 9 4 5 6 7 8 9 10 5 6 7 8 9 10 11 6 7 8 9 10 11 12 6.2.1.1 The Binomial Distribution Let us take up another example, this time of flipping a coin. Say you flip a coin 10 times. So long as I know that it is a fair coin, what should I expect in terms of the distribution of the number of heads I would see? One way to answer this question would be to setup all possible outcomes such as no heads, 1 head, 2 heads, 3 heads, and so on but this would be a very tedious way of doing things. Instead, we rely on the binomial distribution which characterizes the distribution of binary outcomes, with the outcome of interest being tagged as a success and the other category tagged as a failure. Note that success could mean a patient survives (versus dies), a candidate for political office wins (versus loses), a tax audit catches a tax evader (versus fails to detect evasion), the job-training program works (versus it does not), and so on. The binomial distribution is premised upon some assumptions: The number of trials (\\(n\\)) is fixed Each trial is independent of all other trials Only two mutually exclusive and mutually exhaustive outcomes likely to occur in any given trial, with one outcome defined as success and the other defined as failure The probability of observing a success (\\(p\\)) does not vary across trials. Because there are only two outcomes, this means the probability of observing a failure (\\(q\\)) also does not vary across trials. Further, \\(p = 1 - q\\) and \\(q = 1 - p\\) Mathematically, the probability of observing \\(x\\) successes in \\(n\\) trials of a binomial process is given by \\[\\begin{eqnarray*} P\\left[x \\text{ successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\text{where } \\binom{n}{x} = \\dfrac{n!}{x!(n-x)!} \\text{ and } n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 \\end{eqnarray*}\\] Let us understand this distribution with a simple example. If I toss a coin 2 times, what is the probability of getting exactly 1 head? Let success be the variable \\(x\\) and then, in our present example, \\(x=1\\). Now we know that for unbiased coins there is a 50:50 chance of getting heads on any single toss, i.e., \\(p(Heads)=0.50\\). We are also conducting \\(n=2\\) independent trials since we are flipping a coin twice and what happens on the first toss has no impact on what happens in the second toss. How many outcomes are likely in our 2 independent trials? We know this to be \\((2)^{2} = 4\\) … these are \\(\\left[HH, HT, TH, TT \\right]\\). In how many ways can we get 1 head out of 2 tosses? … \\(\\left[HT, TH \\right]\\). So the probability of getting exactly 1 head in 2 tosses is \\(\\dfrac{2}{4} = 0.5\\). This was easy to calculate manually, by spelling out all possible outcomes and then seeing how many of these outcomes match our definition of success. The binomial distribution would have given you the same answer: \\[\\begin{eqnarray*} P\\left[x \\text{ Successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\therefore P\\left[1 \\text{ Success}\\right] = \\binom{2}{1}(0.50)^{1}\\left(1 - 0.50\\right)^{2-1} = \\binom{2}{1}(0.50)^{1}\\left(0.50\\right)^{1} \\\\ \\binom{2}{1} = \\dfrac{2\\times 1}{\\left(1\\right) \\left(1 \\right)} = 2 \\\\ \\therefore, P\\left[1 \\text{ Success}\\right] = (2) \\times (0.5) \\times (0.5) = 0.50 \\end{eqnarray*}\\] If I toss a coin 3 times, what is the probability of getting exactly 1 head? Let \\(x=1\\). We know for unbiased coins \\(p(Heads)=0.50\\). We are also conducting \\(n=3\\) independent trials. How many outcomes are likely in 3 independent trials? We know this to be \\((2)^{3} = 8\\) … these are \\(\\left[HHH, HHT, HTH, HTT, TTT, TTH, THT, THH \\right]\\). In how many ways can we get 1 Head out of 3 tosses? … \\(\\left[HTT, THT, TTH \\right]\\). So the probability of getting exactly 1 Head in 3 tosses is \\(\\dfrac{3}{8} = 0.375\\) Using the binomial distribution, \\[\\begin{eqnarray*} P\\left[x \\text{ Successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\therefore P\\left[1 \\text{ Success}\\right] = \\binom{3}{1}(0.50)^{1}\\left(1 - 0.50\\right)^{3-1} = \\binom{3}{1}(0.50)^{1}\\left(0.50\\right)^{2} \\\\ \\binom{3}{1} = \\dfrac{3 \\times 2\\times 1}{\\left(1\\right) \\left(2 \\times 1 \\right)} = 3 \\\\ \\therefore, P\\left[1 \\text{ Success}\\right] = (3) \\times (0.5) \\times (0.25) = 0.375 \\end{eqnarray*}\\] If I had to now answer the question we began with – what would the distribution of heads look like if I flipped a coin 10 times? – and I went the manual route I would be doing a lot of tedious calculations. Instead, I can use the binomial to calculate the probability of 0 heads, 1 head, 2 heads, and so on. For example, for 0 heads it would be \\[\\begin{eqnarray*} P\\left[x \\text{ Successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\therefore P\\left[0 \\text{ Success}\\right] = \\binom{10}{0}(0.50)^{0}\\left(1 - 0.50\\right)^{10-0} = \\binom{10}{0}(0.50)^{0}\\left(0.50\\right)^{10} \\\\ \\binom{10}{0} = \\dfrac{10 \\times 9 \\times 8 \\times \\ldots\\times 2\\times 1}{\\left(0\\right) \\left(10 \\times 9 \\times 8 \\times \\ldots \\times 2 \\times 1 \\right)} = 1 \\\\ \\therefore, P\\left[0 \\text{ Successes}\\right] = (1) \\times (0.5) \\times (0.0009765625) = 0.0009765625 \\end{eqnarray*}\\] For 1 head it would be \\[\\begin{eqnarray*} P\\left[x \\text{ Successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\therefore P\\left[1 \\text{ Success}\\right] = \\binom{10}{1}(0.50)^{1}\\left(1 - 0.50\\right)^{10-1} = \\binom{10}{1}(0.50)^{1}\\left(0.50\\right)^{9} \\\\ \\binom{10}{1} = \\dfrac{10 \\times 9 \\times 8 \\times \\ldots\\times 2\\times 1}{\\left(1\\right) \\left(9 \\times 8 \\times \\ldots \\times 2 \\times 1 \\right)} = 10 \\\\ \\therefore, P\\left[1 \\text{ Success}\\right] = (10) \\times (0.5) \\times (0.001953125) = 0.009765625 \\end{eqnarray*}\\] and so on. The rest of the calculations are done similarly and listed in the table below: TABLE 6.5: Number of Heads in 10 Coin Flips No. of Heads Relative frequency f(x) 0 0.0010 1 0.0098 2 0.0439 3 0.1172 4 0.2051 5 0.2461 6 0.2051 7 0.1172 8 0.0439 9 0.0098 10 0.0010 These relative frequencies (the probabilities) can also be seen in the plot below. FIGURE 6.4: Proportion of Heads in 10 Coin Flips Now the question is, if you flip a coin 10 times, how many heads should you expect to see, on average? In the language of statistics we speak of probabilistic outcomes in terms of expected value where the expected value of a random variable is a measure of the central tendency of the random variable, and is given by \\(E(x) = \\mu = \\Sigma xf(x)\\). For our 10 flips, the expected value would be … TABLE 6.6: Number of Heads in 10 Coin Flips No. of Heads (x) Relative Frequency (f(x)) Expected Value (x * fx()) 0 0.0010 0.0000000 1 0.0098 0.0097656 2 0.0439 0.0878906 3 0.1172 0.3515625 4 0.2051 0.8203125 5 0.2461 1.2304688 6 0.2051 1.2304687 7 0.1172 0.8203125 8 0.0439 0.3515625 9 0.0098 0.0878906 10 0.0010 0.0097656 Makes sense, doesn’t it? On average you would expect to see 5 heads in 10 flips of an unbiased coin. However, as the table makes quite clear, there is a small but non-zero probability of ending up with fewer or more heads as well. How does any of this apply to the real world? In more ways than one can imagine. For example, based on more than a century of birth statistics we know that the next birth will be a boy is 0.512 and hence that it will be a girl is 0.488. So if you wanted to predict the sex of the child born next the better bet would be that it is a boy. Let us put the binomial distribution into action with a few examples. Example 1 Your city council has been charged with sex discrimination in hiring. In the last fiscal year your city council hired only 3 women out of a candidate pool numbering 12 qualified applicants. On average, some 40% of women tend to make up the qualified applicant pool for these positions. If the city council was not discriminating based on the applicant’s sex, what is the probability of the city hiring three or fewer women? We know the number of experiments/trials is \\(n=12\\). We also know that the number of successes, defined as the number of women hired, is \\(x=3\\). The probability of success, here the probability of hiring a qualified woman applicant, is known to be \\(p=0.40\\). Using the binomial theorem, then, we can calculate the probability of hiring no qualified woman applicant is given by \\[\\begin{eqnarray*} P\\left[x \\text{ Successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\therefore P\\left[0 \\text{ Successes}\\right] = \\binom{12}{0}(0.40)^{0}\\left(1 - 0.40\\right)^{12-0} = \\binom{12}{0}(0.40)^{0}\\left(0.60\\right)^{12} \\\\ \\binom{12}{0} = \\dfrac{12 \\times 10 \\times 9 \\times \\ldots\\times 2\\times 1}{\\left(0\\right) \\left(12 \\times 10 \\times 9 \\times \\ldots \\times 2 \\times 1 \\right)} = 1 \\\\ \\therefore, P\\left[0 \\text{ Successes}\\right] = (1) \\times (1) \\times (0.002176782) = 0.002176782 \\end{eqnarray*}\\] Similarly, calculating the probability of 1, 2 and 3 successes yields estimates of 0.01741426, 0.06385228 and 0.141894, respectively. Thus, the probability of seeing 3 or fewer women hired works out to \\(0.002176782 + 0.01741426 + 0.06385228 + 0.141894 = 0.2253373\\). Formally, then, \\(P(x \\leq 3) = 0.2253373\\). Note that we could have used the online binomial calculator to do these calculations for us, saving precious time. Example 2 Seaman David Brady is one of 16 seamen in Petty Officer Rickels’ unit. Daily, four seamen are assigned to chip paint while the rest are assigned to screen movies suitable for viewing on the naval base. Assume that duty assignments are independent across days. Seamen Brady believes Rickels does not like him because he has ended up chipping paint 16 of the past 20 days. What is the probability that this would happen if Rickels was not discriminating against Brady? The probability of being assigned to chip paint is 0.25 for any individual. Setting \\(p=0.25; n = 20; x = 16\\) in the calculator yields \\(P(x=16)\\) \\(&lt; 0.000001\\). Do the data suggest Rickels is discriminating against Brady? Why do you conclude as you do? Yes, given the almost zero probability of this happening by chance it seems as if Brady is being picked on by Rickels. 6.2.1.2 The Poisson Distribution We can also track hurricanes, cyclones, earthquakes, etc. and measure their intensity. Or perhaps you are interested in figuring out the probability of a Shark Attack in a given year in a given place. You could look at the historical data and figure out the average number of attacks in this place per year. These data are available here. Say I am only interested in incidents in the USA and Australia over the 1900-2017 period and want to know the number of incidents per year per country. This evident in the plot that follows. FIGURE 6.5: Shark Incidents in the USA and Australia (1900-2017) The average number of incidents per country turns out to be almost 10 for Australia and about 17 for the USA. This would tell us that in any given year we should expect, on average, these many shark incidents in each country, respectively. Of course, something has gone on since about 2000 as reflected by the surge in the number of incidents. If we calculated the averages just for the 2000-2017 period we would likely see higher averages than for the pre-2000 period. In passing, note the surprising results both in the plot and in these means: Australia has fewer shark incidents, on average, than does the USA, but the news media usually carry stories of attacks in Australian waters. As it turns out, the variable of interest – number of shark incidents per country per year – is a discrete variable that can never drop below 0 but can assume any value \\(x = 0, 1, 2, 3, 4, 5, \\ldots, n\\). Variables such as these are called count variables and belong to a specific distribution: The Poisson distribution. Mathematically, the Poisson probability distribution is expressed as: \\[f(x) = \\dfrac{e^{-x}\\lambda^{x}}{x!}\\] where \\(x = 0, 1, 2, 3, \\ldots\\), \\(\\lambda &gt; 0\\) is the mean and variance of the distribution, and \\(e=2.71828\\). Several discrete random variables are assumed to be Poisson distributed – number of highway fatalities, number of persons in a household, number of patient visits to the Emergency Room, number of traffic stops, number of terrorist attacks, number of hurricanes rated Category 5 on the Saffir-Simpson Hurricane Wind Scale, the number of citizen complaints filed with the City Clerk, the number of hazardous waste sites per county, the number of parolees violating the conditions of their parole per month, and so on. Say, for example, that on average 10 complaints are filed per month with the City Clerk. If you pick a month at random, what would be the probability of seeing no complaint?; 1 complaint?; 2 complaints; \\(\\ldots\\) 20 complaints? These turn out to be: TABLE 6.7: Probability of Complaints No. of Complaints Probability 0 0.0067379 1 0.0336897 2 0.0842243 3 0.1403739 4 0.1754674 5 0.1754674 6 0.1462228 7 0.1044449 8 0.0652780 9 0.0362656 10 0.0181328 11 0.0082422 12 0.0034342 13 0.0013209 14 0.0004717 15 0.0001572 FIGURE 6.6: Probability Distribution of Complaints Dispersion versus Clumping If a discrete random variable is indeed Poisson generated, the mean should equal the variance. In passing, note that the Poisson distribution is premised on the following assumptions: The probability \\((p)\\) of observing a success in a small space or interval of time is approximately proportional to the area of the space or the length of the time interval The probability of two successes occurring in the same narrow interval of time or in a small space is negligible The probability \\((p)\\) of observing a success in a small space or interval of time does not vary across space or time The probability \\((p)\\) of observing a success in a small space or interval of time is independent of the probability \\((p)\\) of observing a success in the next small space or interval of time If the mean exceeds the variance then we have a dispersed variable (i.e., events occur farther apart in space/time than would be expected by chance). An example of dispersion might be observed in the behavior of territorial animals such as lions and tigers that mark their own territory and guard it from competitors. So if you see one “success” (say, a lion), the probability of seeing another nearby/soon decreases. If the variance exceeds the mean we have clumping (i.e., events occur closer together in space/time than would be expected by chance). An example of clumping might be social animals (that herd together) and contagious diseases (outbreaks will occur within a spatial group). So if you see one “success” (say, a wildebeest), the probability of seeing another nearby/soon increases. Before we move on, note what happens to the distribution as the mean \\((\\lambda)\\) increases. The more commonly the event occurs, the more “normal” does the distribution look. The lower is the mean, the more likely you are to see a positively skewed distribution, and the higher the mean the more likely you will see a negatively skewed distribution. FIGURE 6.7: Some Poisson Distributions When \\(n \\rightarrow \\infty\\) and \\(p \\rightarrow 0\\), the Poisson distribution approximates the Binomial distribution. Likewise, as \\(\\lambda \\rightarrow \\infty\\), the Poisson distribution approximates the Normal distribution. This pattern is visible in the plots that follow. FIGURE 6.8: Some Poisson Distributions tending to the Binomial FIGURE 6.9: Some Poisson Distributions tending to the Normal Poisson distributions have been used to tackle some very fascinating problems in both academic and applied research, including, but not limited to, traffic signal design and operation, authorship of the Federalist Papers, highway traffic, radioactive decay, German air-raids on London during World War II, and positions of glowworms on the ceiling of the Waitomo cave in New Zealand. Two practical applications of the Poisson distribution follow. Example 1 The mean number of crime reports filed per hour during Halloween by the Athens City Police Department is 0.833. What is the probability that in a 24 hour period the ACPD will see 10 or fewer crime reports filed? \\[f(x) = \\dfrac{e^{-x}\\lambda^{x}}{x!} = \\dfrac{e^{-10}0.833^{10}}{10!}\\] Let us use the online calculator for the poisson distribution. We have \\(\\lambda=19.992\\) since the mean for a 24-hour period would be \\(= 0.833 \\times 24 = 19.992\\), and are given \\(x = 10\\). Entering these values into the calculator yields \\(P(x \\leq 10) = 0.0108583424514613 \\approx 0.0108\\). Note that the calculator also generates other probabilities: \\(P(x = 10) = 0.0058396136640344; P(x &lt; 10) = 0.0050187287874269; P(x &gt; 10) = 0.989141657548539; P(x \\geq 10) = 0.9949812712125\\). The answer: There is 1.08% chance of seeing 10 or fewer reports filed within a 24-hour period. What is the probability of 20 or more crime reports being filed in the 24 hour period? Again, using the online calculator will yield \\(P(x \\geq 20) = 0.529031908826421\\); there is 52.90% chance of seeing 20 or more reports filed within a 24-hour period. Note: The high probability shouldn’t be surprising given that the average is almost 20 (\\(19.992\\) to be exact). Example 2 In Berksdale, Nebraska, the average number of fire per day is 0.071. What is the probability of seeing no fires on any given week (7 days)? What is the probability of seeing more than four fires in this week? For the week we know \\(\\lambda = 0.071 \\times 7 = 0.497\\) and \\(x=0\\). The calculator yields \\(P(x=0) = 0.608352983811176 \\approx 0.6084\\). Similarly, \\(P(x &gt; 4) = 0.000167426629467005 \\approx 0.0002\\) 6.3 Continuous Probability Distributions In contrast to discrete random variables, continuous random variables take on a uncountably infinite number of values such that unlike in the case of discrete random variables, where we could calculate the probability of a specific value \\((P(X = x))\\), we now have to calculate the probability of a value falling in an interval, i.e., \\(P(a \\leq x \\leq b)\\), with \\(a\\) and \\(b\\) being the lower and upper limits of the interval. Why do we need to do this, you might wonder. Think about it this way: Since the total number of values possible is infinite and probability is calculated with the total number of possible outcomes as the denominator, you pick any value of \\(x\\) and calculate \\(P(X=x) = \\dfrac{x}{\\infty}\\), you will end up with \\(0\\). Examples of continuous random variables abound … household incomes, county unemployment rates, scores on a standardized test, individuals’ heights and weights, flight times between New York City and Los Angeles, temperature, humidity, population size of each city, town, village, and township in the USA, undulation rates of gliding snakes, and much more. The probability distributions for continuous random variables then assume a different form, and there are several such distributions one could use. However, we will start with the simplest one of these: The Normal distribution. 6.3.1 The Normal Distribution The Normal distribution is the bell-shaped distribution that we have all read about or seen in one way or another. Formally, though, we say that a continuous random variable \\(X\\) follows a normal distribution if its probability density function is defined as: \\[f(X) =\\dfrac{1}{\\sigma{\\sqrt{2\\pi}}}e^{-(X-\\mu)^{2}/2\\sigma^{2}}\\] where \\(\\mu\\) is the mean of \\(X\\), \\(\\sigma\\) is the standard deviation of \\(X\\), \\(\\pi=3.14159\\), and \\(e=2.71828\\) Two parameters describe a Normal distribution – the mean and the standard deviation. As such, different means and/or standard deviations generate different Normal distributions. The examples below show you three different Normal distributions, each with its own unique combination of \\(\\mu\\) and \\(\\sigma\\). FIGURE 6.10: Three Normal Distributions: Different Means, Equal Standard Deviations Similarly, we could have a completely different set of Normal distributions: FIGURE 6.11: Three Normal Distributions: Different Means, Different Standard Deviations FIGURE 6.12: Three Normal Distributions with Differing Means and Standard Deviations In summary, then, there are an infinite number of Normal distributions, each with their own mean and standard deviation. Since it would be impossible for us to know which Normal distribution we are drawing a sample from, we rely on the Standard Normal Distribution, also known as the z-score Distribution. 6.3.2 The Standard Normal Distribution The Standard Normal Distribution is essentially a distribution generated by converting the raw values into their corresponding z-scores. Recall that \\(z = \\dfrac{x - \\mu}{\\sigma}\\). The beauty of this distribution is that no matter what the raw metric is, income, age, height, population size, percapita gross domestic product, and so on, once we create z-scores, they are guaranteed to have \\(\\mu = 0\\) and \\(\\sigma = 1\\). A positive/negative z-score indicates that the observation lies above/below the mean. The absolute value of the z-score indicates how many standard deviation units above/below the mean an observation falls. In brief, z-scores allow us to identify the relative location of an observation in a data-set by telling us how many standard deviation units above or below the mean a particular value \\(x_{i}\\) falls. The beauty of the z-score is that it allows us to compare scores drawn from distributions with dissimilar variability (see below, a redux of the Caribou (ME) versus Boston (MA) example): TABLE 6.8: Comparing Snowfall in Disparate Locations Place Mean Std.Dev. December.2016 Caribou (ME) 110 30 125 Boston (MA) 24 5 39 If you know your geography or have had the good/bad luck to be in Caribou (ME) during the dead of winter, you know that Caribou gets a lot more snowfall than does Boston, just by virtue of Caribou’s latitude and longitude. So how could we really compare what seems to be an apple (Boston, MA) and an orange (Caribou, ME). We cannot unless we convert snowfall into a z-score, using the \\(\\mu\\) and \\(\\sigma\\) for each of the two places. The result is in the last column, yielding z-scores of +0.50 and +3.00 for Caribou and Boston, respectively. What these z-scores tell us is that for Caribou, compared to its usual average and variability, the snowfall it received in December 2016 was only slightly worse than what it gets usually in any given year. On the other hand, Boston had very heavy snowfall relative to its average and the variability around this average. One of the additional strengths of the Standard Normal Distribution is the fact that we can safely assume: about 68% of the data values fall within \\(\\pm\\) 1 standard deviation about 95% of the data values fall within \\(\\pm\\) 2 standard deviation about 99% of the data values fall within \\(\\pm\\) 3 standard deviation z-scores greater/smaller than \\(\\pm{3}\\) are indicative of outliers Graphically, we could depict the areas demarcated by the 1, 2 and 3 standard deviations as follows: FIGURE 6.13: Area of the Standard Normal Distribution within 1 Standard Deviation FIGURE 6.14: Area of the Standard Normal Distribution within 2 Standard Deviations FIGURE 6.15: Area of the Standard Normal Distribution within 3 Standard Deviations Given the symmetry of the distribution, if 68% of the data fall within \\(\\pm 1\\) standard deviation units of the mean, then it must be that 34% of the data fall between the mean \\((0)\\) and \\(-1\\) and 34% of the data fall between the mean \\((0)\\) and \\(+1\\). Likewise, 47.5% of the data fall between the mean \\((0)\\) and \\(-2\\) and \\(+2\\) standard deviation units, respectively, and 49.5% of the data fall between the mean \\((0)\\) and \\(-3\\) and \\(+3\\) standard deviation units, respectively. Indeed, given any interval of z-scores we can easily find the area between these two z-scores via the Standard Normal Distribution table. TABLE 6.9: The Standard Normal Table 0 0.01 0.02 0.03 0.04 0.05 -3.4 0.00034 0.00032 0.00031 0.00030 0.00029 0.00028 -3.3 0.00048 0.00047 0.00045 0.00043 0.00042 0.00040 -3.2 0.00069 0.00066 0.00064 0.00062 0.00060 0.00058 -3.1 0.00097 0.00094 0.00090 0.00087 0.00084 0.00082 -3.0 0.00135 0.00131 0.00126 0.00122 0.00118 0.00114 -2.9 0.00187 0.00181 0.00175 0.00169 0.00164 0.00159 -2.8 0.00256 0.00248 0.00240 0.00233 0.00226 0.00219 -2.7 0.00347 0.00336 0.00326 0.00317 0.00307 0.00298 -2.6 0.00466 0.00453 0.00440 0.00427 0.00415 0.00402 -2.5 0.00621 0.00604 0.00587 0.00570 0.00554 0.00539 -2.4 0.00820 0.00798 0.00776 0.00755 0.00734 0.00714 -2.3 0.01072 0.01044 0.01017 0.00990 0.00964 0.00939 -2.2 0.01390 0.01355 0.01321 0.01287 0.01255 0.01222 -2.1 0.01786 0.01743 0.01700 0.01659 0.01618 0.01578 -2.0 0.02275 0.02222 0.02169 0.02118 0.02068 0.02018 -1.9 0.02872 0.02807 0.02743 0.02680 0.02619 0.02559 -1.8 0.03593 0.03515 0.03438 0.03362 0.03288 0.03216 -1.7 0.04457 0.04363 0.04272 0.04182 0.04093 0.04006 -1.6 0.05480 0.05370 0.05262 0.05155 0.05050 0.04947 -1.5 0.06681 0.06552 0.06426 0.06301 0.06178 0.06057 -1.4 0.08076 0.07927 0.07780 0.07636 0.07493 0.07353 -1.3 0.09680 0.09510 0.09342 0.09176 0.09012 0.08851 -1.2 0.11507 0.11314 0.11123 0.10935 0.10749 0.10565 -1.1 0.13567 0.13350 0.13136 0.12924 0.12714 0.12507 -1.0 0.15866 0.15625 0.15386 0.15151 0.14917 0.14686 -0.9 0.18406 0.18141 0.17879 0.17619 0.17361 0.17106 -0.8 0.21186 0.20897 0.20611 0.20327 0.20045 0.19766 -0.7 0.24196 0.23885 0.23576 0.23270 0.22965 0.22663 -0.6 0.27425 0.27093 0.26763 0.26435 0.26109 0.25785 -0.5 0.30854 0.30503 0.30153 0.29806 0.29460 0.29116 -0.4 0.34458 0.34090 0.33724 0.33360 0.32997 0.32636 -0.3 0.38209 0.37828 0.37448 0.37070 0.36693 0.36317 -0.2 0.42074 0.41683 0.41294 0.40905 0.40517 0.40129 -0.1 0.46017 0.45620 0.45224 0.44828 0.44433 0.44038 0.0 0.50000 0.49601 0.49202 0.48803 0.48405 0.48006 The standard normal table shows you the area below a specific z-score. For example, the area below \\(z = -3.40\\) is 0.0003369. We would denote this as \\(P(z \\leq -3.40) = 0.0003369\\). The meaning of “area” should now be taken as the proportion of z-scores that fall at or below a specific z-score. Indeed, we might as well speak in terms of probability: The probability of finding a z-score less than or equal to -3.40 is 0.0003369. Now look at the table and figure out how much of the area falls at or below \\(z = -1.00\\). This turns out to be 0.1586552, highlighted for you in panel (a) of the figure. Given the symmetry of the curve, this means that the area at or above \\(z = +1.00\\) must also be 0.1586552. If we add these two areas we see that the combined area below and above \\(z = -1.00; z = +1.00\\) works out to \\(0.1586552 + 0.1586552 = 0.3173\\). Since the area under the curve must sum to 1, this means the area between \\(z = -1.00; z = +1.00\\) must be \\(1 - 0.3173 = 0.6827\\). That is, 68.27% of z-scores lie in the interval given by \\(z = \\pm 1\\); see the shaded portion in panel (b) below. Similarly, we could calculate areas between \\(z = \\pm 2.00\\) (see panel (c) below), \\(z = \\pm 3.00\\) (see panel (d) below), and any other pair of z-scores. FIGURE 6.16: Areas Under the Standard Normal Curve Take, for example, the area between z-scores of -0.52 and +1.26. To calculate \\(P(-0.52 \\leq z \\leq +1.26)\\) we start by calculating the area at or below \\(z = -0.52\\). \\(P(z \\leq -0.52)\\) turns out to be 0.30153. We then calculate the area at or above \\(z = +1.26\\). \\(P(z \\geq +1.26)\\) turns out to be 0.10383. So the total area at or beyond these two z-scores is \\(0.30153 + 0.10383 = 0.40536\\). This isn’t the area we need; we need \\(P(-0.52 \\leq z \\leq +1.26)\\) and this can now be calculated as \\(1 - P(z \\leq -0.52) - P(z \\geq +1.26) = 1 - 0.30153 - 0.10383 = 1 - 0.40536 = 0.59464\\). The figure below shows this area: FIGURE 6.17: Areas Under the Standard Normal Curve Online tables are available as well; see one excellent applet by David M. Lane here as well as the popular one at surfstat. If using the David M. Lane calculator, be sure to enter values of the desired area Above/Below/Between/Outside. The same table will also allow you to enter an area and find the z-score(s) associated with that area if you select Value from an area. If using surfstat, select the appropriate graph of the distribution (the area in red shows you what area will be calculated), and then either enter the \\(z-score\\) to find the area/probability or the area/probability (as a proportion) to get the \\(z-score\\). Working with the Standard Normal Distribution Assume that for all traditional public schools in your state, the mean dropout rate in 2017 was \\(\\mu = 5\\), and the standard deviation was \\(\\sigma = 1.25\\). Given these parameters, if a school had a dropout rate of 7%, what percent of schools in the state did as badly or worse? The answer could be obtained as follows: \\[\\begin{eqnarray*} \\left.\\begin{aligned} z = \\dfrac{x - \\mu}{\\sigma} \\\\ z = \\dfrac{7 - 5}{1.25} = 1.6 \\\\ \\text{What is } P(z \\geq 1.6)? \\\\ P(z \\geq 1.6) = 0.05480 \\end{aligned}\\right. \\end{eqnarray*}\\] and thus some 5.48% of schools in the state had dropout rates of 7% or worse (i.e., higher dropout rates). What if the school had a dropout rate of 9%? \\[\\begin{eqnarray*} \\left.\\begin{aligned} z = \\dfrac{x - \\mu}{\\sigma} \\\\ z = \\dfrac{9 - 5}{1.25} = 3.2 \\\\ \\text{What is } P(z \\geq 3.2)? \\\\ P(z \\geq 3.2) = 0.00069 \\end{aligned}\\right. \\end{eqnarray*}\\] i.e., 0.069% of schools in the states had dropout rates of 8% or worse. We can also ask a different question: What dropout rates demarcate the top 10% and the bottom 10% of schools, respectively? We first find the z-scores that demarcate the top 10% and the bottom 10%, respectively. These turn out to be \\(z = +1.28\\) and \\(z = -1.28\\), respectively. \\[\\begin{eqnarray*} z = \\dfrac{x - \\mu}{\\sigma} \\\\ x = \\left(z \\times \\sigma \\right) + \\mu \\\\ x = \\mu + \\left(z \\times \\sigma \\right) \\\\ x_{top} = 5 + \\left(1.28 \\times 1.25 \\right) = 5 + 1.6 = 6.6 \\\\ x_{bottom} = 5 + \\left(-1.28 \\times 1.25 \\right) = 5 - 1.6 = 3.4 \\end{eqnarray*}\\] i.e., dropout rates of 6.6% and 3.4% separate the top 10% and bottom 10% of schools from all schools in the state. The important point about this particular example is the manner in which we rearranged the formula for a z-score to recover the actual dropout rates associated with the top/bottom 10% of schools. The z-score is a versatile entity, often used when looking to combine or compare phenomenon measured on different scales. A classic and important example comes from the health ranking dashboards compiled by various organizations. Take the Health Value Dashboard, County Health Rankings, America’s Health Rankings and the Commonwealth Fund, for example. Most of these dashboards are combining information from such disparate measures as the high school graduation rate, percent of adults with access to health care, percent of kids living in safe neighborhoods, child poverty, air pollution levels, access to bike and other alternative transportation options, and so on. How can you really combine disparate elements? Via the z-score, of course, by converting each measure into a z-score and then adding these z-scores to come up with an overall rank for a state or Washington DC. This is not the only benefit of z-scores; they are often used in regression analyses to ease interpretation of the findings, a benefit we will see in action once we start working with regression models. 6.4 Chapter 6 Practice Problems Problem 1 Calculate the following areas: \\(P(z \\leq 1.96)\\) \\(P(z \\leq -1.96)\\) \\(P(z \\geq -0.87)\\) \\(P(-1.5 \\leq z \\leq 1.5)\\) \\(P(-1.5 \\leq z \\leq -1.0)\\) \\(P(z \\geq 2.58)\\) \\(P(z \\leq -2.58)\\) \\(P(-1.96 \\leq z \\leq 1.96)\\) Problem 2 Find the \\(z-scores\\) that leave the respective area above/below/between/beyond \\(P(z \\geq ?) = 0.02\\) \\(P(z \\leq ?) = 0.35\\) \\(P(z_{low} \\leq ? \\leq z_{high} ) = 0.95\\) in the middle of the distribution \\(P(z_{low} \\leq ? \\leq z_{high} ) = 0.99\\) in the middle of the distribution \\(P(z_{low} \\leq ? \\leq z_{high} ) = 0.90\\) in the middle of the distribution \\(P(z_{low} \\leq ? \\leq z_{high} ) = 0.50\\) in the middle of the distribution Problem 3 Babies born in singleton births in the United States have birth weights (in kilograms) that are distributed normally with \\(\\mu = 3.296; \\sigma = 0.560\\). What is the probability of a baby weighing more than 5 kilograms at birth? What is the probability of the baby weighing between 3 and 4 kilograms at birth? What fraction of babies will have birth weights more than 1.5 standard deviations from the mean in either direction? What fraction of the babies will have birth weights more than 1.5 kilograms from the mean in either direction? Problem 4 A survey of European mitochondrial DNA variation has found that the most common haplotype (genotype), known as “H”, occurs in 40% of people. If we sampled 400 Europeans, what is the probability that At least 180 are haplotype H? At least 130 are haplotype H? Between 115 and 170 (inclusive) are haplotype H? Problem 5 NASA excludes anyone under 62 inches in height and anyone over 75 inches in height from being an astronaut pilot. In metric units these cutoffs are 157.5 cm and 190.5 cm, respectively. Assume that heights are normally distributed with means and standard deviations of 177.6 cm and 9.7 cm for 20-29 year-old men, and 163.2 cm and 10.1 cm for 20-29 year-old women. What proportion of men and women in these age groups would be excluded from being NASA astronaut pilots? Problem 6 The most famous geyser in the world, Old Faithful in Yellowstone National Park, has a mean time between eruptions of 85 minutes. The interval of time between eruptions is normally distributed with a standard deviation of 21.25. What is the probability that a randomly selected time interval between eruptions is longer than 95 minutes? What is the probability that a randomly selected time interval between eruptions is shorter than 95 minutes? What is the probability that a randomly selected time interval between eruptions falls in the interval given by 75 and 95 minutes? Problem 7 The label on a one gallon jug of milk states that the volume of milk is 128 fluid ounces (fl.oz.) Federal law mandates that the jug must contain no less than the stated volume. The actual amount of milk in the jugs is normally distributed with mean \\(\\mu = 129\\) fl. Oz. and standard deviation \\(\\sigma = 0.8\\) fl. Oz. Find the z-score corresponding to a jug containing 128 fl. Oz. of milk. What is the probability that a randomly selected jug will contain less than 128 fl. Oz. of milk? Problem 8 In 2003, the U.S. Bureau of Labor Statistics reported mean annual household expenditure on food and drinks to be $5,700, with a standard deviation of $1,500. Assume these expenditures are normally distributed. How much do the 10% of families with the lowest annual household expenditures on food and drinks spend annually on food and drinks? What percentage of families spend more then $7,000 annually on food and drinks? How much do the families with the top 5% of annual expenditures on food and drinks spend annually? Problem 9 The mean time that a manager at the U.S. Bureau of Economic Analysis spends on annual performance reviews is 45 minutes, with a standard deviation of 9. Assume annual performance reviews are normally distributed. What percentage of annual performance reviews take more than 60 minutes? What percentage of annual performance reviews take between 30 and 60 minutes? What review times demarcate the top 5% and bottom 5% of annual performance reviews, respectively? Problem 10 Ohio University allows its faculty and staff to hold university-provided credit cards that can be used for authorized work-related charges. In a given fiscal year the average amount charged by a university employee is $1000, with a standard deviation of $200. What percent of employees charge more than $1000? What percent of employees charge more than $2000? What charge amount puts an employee in the \\(99^{th}\\) percentile? What charge amount puts an employee in the \\(9^{th}\\) percentile? Problem 11 The mean number of teletype machines that break down in an hour is 0.625. What is the probability of 2 machines breaking down in an hour? What is the probability of of more than 2 machines breaking down in an hour? Problem 12 Acme Call Centers’ customer service representatives receive customer service calls at 48 per hour. What is the probability of receiving 10 calls in a 15 minute interval? What is the probability of receiving at least 10 calls in a 15 minute interval? Suppose no calls are currently on hold. If the agent takes 15 minutes to complete the current call, how many calls do you expect to be waiting by that time? What is the probability of no calls waiting by that time? If no calls are currently being processed, what is the probability of the agent taking three minutes for a coffee break without being interrupted by a call? Problem 13 The National Highway Traffic Safety Administration’s (NHTSA) traffic fatality data for the 1994-2014 are available in this Excel file. Calculate and report the average annual number of fatalities. Based on this annual average, and assuming fatalities are evenly distributed across the 12 months in a year, how many fatalities should we expect per month? What is the probability of seeing less than 3500 fatalities in a month? What is the probability of seeing at least 3500 fatalities in a month? Problem 14 The 2014 FIFA World Cup in Brazil saw 32 teams participate, a total of 64 matches played, and 171 goals scored. Calculate and report the number of goals scored per match. What is the probability of exactly 3 goals being scored in a match? What is the probability of 5 or more goals being scored in a match? What is the probability of no goal being scored in a match? Are goals clumped or dispersed? Problem 17 Fire Engine Ladder Company 81 in Los Angeles responds to 20 calls per Saturday night in the month of July. Given the drought, they have been exceptionally busy in the preceding months and the crew is tired. (a) What is the probability that the company will receive no calls on the upcoming Saturday night and catch some well-deserved rest? (b) What is the probability of having to respond to fewer than 5 calls? (c) Are calls clumped or dispersed? Problem 18 The National Oceanic and Atmospheric Administration (NOAA) provides the following data on tropical cyclones per year and type in the Atlantic Basin. Major Hurricanes are those rated 3, 4, or 5 on the Saffir-Simpson Hurricane Scale. Given these data, how many major hurricanes should you expect in the next calendar year? What is the probability of seeing no major hurricanes next year? What is the probability of seeing more than three major hurricanes next year? Are major hurricanes clumped or dispersed? Problem 19 A bookstore in a Texas county was indicted by a grand jury on several counts of selling racist books. The jury was composed of 22 Baptists and 8 other individuals. Some 40% of the county is Baptist. Given their representation in the population, what is the probability of 22 Baptists serving on the grand jury? Problem 20 The Gallup Poll reports that “employees whose manager involves them in goal setting are four times more likely to be engaged than other employees. Yet this basic expectation only occurs for 30% of employees.” If you selected 20 random employees of state governmental agencies, What is the probability of fewer than 3 saying their manager involves them in goal setting? What is the probability of exactly 3 saying saying their manager involves them in goal setting? What would be the probability of exactly 6 saying saying their manager involves them in goal setting if you drew a random sample of 40 employees? Problem 21 The MPA program at a major university finds that one in five students withdraws before completing the mandatory introduction to statistics course. Assume that in the next Fall term 20 students are enrolled. What is the probability of two students withdrawing? What is the probability of more than two students withdrawing? What is the probability of no student withdrawing? What is the expected number of withdrawals? Problem 22 Military radar and missile detection systems are supposed to identify an attack and issue a warning. What matters though is their reliability. Assume the most state-of-the-art system has a reliability of 90%, meaning that it correctly detects an attack with a probability of 0.9. What is the probability that a single detection system will detect an attack? If two detection systems are installed in the same area and operate independently, what is the probability that at least one will detect an attack? What if three are installed? Would you recommend that multiple systems be installed? Explain why. Problem 23 On any given day in July of any year there is a 40% chance of a thunderstorm between 10:00 AM and 4:00 PM in the Grand Canyon. A hiker is considering a seven-day hike. What is the probability of her not running into any day with thunderstorms? What is the probability of her running into at most two days with thunderstorms? Problem 24 True/False multiple choice tests involving 10 questions, half of which are false, are poor tests because of the guess factor. An unprepared student can easily flip a coin and decide whether to mark True per False per question. If a student pursues this strategy, What is the probability that he/she will earn a passing grade (defined as seven or more right answers)? What is the probability that he/she will earn an A (defined as nine or more right answers)? "],["sampling.html", "Chapter 7 The Theory of Sampling Distributions 7.1 The Standard Error 7.2 The Central Limit Theorem 7.3 Applying the Standard Error and the Central Limit Theorem 7.4 The Sampling Distribution of Binary Proportions 7.5 Point Estimates 7.6 Interval Estimates 7.7 Student’s t distribution 7.8 Key Concepts to Remember 7.9 Chapter 7 Practice Problems", " Chapter 7 The Theory of Sampling Distributions Every time that we draw a sample, we hope that it does indeed reflect the population from which it was drawn. But how much faith can we place in this hope? As it turns out, quite a fair amount of faith so long as we haven’t violated key tenets of sampling, have a sufficiently large sample to work with, and understand key elements of the theory of sampling distributions. That is our holy grail so let us not dither a second. We can start by understanding two key elements of sampling, the science of drawing samples. One could draw samples in many different ways, several more complicated than others but at the heart of it all is our not violating two principles: Each and every observation in the population has the same chance of being drawn into the sample Each observation is drawn into the sample independently of all other observations For example, if sampling from applicants to a job-training program at the welfare center, perhaps to see if a specific job-training program works (or not), I shouldn’t select applicants who seem to be younger or more engaged or non-Hispanic Whites, and so on because this would mean anybody who did not meet the selection criterion would have a \\(0\\) probability of being sampled. Similarly, it would be a bad idea to select individuals who are obviously familiar with each other, maybe even related; by selecting one of them the chance of selecting another individual with them is far greater now than it would be if I didn’t know they knew each other, were related, etc. Samples that meet criteria (1) and (2) are referred to as simple random samples. We also draw a distinction between finite populations (where we know the population size) and infinite populations (where we do not know the population size). Finite Population: Draw a sample of size \\(n\\) from the population such that each possible sample of size \\(n\\) has an identical probability of selection We may sample with replacement, or sample without replacement (the more common – but not necessarily accurate – approach) Infinite Population: We sample such that Each element is selected independently of all other elements Each element has the same probability of being selected Now, assume we do this, that we draw all possible samples of a fixed sample size from a population. I’ll keep this simple by assuming the population is made up of only four numbers: 2, 4, 6, and 8. The population size, \\(N\\) is \\(4\\) and the population mean is \\(\\mu = \\dfrac{20}{4} = 5\\). Now, what if I drew all possible samples of exactly \\(2\\) numbers from this population and then calculated the sample mean \\(\\bar{x}\\) in each of these samples of \\(n = 2\\)? The result is shown below: TABLE 7.1: An Example of Sampling Distributions Sample No.  x1 x2 Mean 1 2 2 2 2 2 4 3 3 2 6 4 4 2 8 5 5 4 2 3 6 4 4 4 7 4 6 5 8 4 8 6 9 6 2 4 10 6 4 5 11 6 6 6 12 6 8 7 13 8 2 5 14 8 4 6 15 8 6 7 16 8 8 8 Eyeball the sample means in the column title \\(\\bar{x}\\) and you will see that the most commonly occurring sample mean is \\(\\bar{x}=5\\), which is equal to the population mean \\(\\mu = 5\\). Is this luck? Not at all. This is what the theory of sampling distributions tell us: On average, the sample mean will equal the population mean so long as the tenets of random sampling have not been violated. Formally, we state this as the Sampling Distribution of \\(\\bar{x}\\) is the probability distribution of all possible values of the sample mean \\(\\bar{x}\\). The Expected Value of \\(\\bar{x}\\) is \\(E(\\bar{x})=\\mu\\) and the Standard Deviation of \\(\\bar{x}\\) is the Standard Error of \\(\\bar{x}\\), calculated for (a) Finite Populations as \\(\\sigma_{\\bar{x}}=\\sqrt{\\dfrac{N-n}{N-1}}\\left(\\dfrac{\\sigma}{\\sqrt{n}}\\right)\\) and for (b) Infinite Populations as \\(\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}\\) Now, it should be somewhat intuitive that the larger the sample that we draw, the more likely is the sample mean to mirror the population mean. Is this true? Let us see with respect to the length of the human genome, something we have mapped almost completely, to the extent that we can deem it a known population. One way of seeing this in action is by first looking at the distribution of gene lengths (measured in nucleotides, “the sub-units that are linked to form the nucleic acids ribonucleic acid (RNA) and deoxyribonucleic acid (DNA), which serve as the cell’s storehouse of genetic information”). The population is shown below: FIGURE 7.1: Population Distribution of Human Gene Lengths The population mean human gene length is 2,622.027 and the standard deviation is 2,036.967. What would the distribution of sample means be if I drew 100 possible samples of \\(n=100\\) from this population? What if I drew 100 samples with \\(n=1,000\\), \\(n=10,000\\), \\(n=15,000\\)? What if I drew all possible samples of \\(n=100\\), \\(n=1,000\\), \\(n=10,000\\), \\(n=15,000\\), calculated the mean in each sample and plotted the distribution of these means? What would such a distribution look like? Would most sample means cluster around the population mean or would they be wandering off, all over the place? Let us see these distributions mapped out for us. Note that the white vertical line in the plots that follows marks the population mean human gene length \\((\\mu = 2,622.07)\\). FIGURE 7.2: Sampling Distributions of Human Gene Lengths Several noteworthy things are going on in these plots. First, individual sample means could end up anywhere in the distribution – some close to \\(\\mu\\) and some far away from \\(\\mu\\). Second, the larger the sample size, the more it seems that the resulting sample means tend to cluster around \\(\\mu\\). These two realizations should nudge you to wonder if sample size is the key to minimizing drift between your sample mean \\(\\bar{x}\\) and the population mean \\(\\mu\\). Yes, it is the key. 7.1 The Standard Error If an important element of doing reliable data analysis is to have our sample mean be as close as possible to the population mean, is there a measure of how far our sample mean might be from the population mean? Yes there is, and we call it the standard error, denoted by \\(\\sigma_{\\bar{x}}\\), where \\(\\sigma_{\\bar{x}} = \\dfrac{\\sigma}{\\sqrt{n}}\\). What does the standard error really tell us? Let us break its message down piece by piece. First, you don’t see the population and so you have to rely on your sample mean to guess what the population mean might be. As such, one could say that the expected value of the sample mean is the population mean, i.e., \\(E(\\bar{x}) = \\mu\\). Now, if you were so lucky as to end up with a sample mean that is \\(\\neq \\mu\\), how far away from \\(\\mu\\) do you think you might be? The answer is provided by the standard error, \\(\\sigma_{\\bar{x}}\\). It is telling you the average distance of a sample mean from the population mean for all samples of a given sample size drawn from a common population with standard deviation of \\(\\sigma\\). So clearly two things dictate how small or large the standard error could be – (i) how much variability there is in the population, i.e., the standard error \\((\\sigma_{\\bar{x}})\\), and (ii) how large a sample we are drawing, i.e., \\((n)\\). It should be very obvious that while we cannot influence variability in the population, sample size is under our control. Since \\(n\\) figures in the denominator in the formula \\(\\sigma_{\\bar{x}} = \\dfrac{\\sigma}{\\sqrt{n}}\\), the larger is \\(n\\), the smaller will \\(\\sigma_{\\bar{x}}\\). To recap, as \\(n \\rightarrow \\infty\\), \\(\\sigma_{\\bar{x}} \\rightarrow 0\\) … that is, as the sample size \\((n)\\) increases the standard error \\((\\sigma_{\\bar{x}})\\) decreases … \\[\\sigma=4000; n=30; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{4000}{\\sqrt{30}}=730.2967\\] \\[\\sigma=4000; n=300; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{4000}{\\sqrt{300}}=230.9401\\] \\[\\sigma=4000; n=3000; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{4000}{\\sqrt{3000}}=73.0296\\] and, as \\(\\sigma \\rightarrow \\infty\\), \\(\\sigma_{\\bar{x}} \\rightarrow \\infty\\) … that is, holding the sample size \\((n)\\) constant, samples drawn from populations with larger standard deviations \\((\\sigma)\\) will have larger standard errors \\((\\sigma_{\\bar{x}})\\) \\[\\sigma=40; n=30; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{40}{\\sqrt{30}}=7.3029\\] \\[\\sigma=400; n=30; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{400}{\\sqrt{30}}=73.0296\\] \\[\\sigma=4000; n=30; \\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{4000}{\\sqrt{30}}=730.2967\\] 7.2 The Central Limit Theorem But is this claim, of the sample mean \\(= \\mu\\) and if it isn’t, drifting, on average, by \\(\\sigma_{\\bar{x}}\\) true for all population distributions? Yes, it is, by virtue of the Central Limit Theorem which stipulates that “For any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means for samples of size \\(n\\) will approach the normal distribution with a mean of \\(\\mu\\) and standard error of \\(\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}\\) as \\(n \\rightarrow \\infty\\)”. This holds true so long as (i) the population is normally distributed, or (ii) the sample size is \\(\\geq 30\\) (note no reference is being made to the population being normally distributed). This is a very crucial theorem in statistics because it allows us to proceed with statistical analyses even when we cannot see the population (the default situation we almost always find ourselves in) so long as \\(n \\geq 30\\). 7.3 Applying the Standard Error and the Central Limit Theorem 7.3.1 Example 1 Tuition costs at state universities is \\(\\sim \\mu=\\$4260; \\sigma=\\$900\\). Sample of \\(n=50\\) is drawn at random. What is the sampling distribution of mean tuition costs? \\[\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{900}{\\sqrt{50}}= 127.28\\] What is \\(P(\\bar{x})\\) within $250 of \\(\\mu\\)? Sampling distribution is \\(\\mu = 4260; \\sigma_{\\bar{x}} = 127.28\\). Within \\(250\\) implies \\(P(4010 \\leq \\mu \\leq 4510)\\) \\[z_{4510}=\\dfrac{4510-4260}{127.28}=1.96\\] \\[z_{4010} = \\dfrac{4010-4260}{127.28} = - 1.96\\] \\[P(4010 \\leq \\mu \\leq 4510) = 0.95\\] What is \\(P(\\bar{x})\\) within $100 of \\(\\mu\\)? We want \\(P(4160 \\leq \\mu \\leq 4360)\\) \\[z_{4360}=\\dfrac{4360-4260}{127.28}=0.79\\] \\[z_{4160} = \\dfrac{4160-4260}{127.28} = - 0.79\\] \\[P(4160 \\leq \\mu \\leq 4360) = 0.5704\\] 7.3.2 Example 2 Average annual cost of automobile insurance is \\(\\sim \\mu=\\$687; \\sigma=\\$230\\) and \\(n= 45\\). Therefore, \\(\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{230}{\\sqrt{45}}= 34.29\\). So the sampling distribution is \\(\\mu = 687; \\sigma_{\\bar{x}} = 34.29\\) What is \\(P(\\bar{x})\\) within \\(\\$100\\)? We need \\(P(587 \\leq \\mu \\leq 787)\\) \\[z_{787}=\\dfrac{787-687}{34.29}=2.92\\] \\[z_{587} = \\dfrac{587-687}{34.29} = -2.92\\] \\[P(587 \\leq \\mu \\leq 787) = 0.9964\\] What is \\(P(\\bar{x})\\) within \\(\\$25\\)? We need \\(P(662 \\leq \\mu \\leq 712)\\) \\[z_{712}=\\dfrac{712-687}{34.29}=0.73\\] \\[z_{662} = \\dfrac{662-687}{34.29} = -0.73\\] \\[P(662 \\leq \\mu \\leq 712) = 0.5346\\] If insurance agency wants to be within $25, would you recommend a larger sample? Yes, since \\(n = 45\\) is unlikely to yield the desired result. 7.4 The Sampling Distribution of Binary Proportions The theory of sampling distributions also extends to binomial random variables. For example, assume that leadership training is sought and completed by some public agency personnel, mid-level perhaps. If we wanted to evaluate what proportion of mid-level public agency personnel have leadership training, we could draw a random sample. Let us assume that in the population, the proportion with such training happens to be \\(0.20\\). Let us generate a sequence of 1,000,000 randomly drawn samples, first with \\(n=20\\), then with \\(n=40\\), and finally with \\(n=100\\). The plots below show you the average proportion with leadership training. In each case, but most so in panels (b) and (c), we see what looks like a normal proportion. The takeway should be that in larger samples the proportion with leadership training is more likely to mirror the true proportion in the population, which turns out to be 0.20 (i.e., 20% of mid-level personnel have completed leadership training). FIGURE 7.3: Sampling Distributions of Proportions In general, then, the theory of sampling distributions applies as well to proportions as it does to means of continuous variables. Before we move on, however, let us see how the standard error is calculated for proportion and a few other key points. The sample proportion is denoted by \\(\\bar{p}=\\dfrac{x}{n}\\) and we expect, on average, that the expected value of the sample proportion is the population proportion, i.e., \\(E(\\bar{p})=p\\). The standard error (aka the standard deviation of \\(\\bar{p}\\)) is calculated as \\(\\sigma_{\\bar{p}}={\\sqrt{\\dfrac{p(1-p)}{n}}}\\), and we can safely assume that the sampling distribution of \\(\\bar{p}\\) is approximately normally distributed if \\(np \\geq 5\\) AND \\(n(1-p) \\geq 5\\) (both conditions must be met).9 Note: When the population proportion \\(p\\) is unknown, the standard error is calculated as \\(s_{\\bar{p}}={\\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n}}}\\) 7.4.1 Example 1 The Governor’s office reports 56% of US households have internet access. If \\(p=0.56\\) and \\(n=300\\), what is: The sampling distribution of \\(\\bar{p}\\)? Given \\(n=300; p=0.56\\), \\(\\sigma_{\\bar{p}}={\\sqrt{\\dfrac{0.56(1-0.56)}{300}}}=0.0287\\) What is \\(Probability (\\bar{p})\\) within \\(\\pm 0.03\\)? We are looking for the interval \\((0.53,0.59)\\) and so we calculate: \\[z_{0.59}=\\dfrac{0.59-0.56}{0.0287}=1.0452\\] \\[z_{0.53}=\\dfrac{0.53-0.56}{0.0287}=-1.0452\\] and hence \\(P(0.53 \\leq \\bar{p} \\leq 0.59)=0.7031\\) Calculate (1) and (2) for \\(n=600\\) and \\(n=1000\\), respectively. 7.4.2 Example 2 The Federal Highway Administration says 76 percent of drivers read warning signs placed by the roadside. Let \\(p=0.76\\) and \\(n=400\\). What is: The sampling distribution of \\(\\bar{p}\\)? Given \\(n=400; p=0.76\\), \\(\\sigma_{\\bar{p}} = {\\sqrt{\\dfrac{0.76(1-0.76)}{400}}} = 0.0214\\) \\(\\text{Probability}(\\bar{p})\\) within \\(\\pm 0.03\\)? We need \\((0.73,0.79)\\) so we calculate \\(z_{0.73}=\\dfrac{0.73-0.76}{0.0214} = -1.40\\) and \\(z_{0.79}=\\dfrac{0.79-0.76}{0.0214} = 1.40\\), and hence \\(P(0.73 \\leq \\bar{p} \\leq 0.79) = 0.8385\\) Repeat (1) and (2) for a sample of \\(n=1600\\) 7.5 Point Estimates Statistics calculated for a population are referred to as population parameters (for e.g., \\(\\mu\\); \\(\\sigma^{2}\\); \\(\\sigma\\)). On the other hand, statistics calculated for a sample are referred to as sample statistics (for e.g., \\({\\bar{x}}\\); \\(s^{2}\\); \\(s\\)). Since populations are beyond our reach we rely on samples to calculate point estimates. A point estimator is a sample statistic that predicts (or estimates) the value of the corresponding population parameter as for, example, in the sample mean \\(\\bar{x}\\) predicting/estimating the population mean \\(\\mu\\), the sample proportion \\(\\bar{p}\\) predicting/estimating the population proportion \\(p\\), and so on. Not all point estimators are created equal, however. In fact, desirable point estimators have the following properties: The sampling distribution of the point estimator is unbiased – it is centered around the population parameter. An unbiased estimator would be, for example, one that gives us \\(E(\\bar{x}) = \\mu\\). If \\(E(\\bar{x}) - \\mu \\neq 0\\), we would have bias. Think of this as a case where the weighing scale in my house is not well calibrated and always shows the weight to be 10 pounds more than it actually happens to be. A different scale might always show a lower weight by 7.5 pounds than the true weight. Both these weighing scales are biased, one upwards the other downwards. The point estimator is efficient – it has the smallest possible variance. Why does efficiency matter? It matters because if the point estimator has large variance we know our standard error will be large (recall: the numerator would be large), and hence the point estimator could have greater drift from the true population parameter. Thus, when choosing a point estimator from the set of point estimators we could work with, we prefer the one that has minimum variance. The point estimator is consistent – it tends towards the population parameter as the sample size increases. This is the result that with increasingly larger samples the point estimate will come increasingly closer to the population parameter. The easiest way to understand bias and efficiency is via the following graphic authored by Sebastian Raschka: FIGURE 7.4: Bias and Efficiency of point estimates The ideal point estimator has no bias and the smallest variance of all point estimators available to us, as shown by the red dots (each dot referencing a point estimate) in the top-left panel titled Low Bias (Accurate), Low variance (Precise). But a point estimate is by itself quite risky because it presents you with one possible value of the corresponding population parameter. For example, your sample might show median household income as \\(\\bar{x} = 45,000\\) and so you claim this is what the median household income must be for the population. However, if I asked you to bet a substantial amount of money on this estimate being accurate, you would (or should) not take the bet because there is a reasonable chance of you being wrong. Instead, if I asked you to give me a range of household income within which the population mean is likely to fall, now you have a span of values you could offer up with more confidence. This span of values is what we call interval estimates. 7.6 Interval Estimates Although point estimates (for e.g., \\(\\bar{x}\\)) tell us the expected value of a parameter based on a sample statistic, we know samples are not all the same even though they were drawn randomly, and have the same sample size. So we try to ask: How much confidence can we place in the point estimate? The margin of error helps us answer this question. Formally, the interval estimate of \\(\\mu = \\bar{x} \\pm \\text{ Margin of error}\\) and the interval estimate of \\(p = \\bar{p} \\pm \\text{ Margin of error}\\) where the margin of error \\(= \\left(z \\times \\sigma \\right)\\). In a base sense, what the margin of error is telling you is how far off, above or below the population mean or proportion, you should expect your sample mean or proportion to be. Hence the reference made in surveys to “The margin of error for this survey is \\(\\pm 2.5\\) percent”, and so on. In any normal distribution of sample means with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the following statement is true: Over all samples of size \\(n\\), the probability is \\(0.95\\) for the event in question. That is, the distance between the sample mean and the population mean will fall in an interval covered by the margin of error: \\(-1.96\\sigma \\leq \\bar{x}-\\mu \\leq +1.96\\sigma\\). Rearranging the former identity yields: \\(\\bar{x}-1.96\\sigma \\leq \\mu \\leq \\bar{x} +1.96\\sigma\\). This expression says that in all possible samples of identical size \\(n\\), there is a 95% probability that \\(\\mu\\) falls within a span of values given by \\(\\bar{x} \\pm 1.96\\sigma\\). The range of values within \\(\\bar{x} \\pm 1.96 \\sigma\\) yield the 95 percent confidence interval (CI) of \\(\\mu\\), and the two boundaries are the 95 percent confidence interval limits. Be careful! We aren’t saying \\(\\mu\\) falls within a known interval, although this is how most people treat confidence intervals. Rather, what we are saying is that in repeated random sampling if we drew all possible samples of a specific sample size \\(n\\), calculated the 95 percent confidence interval in each sample, then plotted these confidence intervals, 95 percent of these 95 percent confidence intervals will include \\(\\mu\\). By extension, this also means that some 5 percent of these 95 percent confidence intervals will not include \\(\\mu\\). Watch the graph that follows; it shows you this principle in action. In particular, the graph shows 100 confidence intervals as horizontal lines, with a vertical line demarcating the value of the population mean \\((\\mu = 20)\\). Red horizontal lines are 95% confidence intervals that have completely missed the population mean. FIGURE 7.5: 95 Percent Confidence Intervals in 100 Random Samples The second plot does the same thing anew by drawing 100 random samples and mapping the confidence intervals. This time around eight of these intervals miss the population mean. FIGURE 7.6: 95 Percent Confidence Intervals in 100 Random Samples Before we move on to some examples, note the z-score values used in calculating the margin of error: z = 1.645 for a 90% confidence level z = 1.960 for a 95% confidence level z = 2.576 for a 99% confidence level 7.6.1 Example 1 Let \\(\\bar{x}=32; \\sigma=6\\). Let also \\(n=50\\). What is the standard error? \\(\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{6}{\\sqrt{50}}=0.8485\\) Find \\(90\\%\\)CI \\(= 32 \\pm 1.645(0.8485)=32 \\pm 1.3957=(30.6043; 33.3957)\\) Find \\(95\\%\\)CI \\(= 32 \\pm 1.960(0.8485)=32 \\pm 1.6630=(30.3370; 33.6630)\\) Find \\(99\\%\\)CI \\(= 32 \\pm 2.576(0.8485)=32 \\pm 2.1857=(29.8143; 34.1857)\\) 7.6.2 Example 2 Let household mean television viewing time be \\(\\sim \\bar{x}=8.5; \\sigma=3.5\\) and \\(n=300\\). Therefore \\(\\sigma_{\\bar{x}}=\\dfrac{\\sigma}{\\sqrt{n}}=\\dfrac{3.5}{\\sqrt{300}}=0.2020\\) Find the \\(95\\)% CI \\(= 8.5 \\pm 1.960(0.2020)=8.5 \\pm 0.3959 = (8.1041; 8.8959)\\) What is the \\(99\\)% CI? \\(= 8.5 \\pm 2.576(0.2020) = 8.5 \\pm 0.520352 = (7.979648; 9.020352)\\) 7.6.3 Interval Estimates for Binary Proportions A similar logic applies to confidence intervals for binary proportions as well, with some differences in how the intervals are calculated. For one, since we don’t know the population proportion \\(p\\) we calculate the standard error \\(s_{\\bar{p}}\\) as \\(s_{\\bar{p}} = {\\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n}}}\\). With the standard error in hand we could calculate a variety of confidence intervals but the three most often seen will be (i) the Agresti-Coull confidence intervals, (ii) the Wald confidence interval, and (iii) Wilson’s confidence intervals. Agresti-Coull Confidence Intervals The Agresti-Coull confidence interval requires us to first calculate \\(p^{&#39;} = \\dfrac{x + 2}{n + 4}\\) and then calculate \\[p^{&#39;} - z_{\\alpha/2}\\sqrt{ \\dfrac{p^{&#39;} \\left(1-p^{&#39;} \\right) } {n+4} } \\leq p \\leq p^{&#39;} + z _{\\alpha/2}\\sqrt{ \\dfrac{p^{&#39;} \\left(1-p^{&#39;} \\right) } {n+4} }\\] Wald Confidence Intervals Wald confidence intervals are calculated with the usual standard error and as: \\[\\bar{p} - z_{\\alpha/2}\\sqrt{ \\dfrac{\\bar{p} \\left(1-\\bar{p} \\right) } {n} } \\leq p \\leq \\bar{p} + z_{\\alpha/2}\\sqrt{ \\dfrac{\\bar{p} \\left(1-\\bar{p} \\right) } {n} }\\] These intervals are normal theory approximations and can be used as the default approach. However, they will yield very conservative intervals (i.e., confidence intervals that will be wider than they really should be) when (i) \\(n\\) is small or (ii) \\(p\\) is close to \\(0\\) or \\(1\\). In those cases you can rely upon the Agresti-Coull approach or, better yet, the Wilson approach (see below). Wilson’s Confidence Interval Wilson’s interval is calculated as follows: \\[\\dfrac{n}{n + z^2_{\\alpha/2}}\\left[ \\left(\\bar{p} + \\dfrac{z^2_{\\alpha/2}}{2n} \\right) \\pm z_{\\alpha/2} \\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n} + \\dfrac{z^2_{\\alpha/2}}{4n^2}} \\right ]\\] which converts to \\[\\dfrac{n}{n + z^2_{\\alpha/2}}\\left[ \\left(\\bar{p} + \\dfrac{z^2_{\\alpha/2}}{2n} \\right) - z_{\\alpha/2} \\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n} + \\dfrac{z^2_{\\alpha/2}}{4n^2}} \\right ] \\leq p \\leq \\dfrac{n}{n + z^2_{\\alpha/2}}\\left[ \\left(\\bar{p} + \\dfrac{z^2_{\\alpha/2}}{2n} \\right) + z_{\\alpha/2} \\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n} + \\dfrac{z^2_{\\alpha/2}}{4n^2}} \\right ]\\] These are not the only intervals one could use and statistics software packages do not always use the same default approach. Hence it is always a good idea to check what method is being used by your software. Before we move on to a few examples, note what happens at differing values of \\(\\bar{p}\\) for a given sample size. TABLE 7.2: The Binomial Proportion Revisited Proportion 1 - Proportion (Proportion) * (1 - Proportion) 0.00 1.00 0.0000 0.05 0.95 0.0475 0.10 0.90 0.0900 0.15 0.85 0.1275 0.20 0.80 0.1600 0.25 0.75 0.1875 0.30 0.70 0.2100 0.35 0.65 0.2275 0.40 0.60 0.2400 0.45 0.55 0.2475 0.50 0.50 0.2500 0.55 0.45 0.2475 0.60 0.40 0.2400 0.65 0.35 0.2275 0.70 0.30 0.2100 0.75 0.25 0.1875 0.80 0.20 0.1600 0.85 0.15 0.1275 0.90 0.10 0.0900 0.95 0.05 0.0475 1.00 0.00 0.0000 Note what happens when \\(\\bar{p}=0\\) and \\(\\bar{p}=1\\): Your numerator in the formula \\(s_{\\bar{p}} = \\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n}}\\) is driven to \\(0\\) and hence so is the standard error. So this problem occurs at the extremes of \\(\\bar{p}\\). Where the numerator \\(\\bar{p}(1 - \\bar{p})\\) is the widest is when \\(\\bar{p}=0.50\\); as \\(\\bar{p}\\) moves away towards \\(0\\) or \\(1\\), \\(\\bar{p}(1 - \\bar{p})\\) steadily decreases. At the extremes, the Wilson will do well but not so the Wald or Agresti-Coull If you calculated the garden variety of confidence intervals for a case where \\(\\bar{p} = 0.04166667\\) you would see the following: FIGURE 7.7: 95 percent Confidence Intervals when Sample Proportion = 0.0417 Note the variance in how wide the intervals are and that some of them cross \\(0\\), which doesn’t make sense because you cannot have a negative sample proportion. On the other hand, if you calculated the garden variety of confidence intervals for a case where \\(\\bar{p} = 0.5\\) you would see muted differences between the different methods used to calculate confidence intervals (see below). FIGURE 7.8: 95 percent Confidence Intervals when Sample Proportion = 0.5000 7.6.4 Example 1 In a survey of a small town in Vermont, voters were asked if they would support closing the local schools and sending the local kids to schools in the neighboring town to more efficiently utilize local tax dollars. A random sample of 153 voters yields 43.1% favoring school closures. What is the 95% confidence interval for the population proportion? Use the Wald approach. We have \\(\\bar{p}=0.431\\) and \\(n = 153\\). This yields a standard error of \\(s_{\\bar{p}} = \\sqrt{\\dfrac{0.431(1-0.431)}{153}} = 0.04003585\\). Given \\(z = \\pm 1.96\\) the confidence interval is: \\(0.431 \\pm 1.96(0.04003585) = 0.3525297 \\text{ and } 0.5094703\\). This can be loosely interpreted as: We are about 95% confident that the population proportion of the town’s voters that support school closures lies in the interval given by 0.3525 and 0.5094. 7.6.5 Example 2 Paper currency in the US often comes into contact with cocaine either directly or indirectly during drug deals or usage, or in counting machines where it wears off from one bill to the next. A forensic survey collected fifty $1 bills and traced cocaine on forty-six bills. What is the best estimate of the proportion of US$1 bills that have detectable levels of cocaine? The best estimate would be our sample proportion of \\(\\bar{p} = \\dfrac{46}{50} = 0.92\\). What is the 99% confidence interval for this estimate? Since our sample proportion is close to \\(1\\) we should use either the Wilson method or the Agresti-Coull approach. Here, the Wilson interval will be given by: 0.7657 and 0.9758. We can be about 99% certain that the proportion of US$1 bills with detectable levels of cocaine lies in the interval 0.7657 and 0.9758. The calculations are shown below: \\[\\dfrac{n}{n + z^2_{\\alpha/2}}\\left[ \\left(\\bar{p} + \\dfrac{z^2_{\\alpha/2}}{2n} \\right) - z_{\\alpha/2} \\sqrt{\\dfrac{\\bar{p}(1-\\bar{p})}{n} + \\dfrac{z^2_{\\alpha/2}}{4n^2}} \\right ]\\] \\[= \\dfrac{50}{50 + 2.58^2}\\left[ \\left(0.92 + \\dfrac{2.58^2}{100} \\right) - 2.58 \\sqrt{\\dfrac{0.92(1-0.92)}{50} + \\dfrac{2.58^2}{10000}} \\right ]\\] and \\[= \\dfrac{50}{50 + 2.58^2}\\left[ \\left(0.92 + \\dfrac{2.58^2}{100} \\right) + 2.58 \\sqrt{\\dfrac{0.92(1-0.92)}{50} + \\dfrac{2.58^2}{10000}} \\right ]\\] These are algebraically complex calculations and hence better done via a software package. Note: There used to be a preference for the exact confidence interval (available in most software packages), and these are an interesting measure but the theory and algebra involved in calculating these intervals go beyond the scope of the class but you should be aware of them. You can use this online calculator for exact confidence intervals. These intervals will not be symmetric about the mean (i.e., extending the same distance above as they do below the mean). This is because a proportion cannot drop below \\(0\\) or go beyond \\(1\\). However, the intervals that use the normal approximation (like the Wald and the Agresti-Coull methods) do not recognize these constraints and hence fail when dealing with extreme sample proportions. However, of late consensus has veered in favor of the Wilson interval for small n and the Agresti-Coull for large n; see this article if you are interested in learning more about this guidance. 7.7 Student’s t distribution In the preceding work with standard errors, margins of error, and confidence intervals we made a bold assumption – that we knew \\(\\sigma\\), variability in the population. However, if we don’t know \\(\\mu\\) and hence are trying to come up with the most precise estimate of \\(\\mu\\) via our sample \\(\\bar{x}\\), how can we really know \\(\\sigma\\)? We cannot, and this is where Student's t distribution comes into play. This distribution was the brainchild of William Sealy Gosset, an employee of the Guinness Brewery. He worked with Karl Pearson on figuring out the challenges of using the standard normal distribution with small samples and unknown population variances. The distribution looks like the standard normal distribution but is fatter in the tails since in small samples more extreme results are possible. An applet is available here but let us see how this distribution works and why it matters. Assume \\(X \\sim N(13,2)\\). If \\(x = 12, n=16\\), what is \\(z_{x=12}\\)? It turns out that \\(z_{x=12}=2\\). Now, what if we don’t know \\(\\mu\\) and \\(\\sigma\\) and draw three samples, each with \\(n=16\\) but \\(s\\) differs in each sample? TABLE 7.3: The t distribution: An Example Sample No.  Std. Dev. Std. Error z-score 1 4 1.00 -1 2 2 0.50 -2 3 1 0.25 -4 Notice what happens here: Each sample yields a different value of \\(z\\) for the same \\(x=12\\) simply because each sample is generating differing \\(\\bar{x}\\) and \\(s\\). This leads to our making far more mistakes than we should if we continue to rely on the \\(z\\) distribution. However, if you switch to the \\(t\\) distribution we gain back some precision. The \\(t = \\dfrac{x - \\bar{x}}{s_{\\bar{x}}}\\) where \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\) and \\(t\\) is distributed with \\(n-1\\) degrees of freedom. FIGURE 7.9: Student’s t versus the Standard Normal z Distribution The red distribution is for the standard normal (\\(z\\)), and the blue distributions are for varying degrees of freedom, first for \\(df=1\\) (panel (a)), then for \\(df=2\\) (panel (b)), then for \\(df=3\\) (panel (c)), and the black distribution is for \\(df=29\\) (panel (d)). Notice that the distribution for \\(df=29\\) is very close to the \\(z\\) distribution. These plots illustrate the work of the \\(t\\) distribution; for small samples, which then have smaller degrees of freedom, the \\(t\\) deviates from the \\(z\\) but as the \\(df\\) increases, the \\(t\\) starts mirroring the \\(z\\) distribution. So is there a rule when you should use the \\(t\\) instead of the \\(z\\)? Yes there is: Use the \\(t\\) distribution whenever \\(\\sigma\\) is unknown and hence the sample standard deviation \\((s)\\) must be used, regardless of the sample size Use the \\(t\\) distribution whenever \\(n &lt; 30\\) even if \\(\\sigma\\) is known Note that disciplines vary in how they settle the question of when to use the \\(z\\) versus the \\(t\\). For example, some will use the \\(t\\) whenever the sample size is less than 200 even if \\(\\sigma\\) is known while others will use \\(z\\) whenever \\(\\sigma\\) is known regardless of the sample size. More prudent analysts will look for outliers and/or heavily skewed data, and if they see either of these issues, they may not rely on the \\(z\\) unless the sample size is at least 50 or more (others would hold a higher standard, that of 100 or 200 data points). For our purposes, however, we will stick with the rule-of-thumb listed above for now. 7.7.1 Example 1 Find the following \\(t\\) score(s): \\(t\\) leaves \\(0.025\\) in the Upper Tail with \\(df=12\\). Answer: \\(t=2.179\\) \\(t\\) leaves \\(0.05\\) in the Lower Tail with \\(df=50\\). Answer: \\(t=-1.676\\) \\(t\\) leaves \\(0.01\\) in the Upper Tail with \\(df=30\\). Answer: \\(t=2.457\\) \\(90\\%\\) of the area falls between these \\(t\\) values with \\(df = 25\\). Answer: \\(t= \\pm 1.708\\) \\(95\\%\\) of the area falls between these \\(t\\) values with \\(df = 45\\). Answer: \\(t= \\pm 2.014\\) 7.7.2 Example 2 Simple random sample with \\(n=54\\) yielded \\(\\bar{x} = 22.5\\) and \\(s=4.4\\). Calculate the standard error. \\(s_{\\bar{x}}=\\dfrac{s}{\\sqrt{n}}=\\dfrac{4.4}{\\sqrt{54}}=\\dfrac{4.4}{7.34}=0.59\\) What is the \\(90\\%\\) confidence interval? Answer: \\(\\bar{x}\\pm t(s_{\\bar{x}})=22.5 \\pm 1.674(0.59)=22.5 \\pm 0.98=21.52; 23.48\\) What is the \\(95\\%\\) confidence interval? Answer: \\(\\bar{x}\\pm t(s_{\\bar{x}})=22.5 \\pm 2.006(0.59)=22.5 \\pm 1.18=21.32; 23.68\\) What is the \\(99\\%\\) confidence interval? Answer: \\(\\bar{x}\\pm t(s_{\\bar{x}})=22.5 \\pm 2.672(0.59)=22.5 \\pm 1.57=20.93; 24.07\\) What happens to the margin of error and the width of the interval as we increase how “confident” we want to be? Answer: The margin of error increases and the confidence interval widens 7.7.3 Example 3 Pilots flying UN Peacekeeping missions fly, on average, 49 hours per month. This is based on a random sample with \\(n=100\\) with \\(s=8.5\\). Find the margin of error for the 95% confidence interval. \\(s_{\\bar{x}}=\\dfrac{s}{\\sqrt{n}}=\\dfrac{8.5}{\\sqrt{100}}=\\dfrac{8.5}{10}=0.85\\) and thus \\(t(s_{\\bar{x}})=1.984(0.85)=1.68\\) What is the \\(95\\%\\) confidence interval? \\(\\bar{x}\\pm t(s_{\\bar{x}})=49 \\pm 1.984(0.85)=49 \\pm 1.68=47.32; 50.58\\) How might we loosely interpret this interval? We might say that we can about 95% certain/confident that the population mean hours flown by pilots serving UN Peacekeeping missions falls in the interval given by \\(47.32\\) hours and \\(50.58\\) hours. 7.8 Key Concepts to Remember If given \\(\\sigma\\) and \\(n \\geq 30\\), use the \\(z\\) If given \\(\\sigma\\) and \\(n &lt; 30\\), use the \\(t\\) If \\(\\sigma\\) is not provided and hence you have to use \\(s\\), use the \\(t\\) regardless of the sample size Expected value of the sample mean is the population mean: \\(E(\\bar{x}) = \\mu\\) Standard Error is \\(\\sigma_{\\bar{x}} = \\dfrac{\\sigma}{\\sqrt{n}}\\) when \\(\\sigma\\) is given and \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\) when \\(\\sigma\\) is unknown Confidence Interval is given by \\(\\bar{x} \\pm z_{\\alpha/2} \\times \\sigma_{\\bar{x}}\\) when using the \\(z\\) and \\(\\bar{x} \\pm t_{\\alpha/2} \\times s_{\\bar{x}}\\) when using the \\(t\\) 7.9 Chapter 7 Practice Problems Problem 1 Babies born in singleton births in the United States have birth weights (in kilograms) that are distributed normally with \\(\\mu = 3.296; \\sigma = 0.560\\). If you took a random sample of a 100 babies, what is the probability that their mean weight \\(\\bar{x}\\) would be greater than 3.5 kilograms? If you took a random sample of a 25 babies, what is the probability that their mean weight \\(\\bar{x}\\) would be greater than 3.5 kilograms? Problem 2 The most famous geyser in the world, Old Faithful in Yellowstone National Park, has a mean time between eruptions of 85 minutes. The interval of time between eruptions is normally distributed with a standard deviation of 21.25. Suppose a simple random sample of 100 time intervals between eruptions was gathered. What is the probability that in this sample the mean eruption interval is longer than 95 minutes? Suppose a simple random sample of 16 time intervals between eruptions was gathered. What is the probability that in this sample the mean eruption interval is shorter than 95 minutes? Problem 3 The label on a one gallon jug of milk states that the volume of milk is 128 fluid ounces (fl.oz.) Federal law mandates that the jug must contain no less than the stated volume. The actual amount of milk in the jugs is normally distributed with mean \\(\\mu = 129\\) fl. Oz. and standard deviation \\(\\sigma = 0.8\\) fl. Oz. Each shift, eight jugs of milk are randomly selected for thorough testing. The products are tested for filling volume, temperature, contamination, fat content, packaging defects, label placement, etc. Find the z-score corresponding to a sample mean volume (for these eight jugs) of 128 fl. Oz. What is the probability that the sample mean of the volume for eight jugs is less than 128 fl. Oz.? (Give your answer accurate to 4 decimal places.) What is the probability that the sample mean of the volume for eight jugs is greater than 128 fl. Oz.? (Give your answer accurate to 4 decimal places.) Problem 4 In 2001 the average price of gasoline in the US was $1.46 per gallon, with a standard deviation of $0.15. What is the probability that the mean price per gallon is within $0.03 of the population mean if you are working with a sample of 30 randomly selected gas stations? What is the probability that the mean price per gallon is within $0.03 of the population mean if you are working with a sample of 50 randomly selected gas stations? What is the probability that the mean price per gallon is within $0.03 of the population mean if you are working with a sample of 100 randomly selected gas stations? Would you recommend a sample size of 30, 50, or 100 to have at least a 0.95 probability that the sample mean is within $0.03 of the population mean? Problem 5 In a random sample of 120 students enrolled in graduate business degree programs, the average undergraduate grade point average (GPA) was found to 3.75. The population standard deviation is supposed to be 0.28. What is the expected value of the average GPA of the population of students enrolled in graduate business degree programs? What is the 95% confidence interval for this mean GPA? What is the 99% confidence interval for this mean GPA? What happens to the confidence interval as you move from (b) to (c)? Why? Briefly explain. Problem 6 These data are records for a random sample of delayed flight arrivals on a single day at Chicago’s (IL) O’Hare Airport. Delays are reported in minutes. Calculate the (i) mean, and the (ii) standard deviation of delays. Calculate the standard error. What is the expected value of the average delay for the population of all flights arriving at this airport? What is the 95% confidence interval of average delay? What is the 99% confidence interval of average delay? If your sample size doubled, what would happen to the standard error? If your sample size quadrupled, what would happen to the standard error? Problem 7 If school absenteeism rate rises above 10% for a district, the state reduces its aid to the school district. Harpo Independent School District, a large urban district, draws a sample of five schools and find absenteeism rates to be 5.4%, 8.6%, 4.1%, 8.9%, and 7.8%, respectively. What is your best estimate of the absenteeism rate in the school district? What is the probability that the district’s absenteeism rate is more than 10%? Find the 95% confidence interval for the district’s absenteeism rate. How could you improve the precision of this confidence interval? Problem 8 In 1955 John Wayne played Genghis Khan in The Conqueror, a movie shot (unfortunately) downwind of a site where 11 aboveground nuclear bomb tests were carried out. Of the cast and crew of 220 who worked on the movie, by the early 1980s some 91 had been diagnosed with cancer. What is your best estimate of the population proportion of individuals exposed to these sites and diagnosed with cancer a little over two decades later? What is the 95% confidence interval for this proportion? Problem 9 A common perception is that individuals with chronic illnesses may be able to delay their deaths until after a special upcoming event, like a major holiday, a wedding, etc. Out of 12,028 deaths from cancer in a given year, 6052 occurred in the week before a special upcoming event. What is the best estimate of the population proportion of deaths that occur in the week before a special upcoming event? What about in the week after a special event? What are the respective 95% confidence intervals? Problem 10 In a certain community with contaminated potable water supply, when pollsters asked a random sample of 232 residents if they would be willing to still use the water, 32 said “yes”. Find the 95% confidence interval for the population proportion likely to say “yes”. Find the 99% confidence interval for the population proportion likely to say “yes”. Based on both confidence intervals, should the city make plans for alternative water delivery to residents because a majority of the population will not continue using the contaminated water? Explain. Problem 11 In 1987, researchers studying the spread of AIDS collected data from a volunteer sample of 4955 adult males in Baltimore, Chicago, Los Angeles, and Pittsburgh. Of these men, 38% tested positive for AIDS. What is the 95% confidence interval for the population proportion of adult males that might test positive for AIDS? Since these data were gathered in a volunteer sample, they are clearly non-random. How might this impact your confidence interval? Explain. Problem 12 Use the data provided here on motor vehicle occupant death rate, by age and gender, 2012 &amp; 2014 to answer the questions that follow. Rate of deaths are calculated by age and gender (per 100,000 population) for motor vehicle occupants killed in crashes. What is the best estimate of the population proportion of the death rate for all ages in your state in (i) 2012 versus (ii) 2014? What is the 95% confidence interval for each estimate calculated above? Technically, for a finite population, \\(\\sigma_{\\bar{p}}={\\sqrt{\\dfrac{N-n}{N-1}}}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\) and for an infinite population \\(\\sigma_{\\bar{p}}={\\sqrt{\\dfrac{p(1-p)}{n}}}\\)↩︎ "],["hypothesis.html", "Chapter 8 The Logic of Hypothesis Testing 8.1 Articulating the Hypotheses to be tested 8.2 Type I and Type II Errors 8.3 The Process of Hypothesis Testing: An Example 8.4 Confidence Intervals for Hypothesis Tests 8.5 The One-Sample t-test 8.6 The Two-group (or Two-sample) t-test 8.7 Paired t-tests 8.8 Chapter 8 Practice Problems", " Chapter 8 The Logic of Hypothesis Testing All of us hypothesize all the time. We look at the world around us, something that happened yesterday or might happen tomorrow, and try to concoct a story in our head about the chance of yesterday’s/tomorrow’s event happening. We may even weave together a silent, internalized narrative about how the event came about. These stories in our head are conjectures, our best guesses, maybe even biased thoughts, but they have not been tested with data. For example, you may be a city employee in the Streets and Sanitation department who feels the city has gotten dirtier over the past five years. You may be an admissions counselor who thinks the academic preparation of the pool of high school graduates applying to your university has gotten better over the years. Maybe you work as a program evaluation specialist in the Ohio Department of Education and need to find out if a dropout recovery program has made any difference in reducing high-school dropout rates. These suspicions, the motivating questions, are hypotheses that have not yet been tested with data. If you were keen to test your hypothesis, you would have to follow an established protocol that starts with a clear articulation of the hypotheses to be tested. Formally, we define hypothesis testing as an inferential procedure that uses sample data to evaluate the credibility of a hypothesis about a population parameter. The process involves (i) stating a hypothesis, (ii) drawing a sample to test the hypothesis, and (iii) carrying out the test to see if the hypothesis should be rejected (or not). Before we proceed, however, memorize the following statement: A hypothesis is an assumption that can neither be proven nor disproven without a reasonable shadow of doubt, a point that will become all too clear by the time we end this chapter.10 8.1 Articulating the Hypotheses to be tested Hypotheses are denoted by \\(H\\), for example, \\(H\\): Not more than 5% of GM trucks breakdown in under 10,000 miles \\(H\\): Heights of North American adult males is distributed with \\(\\mu = 72\\) inches \\(H\\): Mean year-round temperature in Athens (OH) is \\(&gt; 62\\) \\(H\\): 10% of Ohio teachers have the highest rating (Accomplished) \\(H\\): Mean county unemployment rate in an Appalachian county is 12.1% \\(H\\): The Dropout Recovery program has reduced average dropout rates to be no higher than 30% Hypotheses come in pairs, one the null hypothesis and the other the alternative hypothesis. The null hypothesis, often denoted as \\(H_{0}\\), is the assumption believed to be true while the alternative hypothesis, often denoted as \\(H_{a}\\) or \\(H_{1}\\) is the statement believed to be true if \\(H_{0}\\) is rejected by the statistical test. Some expressions follow, all in the context of average heights of adult males in North America \\(H_{0}\\): \\(\\mu &gt; 72\\) inches; \\(H_{1}\\): \\(\\mu \\leq 72\\) \\(H_{0}\\): \\(\\mu &lt; 72\\) inches; \\(H_{1}\\): \\(\\mu \\geq 72\\) \\(H_{0}\\): \\(\\mu \\leq 72\\) inches; \\(H_{1}\\): \\(\\mu &gt; 72\\) \\(H_{0}\\): \\(\\mu \\geq 72\\) inches; \\(H_{1}\\): \\(\\mu &lt; 72\\) \\(H_{0}\\): \\(\\mu = 72\\) inches; \\(H_{1}\\): \\(\\mu \\neq 72\\) \\(H_{0}\\) and \\(H_{1}\\) are mutually exclusive and exhaustive, mutually exclusive in the sense that either \\(H_{0}\\) or \\(H_{1}\\) can be true at a given point in time (both cannot be true at the same point in time), and exhaustive in that \\(H_{0} + H_{1}\\) exhaust the sample space (there is no third or fourth possibility unknown to us). When we setup our null and alternative hypotheses we must pay careful attention to how we word each hypothesis. In particular, we usually want the alternative hypothesis to be the one we hope is supported by the data and the statistical test while the null hypothesis is the one to be rejected. Why? Because rejecting the null implies the alternative must be true. As a rule, then, always setup the alternative hypothesis first and then setup the null hypothesis. For example, say I suspect that dropout rates have dropped below 30% and I hope to find this suspicion supported by the test. I would then setup the hypotheses as follows: \\[\\begin{array}{l} H_0: \\text{ mean dropout rate } \\geq 30\\% \\\\ H_1: \\text{ mean dropout rate } &lt; 30\\% \\end{array}\\] If I don’t know whether dropout rates have decreased or increased, I just suspect they are not what they were last year (which may have been, say, 50%), then I would set these up as follows: \\[\\begin{array}{l} H_0: \\text{ mean dropout rate } = 50\\% \\\\ H_1: \\text{ mean dropout rate } \\neq 50\\% \\end{array}\\] Notice the difference between the two pairs of statements. On the first pass I had a specific suspicion, that rates had fallen below 30% and so I setup the hypotheses with this specificity in mind. On the second go around I had less information to work with, just a feeling that they weren’t 50% and hence I setup less specific hypotheses. This leads us to a very important point you should always bear in mind: Hypotheses testing is only as good as the underlying substantive knowledge used to develop the hypotheses. If you know nothing about the subject you are researching, your hypothesis development will be blind to reality and the resulting \\(H_0\\) and \\(H_1\\) almost guaranteed to be incorrectly setup. There is thus no substitute to reading as widely as possible before you embark on sampling and hypothesis testing. 8.2 Type I and Type II Errors Hypothesis testing is fraught with the danger of drawing the wrong conclusion, and this is true in all research settings. This means that whenever you reject a null hypothesis, there is always some non-zero probability, however small it may be, that in fact you should not have rejected the null hypothesis. Similarly, every time you fail to reject the null hypothesis there is always some non-zero probability that in fact you should have rejected the null hypothesis. Why does this happen? It happens because we do not see the population and are forced to work with the sample we drew, and samples can mislead us by sheer chance. Let us formalize this tricky situation. Assume there are two states of the world, one where the null hypothesis is true (but we don’t know this) and the other where the null hypothesis is false (but we don’t know this). TABLE 8.1: Type I and Type II Errors Decision based on Sample Null is true Null is false Reject the Null Type I error No error Do not reject the Null No error Type II error The correct decisions are shown in the green cells while the incorrect decisions are shown in the red cells. The two errors that could happen are labeled Type I and Type II errors, respectively. A Type I error is a false positive and occurs if you reject the null hypothesis when you shouldn’t have, and a Type II error is a false negative and occurs if you fail to reject the null hypothesis when you should have. When we carry out a hypothesis test we usually default to minimizing the probability of a Type I error but in some cases the probability of a Type II error may be the one we wish to minimize. Both cannot be minimized at the same time. Why these errors are omnipresent in every hypothesis test and why both cannot be minimized at the same time will become clear once we do a few hypothesis tests. The probability of a Type I error given that the null hypothesis is true is labeled \\(\\alpha\\) and the probability of a Type II error given that the null hypothesis is false is labeled \\(\\beta\\). The probability of correctly rejecting a false null hypothesis equals \\(1- \\beta\\) and is called the power of a test. Estimating the power of a test is a fairly complicated task and taken up in some detail in a later Chapter. We also refer to \\(\\alpha\\) as the level of significance of a test, and conventionally two values are used for \\(\\alpha\\), \\(\\alpha = 0.05\\) and \\(\\alpha = 0.01\\). With \\(\\alpha = 0.05\\) we are saying we are comfortable with a false positive, with rejecting the null hypothesis when we shouldn’t have, in at most five out of every 100 tests of the same hypotheses with similarly sized random samples. With \\(\\alpha = 0.01\\) we are saying we are comfortable with a false positive, with rejecting the null hypothesis when we shouldn’t have, in at most one out of every 100 tests of the same hypotheses with similarly sized random samples. If you want a lower false positive rate then go with \\(\\alpha = 0.01\\). There is a trade-off, however: If you choose a lower false positive rate \\((\\alpha = 0.01)\\) then it will generally be harder to reject the null hypothesis than it would have been with a higher false positive rate \\((\\alpha = 0.05)\\). 8.3 The Process of Hypothesis Testing: An Example Assume we want to know whether the roundabout on SR682 in Athens, Ohio, has had an impact on traffic accidents in Athens. We have historical data on the number of accidents in years past. Say the average per day used to be 6 (i.e., \\(\\mu = 6\\)). To see if the roundabout has had an impact, we could gather accident data for a random sample of 100 days \\((n=100)\\) from the period after the roundabout was built. Before we do that though, we will need to specify our hypotheses. What do we think might be the impact? Let us say the City Engineer argues that the roundabout should have decreased accidents. If he is correct then the sample mean (\\(\\bar{x}\\)) should be less than the population mean (\\(\\mu\\)) i.e., \\(\\bar{x} &lt; \\mu\\). If he is wrong then the sample mean (\\(\\bar{x}\\)) should be at least as much as the population mean (\\(\\mu\\)) i.e., \\(\\bar{x} \\geq \\mu\\). We know from the theory of sampling distributions that the distribution of sample means, for all samples of size \\(n\\), will be normally distributed. Most sample means would be in the middle of the distribution but, of course, we know that by sheer chance we could end up with a sample mean from the tails (the upper and lower extremes of the distribution). This will happen with a very small probability but it could happen, so bear this possibility in mind. The sampling distribution is shown below: FIGURE 8.1: Sampling Distribution of Mean Traffic Accidents If we believe the City Engineer, we would setup the hypotheses as \\(H_0\\): The roundabout does not reduce accidents, i.e., \\(\\mu \\geq \\mu_0\\), and \\(H_1\\): The roundabout does reduce accidents, i.e., \\(\\mu &lt; \\mu_0\\). We then calculate the sample mean \\((\\bar{x})\\) and the sample standard deviation \\((s)\\), then the standard error of the sample mean: \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\), and finally the \\(t_{calculated} = \\dfrac{\\bar{x} - \\mu}{s_{\\bar{x}}}\\). The next step would involve using \\(df=n-1\\) and finding the area to the left of \\(t_{calculated})\\). If this area were very small then we could conclude that the roundabout must have worked to reduce accidents. How should we define ``very small’’? By setting \\(\\alpha\\) either to 0.05 or to 0.01. Once we set \\(\\alpha\\) we would Reject \\(H_0\\) if \\(P(t_{calculated}) \\leq \\alpha\\) because this would imply that the probability of getting a \\(t\\) value such as the one we calculated so far from the middle of the distribution, by sheer chance, is very small. Thus our findings must be reliable, the data must be providing sufficient evidence to conclude that the roundabout has reduced accidents. If, however, \\(P(t_{calculated}) &gt; \\alpha\\) then we would Fail to reject \\(H_0\\); the data would be providing us with insufficient evidence to conclude that the roundabout has reduced accidents. FIGURE 8.2: Rejecting the Null FIGURE 8.3: Failing to Reject the Null In the plots above, the green region in the figure shows where we need our calculated \\(t\\) to fall before we can reject the null hypothesis, and the grey region shows the area where we need our calculated \\(t\\) to fall if we are not rejecting the null hypothesis. The green region is often referred to as the critical region or the rejection region. What if the City Engineer had no clue whether accidents might have decreased or increased? Well, in that case we would we would setup the hypotheses as \\(H_0\\): The roundabout has no impact on accidents, i.e., \\(\\mu = \\mu_0\\), and \\(H_1\\): The roundabout has an impact on accidents, i.e., \\(\\mu \\neq \\mu_0\\). Next steps would be to calculate the sample mean \\((\\bar{x})\\) and the sample standard deviation \\((s)\\), followed by the standard error of the sample mean: \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\), then \\(t_{calculated} = \\dfrac{\\bar{x} - \\mu}{s_{\\bar{x}}}\\). Using \\(df=n-1\\), we would then find the area to the left/right of \\(\\pm t_{calculated})\\). If this area were very small we would conclude that the roundabout must have worked to reduce accidents. Once again, how should we define “very small”? By setting \\(\\alpha\\) either to 0.05 or to 0.01. We could then Reject \\(H_0\\) if \\(P(\\pm t_{calculated}) \\leq \\alpha\\); the data would be providing us with sufficient evidence to conclude that the roundabout had an impact on accidents. If \\(P(\\pm t_{calculated}) &gt; \\alpha\\) then we would Fail to reject \\(H_0\\); the data would be providing us with insufficient evidence to conclude that the roundabout had an impact on accidents. Watch the plots below to see the critical region in this particular scenario. FIGURE 8.4: Rejecting the Null FIGURE 8.5: Failing to Reject the Null Note the difference between the first time we tested the City Engineer’s suspicions and the second time we did so. In the first instance he/she had a very specific hypothesis, that accidents would have decreased. This led us to specify \\(H_1: \\mu &lt; \\mu_{0}\\). Given the inequality \\(&lt;\\) we focused only on the left side of the distribution to decide whether to reject the null hypothesis or not. In the second instance he/she had a vague hypothesis, that accidents will occur at a different rate post-construction of the roundabout. This led us to specify \\(H_1: \\mu \\neq \\mu_{0}\\). Given the possibility that accidents could have increased/decreased, we now focused on both sides of the distribution (left and right) to decide whether to reject the null hypothesis or not. Hypothesis tests of the first kind are what we call one-tailed tests and those of the second kind are what we call two-tailed tests. Once we do a few hypothesis tests I will explain how two-tailed tests require more evidence from your data in order to reject the null hypothesis; one-tailed tests make it relatively easier to reject the null hypothesis. For now, here is recipe for specifying hypotheses … State the hypotheses If we want to test whether something has “changed” then \\(H_0\\) must specify that nothing has changed \\(\\ldots H_0:\\mu = \\mu_{0}; H_1: \\mu \\neq \\mu_{0} \\ldots\\) two-tailed If we want to test whether something is “different” then \\(H_0\\) must specify that nothing is different \\(\\ldots H_0:\\mu = \\mu_{0}; H_1: \\mu \\neq \\mu_{0}\\ldots\\) two-tailed If we want to test whether something had an “impact” then \\(H_0\\) must specify that it had no impact \\(\\ldots H_0:\\mu = \\mu_{0}; H_1: \\mu \\neq \\mu_{0}\\ldots\\) {two-tailed} If we want to test whether something has “increased” then \\(H_0\\) must specify that it has not increased \\(\\ldots H_0:\\mu \\leq \\mu_{0}; H_1: \\mu &gt; \\mu_{0}\\ldots\\) {one-tailed} If we want to test whether something has “decreased” then \\(H_0\\) must specify that it has not decreased \\(\\ldots H_0:\\mu \\geq \\mu_{0}; H_1: \\mu &lt; \\mu_{0}\\ldots\\) {one-tailed} Collect the sample and set \\(\\alpha=0.05\\) or \\(\\alpha=0.01\\) Calculate \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}}\\), \\(\\bar{x}\\), \\(df=n-1\\) Calculate the \\(t\\) Reject \\(H_0\\) if calculated \\(t\\) falls in the critical region; do not reject \\(H_0\\) otherwise 8.3.1 Example 1 Last year, Normal (IL) the city’s motor pool maintained the city’s fleet of vehicles at an average cost of \\(\\$346\\) per car. This year Jack’s Crash Shop is doing the maintenance. The city engineer notices that in a random sample of 36 cars repaired by Jack, the mean repair cost is \\(\\$330\\) with a standard deviation of \\(\\$120\\). Is going with Jack saving the city money? Here are the hypotheses: \\(H_0: \\mu \\geq 346\\) and \\(H_1: \\mu &lt; 346\\) Let us choose \\(\\alpha = 0.05\\) and calculate the degrees of freedom: \\(df=n-1=36-1=35\\). We need the standard error so let us calculate that: \\(s_{\\bar{x}} = \\dfrac{s}{\\sqrt{n}} = \\dfrac{120}{\\sqrt{36}} = 20\\). The calculated \\(t\\) will be \\(t = \\dfrac{\\bar{x} - \\mu_{0}}{s_{\\bar{x}}} = \\dfrac{330-346}{20} = \\dfrac{-16}{20} = -0.80\\) FIGURE 8.6: Example 1 The green area shows the critical region, while the gray area shows the rest of the distribution. The red dot shows where our calculated \\(t = -0.80\\) falls. Since it does not fall in the critical region we fail to reject \\(H_0\\). The data provide insufficient evidence to conclude that going with Jack’s shop is saving the city money. How did we find the critical region? We found it by looking up what we call the critical t value for given degrees of freedom. Here we have \\(df = 35\\). If I use an online table for finding \\(t\\) values and areas under the distribution, like the one here, I see that the critical region starts at \\(t=-1.69\\) and extends left to the end of the distribution’s left-tail. This is the green area in the plot. So another way of deciding whether to reject the null or not is to compare the calculated \\(t\\) to the critical \\(t\\) using the following guidelines: \\(H_1: \\mu &lt; \\mu_{0}\\), reject \\(H_0\\) if \\(-t_{calculated} \\leq -t_{critical}\\) \\(H_1: \\mu &gt; \\mu_{0}\\), reject \\(H_0\\) if \\(t_{calculated} \\geq t_{critical}\\) \\(H_1: \\mu \\neq \\mu_{0}\\), reject \\(H_0\\) if \\(\\lvert{t_{calculated}}\\rvert \\geq \\lvert{t_{critical}}\\rvert\\) Using this rule, my calculated \\(t = -0.80\\) is not less than or equal to the critical \\(t = -1.69\\) and so I cannot reject the null hypothesis. I could have also just looked up my calculated \\(t\\) and then found \\(P(t \\leq -0.80)\\). If this area were smaller than \\(\\alpha\\), I could reject the null hypothesis. If this area were greater than \\(\\alpha\\) I would fail to reject the null hypothesis. Using the online table if I enter values for the calculated \\(t\\) and the degrees of freedom I see the area to be \\(0.2146\\). This is quite a bit larger than \\(\\alpha = 0.05\\) so I fail to reject the null hypothesis. This is the more common way of making decisions about hypothesis tests so let us memorize this rule: \\(P(t \\leq t_{calculated}) \\leq \\alpha\\): Reject the null hypothesis \\(P(t \\leq t_{calculated}) &gt; \\alpha\\): Do not reject the null hypothesis 8.3.2 Example 2 Kramer’s (TX) Police Chief learns that her staff clear 46.2% of all burglaries in the city. She wants to benchmark their performance and to do this she samples 10 other demographically similar cities in Texas. She finds their clearance rates to be as follows: 44.2 32.1 40.3 32.9 36.4 29.0 49.4 46.4 51.7 41.0 Is Kramer’s clearance rate significantly different from those of other similar Texas cities? \\(H_0: \\mu = 46.2\\) and \\(H_1: \\mu \\neq 46.2\\). Set \\(\\alpha = 0.05\\) and note \\(df=n-1=10-1=9\\), \\(\\bar{x}=40.34\\), and \\(s_{\\bar{x}} = 2.4279\\) \\(t = \\dfrac{ \\bar{x} - \\mu_{0} }{ s_{\\bar{x}} } = \\dfrac{40.34-46.2}{2.4279} = -2.414\\) Critical \\(t = \\lvert{2.262}\\rvert\\). Our calculated \\(t = -2.414\\) clearly surpasses critical \\(t\\) and so we can reject the null hypothesis; Kramer’s clearance rate seems to be different from that of demographically similar cities. If I looked up \\(P(t \\leq t_{calculated})\\) I would find this to be \\(0.039\\). Since this is less than \\(\\alpha = 0.05\\) I can easily reject the null hypothesis. The graphical depiction of the decision is shown below: FIGURE 8.7: Example 2 8.4 Confidence Intervals for Hypothesis Tests Confidence intervals can and should be used for hypothesis tests as well because they are more informative than simply providing the sample mean and our decision about rejecting/not rejecting the null hypothesis. They are used as follows: Calculate the appropriate confidence interval for the \\(\\alpha\\) used for the hypothesis test. This interval will be calculated for the sample mean (or proportion). See if the confidence interval traps the null value used in the hypotheses. If it does, then the null should not be rejected. If it does not, then the null should be rejected. Let us take the Kramer (TX) example for demonstration purposes. Our sample mean was 40.34 and the population mean was 46.2, the standard error was 2.4279, the critical \\(t\\) was 2.262, and \\(\\alpha\\) was 0.05. Consequently, the, 95% interval would be \\(40.34 \\pm 2.262(2.4279) = 40.34 \\pm 5.49191 = 34.84809 \\text{ and } 45.83191\\). Note that the value of 46.2 does not fall in this interval and hence we can reject the null hypothesis. If you calculate the appropriate interval for Jack’s Crash Shop in Example 1 you will see the two-tail critical value being \\(\\pm 2.03224451\\) and the resulting interval calculated as 289.3551 and 370.6449. In Example 1 we failed to reject the null hypothesis. Since our confidence interval includes the population value of 346 we fail to reject the null here as well. As a rule, then, if you are able to reject the null hypothesis your confidence interval will not subsume the population value used in the null hypothesis. Likewise, if you are unable to reject the null hypothesis your confidence interval will not subsume the population value used in the null hypothesis. 8.5 The One-Sample t-test The preceding hypothesis tests are what we call a one-sample t-test where we have a population mean, a single sample to work with, and are looking to test whether the sample mean differs from the population mean. These tests are built upon some assumptions and if these assumptions are violated the test-based conclusions become wholly unreliable. 8.5.1 Assumptions The assumptions that apply to the one-sample t-test are: The variable being tested is an interval-level or ratio-level measure The data represent a random sample There are no significant outliers The variable(s) being tested are approximately normally distributed The random sample assumption we generally hold to be true because if we knew we had messed up and ended up with a non-random sample, I hope we would be smart enough to avoid doing any kind of a statistical test altogether. Similarly, one would (hopefully) know that our measures were neither interval nor ratio-level and hence we should not do a t-test. The assumptions that need to be tested are thus those of normality and significant outliers. We will do these shortly. 8.6 The Two-group (or Two-sample) t-test The more interesting tests involve two groups such as, for example, comparing reading proficiency levels across male and female students, studying the wage gap between men and women, seeing if the outcome differs between those given some medicine versus those given a placebo, and so on. The logic underlying the hypothesis test is simple: If both groups come from a common population, their means should be identical Of course, the means could differ a little by chance but a large difference in the means would happen with a small probability. If this probability is less than or equal to the test \\(\\alpha\\), we reject the null hypothesis.If this probability is greater than the test \\(\\alpha\\) we do not reject the null hypothesis. Let us see this logic by way of a visual representation. In the plot below you see the bell-shaped distribution that characterizes the population and two dots, the red marking the mean reading score for female students and the blue marking the mean reading score for male students. Both are identical; \\(\\mu_{female} = \\mu_{male}\\), and this would be true if there was no difference between the two groups. FIGURE 8.8: A Normal Distribution You could, of course, end up seeing the means some distance apart purely by chance even though in the population there is no difference between the groups. This is shown for four stylized situations although the possibilities of how far apart the means might be are countless. FIGURE 8.9: Common Parent Population Since chance can yield the two means some distance apart, the hypothesis test is designed to help us determine whether chance is driving the drift we see or is it really the case that the groups come from separate populations. If they do come from different populations, we would expect to see the following: FIGURE 8.10: Separate Parent Populations Note that here the sample means are in the middle of their respective distributions, exactly what you would expect to see in most of your sample. The question thus becomes whether any differences in sample means that we see should be attributed to chance or to the rejection of the belief that both groups share a common parent population. The hypotheses are setup as shown below: Two-Tailed Hypotheses: \\(H_0\\): \\(\\mu_{1} = \\mu_{2}\\); \\(H_A\\): \\(\\mu_{1} \\neq \\mu_{2}\\) and these can be rewritten as \\(H_0\\): \\(\\mu_{1} - \\mu_{2} = 0\\); \\(H_A\\): \\(\\mu_{1} - \\mu_{2} \\neq 0\\) One-Tailed Hypotheses: \\(H_0\\): \\(\\mu_{1} \\leq \\mu_{2}\\); \\(H_A\\): \\(\\mu_{1} &gt; \\mu_{2}\\), rewritten as \\(H_0\\): \\(\\mu_{1} - \\mu_{2} \\leq 0\\); \\(H_A\\): \\(\\mu_{1} - \\mu_{2} &gt; 0\\) One-Tailed Hypotheses: \\(H_0\\): \\(\\mu_{1} \\geq \\mu_{2}\\); \\(H_A\\): \\(\\mu_{1} &lt; \\mu_{2}\\), rewritten as \\(H_0\\): \\(\\mu_{1} - \\mu_{2} \\geq 0\\); \\(H_A\\): \\(\\mu_{1} - \\mu_{2} &lt; 0\\) The test static is \\(t = \\dfrac{\\left(\\bar{x}_{1} - \\bar{x}_{2} \\right) - \\left(\\mu_{1} - \\mu_{2} \\right)}{SE_{\\bar{x}_{1} - \\bar{x}_{2}}}\\), with \\(x_1\\) and \\(x_2\\) representing the sample means of group 1 and group 2, respectively. Given that we have two groups we calculate the standard errors and the degrees of freedom in a more nuanced manner. In particular, we have to decide whether, assuming the null hypothesis to be true (i.e., the groups do indeed come from two different parent populations), these populations have (i) equal variance, or (ii) unequal variances. Assuming Equal Variances With equal population variances, we calculate the standard error as: \\[s_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\dfrac{n_1 + n_2}{n_1n_2}}\\sqrt{\\dfrac{(n_1 -1)s^{2}_{x_{1}} + (n_2 -1)s^{2}_{x_2}}{\\left(n_1 + n_2\\right) -2}}\\] and the degrees of freedom are \\(df = n_1 + n_2 - 2\\). Note that \\(s^{2}_{x_1}\\) and \\(s^{2}_{x_2}\\) are the variances of group 1 and group 2, respectively, while \\(n-1\\) and \\(n_2\\) are the sample sizes of groups 1 and 2, respectively. Assuming Unequal Variances With unequal populations variances things get somewhat tricky because now we calculate the standard error as \\[s_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\dfrac{s^{2}_{x_1}}{n_1} + \\dfrac{s^{2}_{x_2}}{n_2}}\\] and the approximate degrees of freedom as \\[\\dfrac{\\left(\\dfrac{s^{2}_{x_1}}{n_1} + \\dfrac{s^{2}_{x_2}}{n_2} \\right)^2}{\\dfrac{1}{(n_1 -1)}\\left(\\dfrac{s^{2}_{x_1}}{n_1}\\right)^2 + \\dfrac{1}{(n_2 -1)}\\left(\\dfrac{s^{2}_{x_2}}{n_2}\\right)^2}\\] These degrees of freedom are derived from the Welch-Satterthwaite equation, named for its authors, and hence the two-group t-test with unequal variances assumed is also called Welch's t-test. Once again, regardless of equal or unequal variances assumed, our decision criteria do not change: Reject the null hypothesis if the p-value of the calculated \\(t\\) is \\(\\leq \\alpha\\); do not reject otherwise. Let us see a few examples, although I will not manually calculate the standard errors or degrees of freedom since these days statistical software would be used de jure. Assumptions The two-group t-test is driven by some assumptions. Random samples … usually assumed to be true otherwise there would be no point to doing any statistical test Variables are drawn from normally distributed Populations … this can be formally tested and we will see how to carry out these tests shortly. However, I wouldn’t worry too much if normality is violated. Bear this in mind. Variables have equal variances in the Population … this can be formally tested and we will see how to carry out these tests shortly Although we will be testing two of the assumptions, you may want to bear the following rules of thumb in mind since testing assumptions is not always a cut-and-dried affair. Draw larger samples if you suspect the Population(s) may be skewed Go with equal variances if both the following are met: Assumption theoretically justified, standard deviations fairly close \\(n_1 \\geq 30\\) and \\(n_2 \\geq 30\\) Go with unequal variances if both the following are met: One standard deviation is at least twice the other standard deviation \\(n_1 &lt; 30\\) or \\(n_2 &lt; 30\\) Note also that the t-test is robust in large samples and survives some degree of skewness so long as the skew is of similar degree and direction in each group. How large is “large”? Excellent question but one without a formulaic answer since disciplines define large differently and the sample sample size that someone might see as “small” would appear “large” to others. 8.6.1 Example 1 The Athens County Public Library is trying to keep its bookmobile alive since it reaches readers who otherwise may not use the library. One of the library employees decides to conduct an experiment, running advertisements in 50 areas served by the bookmobile and not running advertisements in 50 other areas also served by the bookmobile. After one month, circulation counts of books are calculated and mean circulation counts are found to be 526 books for the advertisement group with a standard deviation of 125 books and 475 books for the non-advertisement group with a standard deviation of 115 books. Is there a statistically significant difference in mean book circulation between the two groups? Since we are being asked to test for a “difference” it is a two-tailed test, with hypotheses given by: \\[\\begin{array}{l} H_0: \\text{ There is no difference in average circulation counts } (\\mu_1 = \\mu_2) \\\\ H_1: \\text{ There is a difference in average circulation counts } (\\mu_1 \\neq \\mu_2) \\end{array}\\] Since both groups have sample sizes that exceed 30 we can proceed with the assumption of equal variances and calculate the standard error and the degrees of freedom. The degrees of freedom as easy: \\(df = n_1 + n_2 - 2 = 50 + 50 - 2 = 98\\). The standard error is \\(s_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\dfrac{n_1 + n_2}{n_1n_2}}\\sqrt{\\dfrac{(n_1 -1)s^{2}_{x_{1}} + (n_2 -1)s^{2}_{x_2}}{\\left(n_1 + n_2\\right) -2}}\\) and plugging in the values we have \\[s_{\\bar{x}_1 - \\bar{x}_2} = \\sqrt{\\dfrac{50 + 50}{2500}} \\sqrt{\\dfrac{(50 -1)(125^2) + (50 -1)(115^2)}{\\left(50 + 50\\right) -2}} = (0.2)(120.1041) = 24.02082\\] The test statistic is \\[t = \\dfrac{\\left( \\bar{x}_1 - \\bar{x}_2 \\right) - \\left( \\mu_1 - \\mu_2 \\right) }{s_{\\bar{x}_1 - \\bar{x}_2}} = \\dfrac{(526 - 475) - 0}{24.02082} = \\dfrac{51}{24.02082} = 2.123158\\] Since no \\(\\alpha\\) is given let us use the conventional starting point of \\(\\alpha = 0.05\\). With \\(df=98\\) and \\(\\alpha = 0.05\\), two-tailed, the critical \\(t\\) value would be \\(\\pm 1.98446745\\). Since our calculated \\(t = 2.1231\\) exceeds the critical \\(t = 1.9844\\), we can easily reject the null hypothesis of no difference. These data suggest there is a difference in average circulation counts between the advertisement and no advertisement groups. We could have also used the the p-value approach, rejecting the null hypothesis of no difference if the p-value was \\(\\leq \\alpha\\). The p-value of our calculated \\(t\\) turns out to be 0.0363 and so we can reject the null hypothesis. Note, in passing, that had we used \\(\\alpha = 0.01\\) we would have been unable to reject the null hypothesis because \\(0.0363\\) is \\(&gt; 0.01\\). The 95% confidence interval is given by \\((\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2}(s_{\\bar{x}_1 - \\bar{x}_2}) = 51 \\pm 1.9844(24.02082) = 51 \\pm 47.66692 = 3.33308 \\text{ and } 98.66692\\). We can be about 95% confident that the true difference between the groups lies in this interval. Had we used the 99% interval for a test with \\(\\alpha = 0.01\\) the interval would be \\(51 \\pm 2.627(24.02082) = -12.10269 \\text{ and } 114.1027\\), subsuming the null hypothesis difference of \\(0\\) and leaving us unable to reject the null hypothesis. 8.6.2 Example 2 Say we have a large data-set with a variety of information about several cars, gathered in 1974. One of the questions we have been tasked with testing is whether the miles per gallon yield of manual transmission cars in 1974 was greater than that of automatic transmission cars. Assume they want us to use \\(\\alpha = 0.05\\). We have thirteen manual transmission cars and 19 automatic transmission cars, and the means and standard deviations are 24.3923 and 6.1665 for manual, and 17.1473 and 3.8339 for automatic cars, respectively. The hypotheses are: \\[\\begin{array}{l} H_0: \\text{Mean mpg of manual cars is } \\leq \\text{ the mean mpg of automatic cars} (\\bar{x}_{man} \\leq \\bar{x}_{auto}) \\\\ H_1: \\text{Mean mpg of manual cars is } &gt; \\text{ the mean mpg of automatic cars} (\\bar{x}_{man} &gt; \\bar{x}_{auto}) \\end{array}\\] The calculated \\(t\\) is 4.1061 and the p-value is 0.0001425, allowing us to reject the null hypothesis. The data suggest that average mpg of manual cars is not not \\(\\leq\\) that of automatic cars. Note a couple of things here: (i) We have a one-tailed hypothesis test, and (ii) we are assuming equal variances since both conditions are not met for assuming unequal variances. In addition, note that the confidence interval is found to be \\((3.6415, 10.8483)\\), indicating that we can be 95% confident that the average manual mpg is higher than average automatic mpg by anywhere between 3.64 mpg and 10.84 mpg. 8.6.3 Caution If, for two groups, the confidence intervals around each group mean do not overlap with that of the other group, we can safely infer the two groups are statistically significantly different. Be careful, however, to not automatically infer no statistically significant differences just because the confidence intervals of two groups overlap. Why, you ask? See here and here. This is a common mistake a lot of us slip into. 8.7 Paired t-tests Assume, for example, that we have measured some outcome for the same set of individuals at two points in time. This situation could be akin to gathering a random sample of \\(6^{th}\\) grade students, recording their mathematics scores on the statewide assessment, and then some years later recording their \\(8^{th}\\) grade mathematics scores on the statewide assessment. We might be curious to know if on average, students’ mathematics performance has improved over time, either organically or because of some special mathematics tutoring program that implemented in the school. Or we may have a situation where the question is whether hospitalization tends to depress patients and so we measure their mental health before hospitalization and then again after hospitalization. We need not have the same individuals measured twice; paired t-tests also work with two groups with different individuals in each group so long as the groups are similar. What does that mean? Well, think of it as follows. Say I am interested in the hospitalization question. I know that if I administer some mental health assessment (most likely with a battery of survey-type questions) prior to hospitalization, and then I go back with the same battery of questions post-hospitalization, the patients might catch on to the purpose of my study and tailor their responses accordingly. If they don’t respond honestly to my questions, the resulting data will be contaminated. How can I get around this hurdle? A common technique used in situations such as these, where you cannot go back to the same set of individuals, involves finding individuals who are similar to the first group such that both groups have the same average age of the patients, the same proportion of males versus females, similar distributions of income, pre-existing medical conditions, and so on. If we employ such an approach then we have what we call a matched pairs design. Assumptions The following assumptions apply to paired t-tests: Random samples … usually assumed to be true otherwise there would be no point to doing any statistical test Variables are drawn from normally distributed Populations … this can be formally tested and we will see how to carry out these tests shortly The Testing Protocol Let us see how the test is carried out with reference to a small data-set wherein we have six pre-school children’s scores on a vocabulary test before a reading program is introduced into the pre-school \\((x_1)\\) and then again after the reading program has been in place for a few months \\((x_2)\\). TABLE 8.2: Vocabulary Scores pre- and post-intervention Child ID Pre-intervention score Post-intervention score Difference = Pre - Post 1 6.0 5.4 0.6 2 5.0 5.2 -0.2 3 7.0 6.5 0.5 4 6.2 5.9 0.3 5 6.0 6.0 0.0 6 6.4 5.8 0.6 Note the column \\(d_i\\) has the difference of the scores such that for Child 1, \\(6.0 - 5.4 = 0.6\\), for Child 2, \\(5.0 - 5.2 = -0.2\\), and so on. The mean, variance and standard deviation of \\(d\\) are calculated as follows: \\[\\begin{eqnarray*} d_{i} &amp;=&amp; x_{1} - x_{2} \\\\ \\bar{d} &amp;=&amp; \\dfrac{\\sum{d_i}}{n} \\\\ s^{2}_{d} &amp;=&amp; \\dfrac{\\sum(d_i - \\bar{d})^2}{n-1} \\\\ s_d &amp;=&amp; \\sqrt{\\dfrac{\\sum(d_i - \\bar{d})^2}{n-1}} \\end{eqnarray*}\\] Note also that if the reading program is very effective, then we should see average scores being higher post-intervention, even if some students don’t necessarily improve their individual scores. Say we have no idea what to expect from the program. In that case, our hypotheses would be: \\[\\begin{array}{l} H_0: \\mu_d = 0 \\\\ H_1: \\mu_d \\neq 0 \\end{array}\\] The test statistic is given by \\(t = \\dfrac{\\bar{d} - \\mu_d}{s_d/\\sqrt{n}}; df=n-1\\) and the interval estimate calculated as \\(\\bar{d} \\pm t_{\\alpha/2}\\left(\\dfrac{s_d}{\\sqrt{n}}\\right)\\). Once we have specified our hypotheses, selected \\(\\alpha\\), and calculated the test statistic, the usual decision rules apply: Reject the null hypothesis if the calculated \\(p-value \\leq \\alpha\\); do not reject the null hypothesis if the calculated \\(p-value &gt; \\alpha\\). In this particular example, it turns out that \\(\\bar{d}=0.30\\); \\(s_d=0.335\\); \\(t = 2.1958\\), \\({\\circled{\\color{blue}{df = 5}}}\\), \\(p-value = 0.07952\\) and 95% CI: \\(0.3 \\pm 0.35 = (-0.0512, 0.6512)\\). Given the large \\(p-value\\) we fail to reject \\(H_0\\) and conclude that these data do not suggest a statistically significant impact of the reading program. 8.7.1 Example 1 Over the last decade, has poverty worsened in Ohio’s public school districts? One way to test worsening poverty would be to compare the percent of children living below the poverty line in each school district across two time points. For the sake of convenience I will use two American Community Survey (ACS) data sets that measure Children Characteristics (Table S0901), one the 2011-2015 ACS and the other the 2006-2010 ACS. While a small snippet of the data are shown below for the 35 school districts with data for both years, you can download the full dataset from here. TABLE 8.3: Percent of Children in Poverty District 2006-2010 2011-2015 Akron City School District, Ohio 35.3 41.0 Brunswick City School District, Ohio 6.8 7.6 Canton City School District, Ohio 44.1 49.6 Centerville City School District, Ohio 10.5 5.4 Cincinnati City School District, Ohio 39.5 43.0 Cleveland Municipal School District, Ohio 45.8 53.3 \\[\\begin{array}{l} H_0: \\text{ Poverty has not worsened } (d \\leq 0) \\\\ H_1: \\text{ Poverty has worsened } (d &gt; 0) \\end{array}\\] Subtracting the 2006-2010 poverty rate from the 2011-2015 poverty rate for each district and then calculating the average difference \\((d)\\) yields \\(\\bar{d} = 4.328571\\) and \\(s_{d} = 3.876746\\). With \\(n=35\\) we have a standard error \\(s_{\\bar{d}} = \\dfrac{s_d}{\\sqrt{n}} = \\dfrac{3.876746}{\\sqrt{35}} = 0.6552897\\). The test statistic is \\(t = \\dfrac{\\bar{d}}{s_{\\bar{d}}} = \\dfrac{4.328571}{0.6552897} = 6.605584\\) and has a \\(p-value = 0.0000001424\\), allowing us to easily reject the null hypothesis. These data suggest that school district poverty has indeed worsened over the intervening time period. The 95% confidence interval is \\((2.9968 \\text{ and } 5.6602)\\). 8.7.2 Example 2 A large urban school district in a Midwestern state implemented a reading intervention to boost the district’s scores on the state’s English Language Arts test. The intervention was motivated by poor performance of the district’s \\(4^{th}\\) grade cohort. Three years had passed before that cohort was tested in the \\(8^{th}\\) grade. Did the intervention boost ELA scores, on average? TABLE 8.4: English Language Arts: Scaled scores, grades Three and Eight Student ID Grade Scaled Score AA0000001 3 583 AA0000002 3 583 AA0000003 3 583 AA0000004 3 668 AA0000005 3 627 AA0000006 3 617 \\[\\begin{array}{l} H_0: \\text{ Intervention did not boost ELA scores } (d \\leq 0) \\\\ H_1: \\text{ Intervention did boost ELA scores } (d &gt; 0) \\end{array}\\] We have \\(\\bar{d} = 14.62594\\), \\(s_d = 66.27296\\), \\(df = 12955\\) and the standard error is \\(0.5822609\\). The test statistic then is \\(t = \\dfrac{\\bar{d}}{s_{\\bar{d}}} = \\dfrac{14.62594}{0.5822609} = 25.11922\\) and has a \\(p-value\\) that is very close to \\(0\\). Hence we can reject the null hypothesis; these data suggest that the reading intervention did indeed boost English Language Arts scores on average. 8.8 Chapter 8 Practice Problems Problem 1 In the last few years Ohio, like many other states, established a framework for rating teachers and principals/assistant principals. Prior to this policy shift, the existing approach to rating educators yielded very few ineffective teachers, a problem well documented by The Widget Effect. Under the new rating system, the expectation is that the percentage of ineffective teachers will be greater than in the past. Assume that the old system yielded an average of 0.5% of ineffective educators in any school district., (i.e., \\(\\mu = 0.5\\)). Given this reality, establish the appropriate null and alternative hypotheses. Assume that your team of program evaluators carries out a statistical test and rejects the null hypothesis. Which error – Type I or Type II – should you be more concerned about? Explain, in the context of the problem, what committing a Type I error would mean. Explain, in the context of the problem, what committing a Type II error would mean. Problem 2 Write the missing hypothesis for each of the hypotheses listed below, both in words and with appropriate mathematical notation: \\(H_0\\): Average amount of rainfall per year is the same today as what it was in 1977 \\(H_1\\): Adult males’ average heights have increased in recent years \\(H_0\\): College dropout rates of first generation students are either the same as or lower than those of their non-first generation peers \\(H_1\\): A human can swim faster in syrup than in water \\(H_1\\): Average waiting time (in minutes) between eruptions of The Old Faithful geyser exceed 20 minutes \\(H_0\\): Average number of words typed per minute is not affected by typing tutorials \\(H_0\\): The number of migrant deaths in Mediterranean waters have not changed since last year Problem 3 What factor can a research control to reduce the risk of committing a Type I error? What factor can a researcher control to reduce the standard error? Problem 4 A researcher carried out a one-tailed hypothesis test using \\(\\alpha = 0.01\\) and rejected \\(H_0\\). Another researcher repeated the hypothesis test but with \\(\\alpha = 0.05\\) and as a two-tailed test, and failed to reject \\(H_0\\). Can both analyses be correct? Explain your answer. Problem 5 Is crime in America rising or falling? The answer is not as simple as politicians make it out to be because of how the FBI collects crime data from the country’s more than 18,000 police agencies. National estimates can be inconsistent and out of date, as the FBI takes months or years to piece together reports from those agencies that choose to participate in the voluntary program. To try to fill this gap, The Marshall Project collected and analyzed more than 40 years of data on the four major crimes the FBI classifies as violent — homicide, rape, robbery and assault — in 68 police jurisdictions with populations of 250,000 or greater. The research team calculated the rate of crime in each category and for all violent crime, per 100,000 residents in the jurisdiction. Setup the appropriate hypotheses to test the suggestion that violent crimes per 100,000 (violent_per_100k) exceeded 800 in 2015. Using \\(\\alpha = 0.05\\), carry out the hypothesis test and state your conclusion in words. Report and interpret the corresponding confidence interval for your estimate of average violent crimes per 100,000 in 2015. Problem 6 With The Marshall Project data-set, answer the questions that follow . Setup the hypotheses to test the belief that in 2015, larger cities (defined as cities with population sizes \\(\\geq 500,000\\)) have more violent crimes per 100,000 than smaller cities (defined as cities with population sizes \\(&lt; 500,000\\)). The variable is called total_pop in the data-set. Carry out the appropriate test and state your conclusion in words. Use \\(\\alpha = 0.01\\). Report and interpret the corresponding confidence interval for your estimated difference in average violent crimes per 100,000 in small versus large cities. Problem 7 Continuing on with the Marshall Project data-set, Setup the hypotheses to test the belief that cities’ homicides per 100,000 declined between 1975 and 2015. Carry out the appropriate test and state your conclusion in words. Use \\(\\alpha = 0.05\\). Report and interpret the corresponding confidence interval for your estimated difference. Problem 8 The Evidence and Data for Gender Equality (EDGE) project is a joint initiative of the United Nations Statistics Division and UN Women that seeks to improve the integration of gender issues into the regular production of official statistics for better, evidence-based policies. EDGE aims to accelerate existing efforts to generate internationally comparable gender indicators on health, education, employment, entrepreneurship and asset ownership in three many ways, one of which is to develop an online interactive platform to disseminate gender-relevant data on education, employment, and health. We will focus on the maternal mortality ratio (modeled estimate, per 100,000 live births), available for the 1990 - 2015 period by country and region. You can access this indicator from this file. Two questions are of interest here: a belief that in 2015 the average maternal mortality ratio was \\(&lt; 160\\), and an interest in seeing if on average maternal mortality ratios declined between 1990 and 2015. Setup the appropriate hypothesis for questions (1) and (2), carry out the test (choose an appropriate value for \\(\\alpha\\)), state your conclusion in words, and both report and interpret the corresponding confidence interval estimates. Problem 9 On September \\(25^{th}\\) 2015, countries adopted a set of goals to end poverty, protect the planet, and ensure prosperity for all as part of a new sustainable development agenda. Each goal has specific targets to be achieved over the next 15 years. The World Bank has a wonderful database that provides country-level information on a variety of indicators that feature in The Atlas of Sustainable Development Goals. Focus on a specific indicator – Access to a mobile phone or internet at home” for males and females aged 15 years old or older, respectively, in 2014. A casual observer suspects that gender discrimination leads to males having differential access a mobile phone or internet at home. Test this suspicion with the data-set available here. State your conclusions, including the interpretation of the confidence intervals for your estimated difference. Problem 10 The Ohio Department of Education gathers and releases a wealth of data on public schools and community schools; see, for instance, here and here. A snippet of these data are provided for you in this file. Note that you have data for community schools and public school districts; private and parochial schools are excluded. Thirty-two of our 88 counties are Appalachian, as flagged by the variable Appalachian in the data-set. Focusing on third-grade reading proficiency rates (X3rd.Grade.Reading), test whether the proportion of third-grade students rated proficient or better in reading differs between Appalachian and non-Appalachian districts. Report and interpret your conclusion and the confidence interval for the estimated difference. Problem 11 The Lalonde data-set with 722 observations on the following 12 variables age = age in years. educ = years of schooling. black = indicator variable for blacks (1 = Black). hisp = indicator variable for Hispanics (1 = Hispanic). married = indicator variable for martial status (1 = Married). nodegr = indicator variable for high school diploma (1 = no high school graduate or equivalent). re75 = real earnings in 1975. re78 = real earnings in 1978. treat = an indicator variable for treatment status (1 = treatment group). “Key to this data-set is the treatment, which refers to the National Supported Work Demonstration (NSW), a temporary employment program designed to help disadvantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environment. Unlike other federally sponsored employment and training programs, the NSW program assigned qualified applicants to training positions randomly. Those assigned to the treatment group received all the benefits of the NSW program, while those assigned to the control group were left to fend for themselves.” Lalonde (1986). Note that real earnings in 1975 are before the NSW program began and real earnings in 1978 are the outcome of interest because they were recorded after the NSW treatment had occurred. Using these data answer the following questions: Did the NSW program have an impact on the real earnings of participants? Did the NSW program increase the real earnings of participants? Focusing only on participants in the NSW program, Did the real earnings in 1978 differ by marital status? What about between participants with/without a high school degree? Problem 12 “The Student/Teacher Achievement Ratio (STAR) was a four-year longitudinal class-size study funded by the Tennessee General Assembly and conducted by the State Department of Education. Over 7,000 students in 79 schools were randomly assigned into one of three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade. The Project STAR public access data set contains data on test scores, treatment groups, and student and teacher characteristics for the four years of the experiment, from academic year 1985–1986 to academic year 1988–1989. The test score data analyzed in this chapter are the sum of the scores on the math and reading portion of the Stanford Achievement Test.” Source These data can be downloaded from here. Note the key variables as defined below: gender = student’s gender 1/male, 2/female ethnicity = student’s ethnicity 1/caucasian, 2/african-american, 3/asian, 4/hispanic, 5/american-indian, 6/other stark, star1, star2, star3 = whether the student was in a 2/small classroom, 1/a regular classroom, or 3/a regular classroom with a teaching aide readk, read1, read2, read3 = reading test scores in kindergarten/grade 1/grade 2/grade 3 mathk, math1, math2, math3 = mathematics test scores in kindergarten/grade 1/grade 2/grade 3 lunchk, lunch1, lunch2, lunch3 = student eligible for free/reduced lunch in kindergarten/grade 1/grade 2/grade 3 with 1/non-free, 2/free schoolk, school1, school2, school3 = school type student attended in kindergarten/grade 1/grade 2/grade 3 with 1/inner-city, 2/suburban, 3/rural, 4/urban degreek, degree1, degree2, degree3 = teacher’s highest degree earned in kindergarten/grade 1/grade 2/grade 3 with 1/bachelor, 2/master, 3/specialist, 4/phd ladderk, ladder1, ladder2, ladder3 = teacher’s career ladder status in kindergarten/grade 1/grade 2/grade 3 with 1/level 1, 2/level 2, 3/level 3, 4/apprentice, 5/probation, 6/pending experiencek, experience1, experience2, experience3 = teacher’s years of teaching experience in kindergarten/grade 1/grade 2/grade 3 tethnicityk, tethnicity1, tethnicity2, tethnicity3 = teacher’s ethnicity in kindergarten/grade 1/grade 2/grade 3 with 1/caucasian, 2/african-american Comparing students who were in a small classroom in all four grades to students in a regular classroom in all four grades, Did reading scores improve between grades k and 3? What about mathematics scores? Comparing students who were in a small classroom in all four grades to students in a regular classroom (with our without an aide) in all four grades, Was there a difference in reading scores in the fourth grade? What about mathematics scores? Problem 13 The data for this analysis come from the University of Michigan Panel Study of Income Dynamics (hereafter PSID) for the year 1975 (interview year 1976). Although this year was atypical of most of the 1970’s, only in 1976 did PSID directly interview the wives in the households. During all other years the head of the household’s interview supplied information about the wife’s labor market experiences during the previous year. One suspects that the own reporting is more accurate, and it is for this reason that many recent studies of married women’s labor supply have used these data. This sample consists of 753 married white women between the ages of 30 and 60 in 1975, with 428 working at some time during the year. This sample size is smaller than most used in the studies reported in Table I. The dependent variable is hours = the wife’s annual hours of work in 1975, and measured as the product of the number of weeks the wife worked for money in 1975 and the average number of hours of work per week during the weeks she worked. The measure of the wage rate is the average hourly earnings, defined by dividing the total labor income of the wife in 1975 by the above measure of her hours of work. inlf = labor force participation in 1975 = 1 hours = wife’s hours of work in 1975 kidslt6 = number of children less than 6 years old in household kidsge6 = number of children between ages 6 and 18 in household age = wife’s age educ = wife’s educational attainment, in years wage = wife’s average hourly earnings, in 1975 dollars repwage = wife’s wage reported at the time of the 1976 interview (not= 1975 estimated wage) hushrs = husband’s hours worked in 1975 husage = husband’s age huseduc = husband’s educational attainment, in years huswage = husband’s wage, in 1975 dollars faminc = family income, in 1975 dollars mtr = Federal marginal tax rate facing women mothereduc = wife’s mother’s educational attainment, in years fathereduc = wife’s father’s educational attainment, in years unemprate = unemployment rate in county of residence, in percentage points city = lives in large city (SMSA) = 1 exper = actual years of wife’s previous labor market experience nwifeinc = \\(faminc - (wage \\times hours)\\) in thousand dollars lwage = log(wage) expersq = \\(exper^2\\) Did the wife’s annual hours of work differ according to whether the family lived in a city or not? Create a new variable \\(= 1\\) if the family had any children under 6 years of age and \\(= 0\\) otherwise. Then test whether the wife’s annual hours of work were greater for families with no children in this age bracket versus at least one child in this age bracket. Problem 14 The following data-set contains information on 2.725 individuals’ criminal arrest histories. The variables include narr86 = number of times arrested, 1986 nfarr86 = number of felony arrests, 1986 nparr86 = number property crime arrests, 1986 pcnv = proportion of prior convictions avgsen = average sentence length, months tottime = time in prison since 18 (months) ptime86 = months in prison during 1986 qemp86 = number of quarters employed, 1986 inc86 = legal income, 1986, in hundreds of dollars durat = recent unemployment duration black = 1 if Black hispan = 1 if Hispanic pcnvsq = pcnv\\(^2\\) pt86sq = ptime86\\(^2\\) inc86sq = inc86\\(^2\\) Was the average sentence length greater for Black versus non-Black individuals? What about for Hispanic versus non-Hispanic individuals? It would be remiss of us not to recognize the existence of a very passionate and vigorous Null Hypothesis Significance Testing (NHST) controversy. While the details of the controversy are too advanced for our present purposes, here are some articles you should read: The Truth Wears Off: Is there something wrong with the scientific method? and Do We Really Need the S-word?↩︎ "],["chisq.html", "Chapter 9 Working with Multinomial Data 9.1 A Single Multinomial Variable (Goodness-of-fit test) 9.2 The \\(\\chi^2\\) Test of Independence/Association 9.3 Fisher’s Exact Test 9.4 A Cautionary Tale 9.5 Chapter 9 Practice Problems", " Chapter 9 Working with Multinomial Data Very often the variables of interest will be multinomial – variables that have more than two categories. For example, you may be working with Gallup Poll data where the responses to a question asking how an individual feels about the statement “How much of the time do you think you can trust government in Washington to do what is right – just about always, most of the time, or only some of the time?”. Here we have three discrete response categories. Maybe you work for Mars Inc. and have been tasked to spruce up quality control because some intrepid researcher has found the color distribution of M&amp;Ms to be way off what the company would like it to be. Now you have a six category response – blue, orange, green, yellow, red and brown. You go to the Tennessee plant and draw a sample of M&amp;Ms coming off the production line and count the number of candies of each color. How can you analyze your sample data to figure out if indeed the color distribution is way off? You can do it via a hypothesis test involving the Chi-square \\((\\chi^2)\\) distribution. Let us start with a stylized example. 9.1 A Single Multinomial Variable (Goodness-of-fit test) Say the research group I am working with is developing four public service announcements (PSAs) that warn listeners about the warning signs of a developing opioid addiction. These announcements are aired daily on major television networks during prime time. After two weeks of air time a telephone survey is used to gauge which of the four PSAs was recalled in the greatest detail by a listener. Of the 300 respondents, here is the breakdown of recall: PSA a = 85; PSA b = 95; PSA c = 50, and; PSA d = 70. Is this what we expected to see? We had no clue going in as to which one would be more effective than another and so the best we can do is admit that we started airing the PSAs assuming each was equally effective. If they were equally effective, then recall should be evenly distributed as well, i.e., exactly one-fourth of those surveyed should have recalled PSA a, another fourth b, another fourth c, and the last fourth d. That is, we expected to see the following distribution: PSA a = 75; PSA b= 75; PSA c = 75; PSA d = 75. This situation naturally leads to specific hypotheses: \\[\\begin{array}{l} H_0: \\text{PSAs are equally effective, (i.e., } P_a = P_b = P_c = P_d = 25\\%) \\\\ H_1: \\text{PSAs are NOT equally effective, (i.e., } P_a \\neq P_b \\neq P_c \\neq P_d = 25\\%) \\end{array}\\] Let us organize these two sets of numbers, the observed frequencies \\((f_i)\\) and the expected frequencies \\((e_i)\\), into a table. TABLE 9.1: Expected and Observed Recall of the four PSAs Observed Expected a 85 75 b 95 75 c 50 75 d 70 75 Total 300 300 We clearly see a drift in each row of the table, with the largest drift between observed and expected frequencies occurring for PSA c, then PSA b, then PSA a, and the least for PSA d. Certainly, some of this drift could be due to chance. How can we test whether the drift is due to chance or meaningful enough to claim a difference in the effectiveness of the four PSAs? Via a hypothesis test of course. The test involves each cell of the table wherein we calculate the following quantity: \\(\\dfrac{(observed - expected)^2}{expected}\\) We then add the resulting value for each cell of the table, ending up with the \\(\\text{calculated } \\chi^2_{df}\\). This sum can be compared to the critical value from the \\(\\chi^2\\) distribution with degrees of freedom given by \\(df = \\text{No. of categories } - 1\\) or we can simply calculate the \\(p-value\\) for our \\(\\text{calculated } \\chi^2_{df}\\). If the \\(p-value\\) is \\(\\leq \\alpha\\), we reject the null hypothesis. The table below shows you the calculations. Note that we have four columns and hence the degrees of freedom are \\(df=4-1=3\\). TABLE 9.2: Calculating the Chi-square Observed Expected Obs. - Exp. (Obs. - Exp) Squared (Obs. - Exp) Squared / Exp. a 85 75 10 100 1.3333333 b 95 75 20 400 5.3333333 c 50 75 -25 625 8.3333333 d 70 75 -5 25 0.3333333 Total 300 300 0 1150 15.3333333 There is a unique \\(\\chi^2\\) distribution for each degree of freedom, as shown below. The horizontal “dashed” line shows you where the \\(p-value = 0.05\\) lies on the \\(y-axis\\) while the \\(x-axis\\) shows you the \\(\\chi^2\\) values. For a given degrees of freedom, we look at where the calculated \\(\\chi^2\\) lies and if it is at or below the horizontal line, then we know the calculated \\(\\chi^2\\) has a \\(p-value \\leq 0.05\\). FIGURE 9.1: The Chi-Square distribution for specific degrees of freedom In our case, the \\(\\chi^2_{df=3} = 15.33\\) is marked by the red dot and is clearly located well below the \\(p-value = 0.05\\) line. In fact, the \\(\\chi^2_{df=3}\\) has a \\(p-value = 0.001555293\\). Consequently, we can reject the null hypothesis; the four PSAs are not recalled with equal frequency. FIGURE 9.2: Chi-Square = 15.33, df = 3 Note: There is an online p-value calculator. You can also conduct the test online. 9.1.0.1 Example 1 In the preceding example we had no a priori information as to what distribution to expect and hence we set each PSA’s recall to 25%. At times we may have the necessary information as, for instance, in the M&amp;Ms example that follows. Specifically, we know that Mars Inc. claims the following distribution for M&amp;Ms’ colors: Blue = 24%; Orange = 20%; Green = 16%; Yellow = 14%; Red = 13%, and; Brown = 13%. Now, say you have a sample of 200 M&amp;Ms that you randomly collect from the production line. How should you expect the six colors to be distributed if Mars Inc. has perfect quality control? Well, you should expect to see 24% Blue, which would be \\(0.24 \\times 200 = 48\\), 20% Orange, which would be \\(0.20 \\times 200 = 40\\), and in a similar vein, \\(0.16 \\times 200 = 32\\) Green, \\(0.14 \\times 200 = 28\\) Yellow, \\(0.13 \\times 200 = 26\\) Red, and \\(26\\) Brown. But the observed distribution of colors turns out to be somewhat different than expected (see the table). TABLE 9.3: Calculating the Chi-square Observed Expected Blue 42 48 Orange 38 40 Green 32 32 Yellow 28 28 Red 28 26 Brown 32 26 Total 200 200 Is the drift between the observed and the expected frequencies different enough to reject the notion that Mars Inc.’s quality control is flawless? Let us see. \\[\\begin{array}{l} H_0: \\text{Colors are distributed as claimed by Mars Inc.} \\\\ H_1: \\text{Colors are NOT distributed as claimed by Mars Inc.} \\end{array}\\] Let us now setup the table of calculations: TABLE 9.4: Calculating the Chi-square Observed Expected Obs. - Exp. (Obs. - Exp) Squared (Obs. - Exp) Squared / Exp. Blue 42 48 -6 36 0.7500000 Orange 38 40 -2 4 0.1000000 Green 32 32 0 0 0.0000000 Yellow 28 28 0 0 0.0000000 Red 28 26 2 4 0.1538462 Brown 32 26 6 36 1.3846154 Total 200 200 0 80 2.3884615 We have six categories so \\(df = 6 - 1 = 5\\). The calculated \\(\\chi^2_{df=5} = 2.388461\\), and has a \\(p-value = 0.7931913\\). Since this is greater than \\(\\alpha = 0.05\\) we fail to reject the null hypothesis; This sample suggests that M&amp;M colors are likely distributed as claimed by Mars Inc. The graph shows you where the \\(p-value\\) falls for our calculated \\(\\chi^2\\). FIGURE 9.3: Chi-Square = 2.388461, df = 5 In this example, we were unable to reject the null hypothesis because the difference between the observed and the expected frequencies was too small and these small differences yielded a small \\(\\chi^2\\) value. One simple takeaway, therefore, should be that if we see large differences before we start the test we should anticipate that the null hypothesis will not be rejected. 9.1.0.2 Example 2 During March 5-7 in 2001, Gallup asked a random sample of U.S. Adults living in the 50 states and Washington DC the following question: “Next, I’m going to read a list of problems facing the country. For each one, please tell me if you personally worry about this problem a great deal, a fair amount, only a little, or not at all? First, how much do you personally worry Race Relations?” The responses were: Great deal \\(=28\\%\\), Fair amount \\(=34\\%\\), Only a little \\(=23\\%\\), and Not at all \\(15\\%\\). When Gallup asked the question again this year (during March 1-5, 2017), the responses were as follows: Great deal \\(=428\\), Fair amount \\(= 275\\), Only a little \\(= 173\\), and Not at all \\(=122\\). Have American adults’ concerns over race relations changed since 2001? \\[\\begin{array}{l} H_0: \\text{ American adults&#39; concerns over race relations have not changed since 2001} \\\\ H_1: \\text{ American adults&#39; concerns over race relations HAVE changed since 2001} \\end{array}\\] The sample size is 998 so we first calculate expected frequencies based on the distribution of responses in 2001. The observed frequencies are given so the rest of the calculations can proceed as usual. See the table below: TABLE 9.5: Calculating the Chi-square Observed Expected Obs. - Exp. (Obs. - Exp) Squared (Obs. - Exp) Squared / Exp. Great Deal 428 279.44 148.56 22070.074 78.979651 Fair Amount 275 339.32 -64.32 4137.062 12.192215 Only a Little 173 229.54 -56.54 3196.772 13.926861 Not at All 122 149.70 -27.70 767.290 5.125518 Total 998 998.00 0.00 30171.198 110.224244 The resulting \\(\\chi^2_{df=3} = 110.2242\\), and has a \\(p-value\\) that is practically zero. We can thus reject the null hypothesis; these data suggest that American adults’ concerns over race relations HAVE changed since 2001. 9.1.1 Assumptions The \\(\\chi^2\\) test is also premised upon some assumptions. Namely, No category has expected frequencies \\((e_i)\\) less than \\(1\\) No more than 20% of the categories should have expected frequencies \\(&lt; 5\\) If these assumptions are violated and we nevertheless proceed with the test, the resulting conclusion will be unreliable if not completely invalid. With either assumption going unmet, we may have a working solution though, but this depends upon what the categories represent. For example, say the categories refer to a survey question where the response options were “Strongly Disagree”, “Disagree Somewhat”, “Neither Agree nor Disagree”, “Agree Somewhat”, and “Strongly Agree”. If the violation occurs for Strongly Agree or Agree Somewhat, or then for Strongly Disagree or Disagree Somewhat, we could collapse the offending categories, as shown below. TABLE 9.6: Collapsing Response Categories Observed Strongly Disagree 4 Disagree Somewhat 3 Neither Agree nor Disagree 10 Agree Somewhat 11 Strongly Agree 16 Observed Disagree 7 Neither Agree nor Disagree 10 Agree 27 9.2 The \\(\\chi^2\\) Test of Independence/Association The \\(\\chi^2\\) test can also be used to test for an association (aka a relationship) between two categorical variables. For example, Dunkin may have retained you to advise them on how to pitch different coffees to men and women. You monitor sales at a store and walk away with the following data: TABLE 9.7: Coffee Preferences by Sex Light Regular Dark Total Male 20 40 20 80 Female 30 30 10 70 Total 50 70 30 150 Maybe you are curious to know if coffee preferences are associated with an individual’s sex. The null hypothesis would be the one you would look to reject and so you might state the hypotheses as follows: \\[\\begin{array}{l} H_0: \\text{Coffee preferences are not associated with sex} \\\\ H_1: \\text{Coffee preferences ARE associated with sex} \\end{array}\\] If the null hypothesis is indeed true, how many men should you have expected to say they prefer Light coffee? Given probability theory, if we think of this question as drawing an individual at random and asking ourselves what is the probability that the individual is Male (event A) and prefer Light coffee (event B)? Well, if two events A and B are independent, the probability of A and B both occurring can be calculated as \\(P(A \\text{ and } B) = P(A) \\times P(B)\\). What is the probability of drawing a Male? \\(P(A) = \\dfrac{80}{150} = 0.5333333\\) since we have a total of 150 individuals of whom 80 are Male. What is the probability of drawing an individual who prefers Light coffee? \\(P(B) = \\dfrac{50}{150} = 0.3333333\\). So \\(P(\\text{Male and Light}) = P(A) \\times P(B) = \\dfrac{80}{150} \\times \\dfrac{50}{150} = 0.1777778\\). We can flip this probability into a number: How many individuals in the sample should be Male and prefer Light coffee if the two are not related? That should be \\(0.1777778 \\times 150 = 26.66667\\). Similar calculations can be done for all the other combinations of Sex and Coffee. In brief, for each cell in the contingency table, calculate \\[e_i = \\dfrac{\\text{Row}_i \\text{ Total} \\times \\text{Column}_i \\text{ Total}}{\\text{Sample Size}}\\] Male &amp; Light: \\(e_{11}=\\dfrac{(80)(50)}{150}=\\dfrac{4000}{150}=26.67\\) Male &amp; Medium: \\(e_{12}=\\dfrac{(80)(70)}{150}=\\dfrac{5600}{150}=37.33\\) Male &amp; Dark: \\(e_{13}=\\dfrac{(80)(30)}{150}=\\dfrac{2400}{150}=16.00\\) Female &amp; Light: \\(e_{21}=\\dfrac{(70)(50)}{150}=\\dfrac{3500}{150}=23.33\\) Female &amp; Medium: \\(e_{22}=\\dfrac{(70)(70)}{150}=\\dfrac{4900}{150}=32.67\\) Female &amp; Dark: \\(e_{23}=\\dfrac{(70)(30)}{150}=\\dfrac{2100}{150}=14.00\\) Voila! We have all expected frequencies and can proceed with the test. First, we calculate, for each cell in the contingency table, \\(\\dfrac{(f_{ij}-e_{ij})^{2}}{e_{ij}}\\). We then add the resulting value over all cells. This yields \\[ \\chi^{2}_{df} = \\sum_{i} \\sum_{j} \\dfrac{(f_{ij} - e_{ij})^{2}}{e_{ij}} \\] where \\(df=(r-1)(c-1)\\), with \\(r=\\) the number of rows in the table and \\(c=\\) number of columns in the table. Our example has 2 rows and 3 columns so \\(df = (r-1)(c-1) = (2-1)(3-1) = (1)(2) = 2\\) TABLE 9.8: Calculations for Coffee Preference by Sex Observed Expected Obs. - Exp. (Obs. - Exp) Squared (Obs. - Exp) Squared / Exp. Male L 20 26.67 -6.67 44.4889 1.6681252 Male R 40 37.33 2.67 7.1289 0.1909697 Male D 20 16.00 4.00 16.0000 1.0000000 Female L 30 23.33 6.67 44.4889 1.9069396 Female R 30 32.67 -2.67 7.1289 0.2182094 Female D 10 14.00 -4.00 16.0000 1.1428571 Total 150 150.00 0.00 135.2356 6.1271010 So we have \\(\\chi^2_{df=2} \\approx 6.13\\). Looking up the p-value in the online calculator shows the \\(p-value\\) to be \\(0.046654\\). Note that you can also conduct the test online so long as you have the observed and expected frequencies. Note also that you cannot have more than 20% of your cells with an expected frequency \\(&lt; 5\\) or any cell with expected frequencies \\(&lt; 1\\). When that happens, an alternative test, Fisher's Exact Test, is recommended. Here is another example, this time from the Open Data Policing project. The specific piece of data I want to look at are the number of traffic stops resulting in a ticket, by race, in the Greater Chicago area. TABLE 9.9: Race, Traffic Stops, and Tickets Race Tickets Stops White 222 18206 Black 1138 22560 Hispanic 1141 12643 Asian 22 3102 Total 2523 56511 \\[\\begin{array}{l} H_0: \\text{Driver&#39;s race is independent of traffic stops resulting in a ticket} \\\\ H_1: \\text{Driver&#39;s race is NOT independent of traffic stops resulting in a ticket} \\end{array}\\] Using the online calculator we find \\(\\chi^2_{df=3} = 1077.88\\) with a \\(p-value &lt; 0.0001\\) and can easily reject the null hypothesis; these data suggest that driver’s race and getting a ticket are not independent. 9.3 Fisher’s Exact Test This test involves calculating the probability of ending up with the observed frequencies as recorded. It is computationally intensive because it involves calculating, under the assumption that \\(H_0\\) is true, all possible tables that would yield the same row totals and column totals. For example, if we had a \\(2 \\times 2\\) contingency table such as the one below, TABLE 9.10: A Stylized Example of Fisher’s Exact Test Column 1 Column 2 Total Row 1 1 8 9 Row 2 4 5 9 Total 5 13 18 the perturbations of cell frequencies would be as shown below, all holding the row and column totals fixed: TABLE 9.11: All Possible Tables with fixed Marginals (1) Column 1 Column 2 Total Row 1 0 9 9 Row 2 5 4 9 Total 5 13 18 Column 1 Column 2 Total Row 1 1 8 9 Row 2 4 5 9 Total 5 13 18 TABLE 9.11: All Possible Tables with fixed Marginals (2) Column 1 Column 2 Total Row 1 2 7 9 Row 2 3 6 9 Total 5 13 18 Column 1 Column 2 Total Row 1 3 6 9 Row 2 2 7 9 Total 5 13 18 TABLE 9.11: All Possible Tables with fixed Marginals (3) Column 1 Column 2 Total Row 1 4 5 9 Row 2 1 8 9 Total 5 13 18 Column 1 Column 2 Total Row 1 5 4 9 Row 2 0 9 9 Total 5 13 18 The probability of observing a given table would have to be calculated as \\(\\dfrac{(a+b)!(c+d)!(a+c)!(b+d)!}{n!a!b!c!d!}\\), and once that is done, the \\(p-value\\) for Fisher’s exact test calculated by summing all probabilities less than or equal to the probability of the observed table. I am sure it is clear why you would only do this test via a computer, not by hand. There is an online calculator as well for \\(2 \\times 2\\), \\(2 \\times 4\\), and \\(3 \\times 3\\) contingency tables. To complete the example, note that the observed table has a probability of \\(0.132\\). The \\(p-value\\) for Fisher’s exact test is the sum of four of the other tables \\(0.132 + 0.132 + 0.0147 + 0.0147 = 0.293\\). This \\(pvalue\\) is not \\(\\leq 0.05\\) so we would fail to reject the null hypothesis; the data suggest that the row variable and the column variable are independent. 9.4 A Cautionary Tale One has to be careful when working with large samples because if you have enough data, finding statistical significance becomes easy. You can see this very clearly with the \\(\\chi^2\\). Washington state’s Public Interest Research Group (PIRG) found in its recent study that 46% of full-time college students work 25 or more hours per week. A sample of 200 included 90 who worked 1-15 hours per week, 60 who worked 16-24 hours per week, and 50 who worked 25-34 hours per week. Students were also asked if their work had a positive, negative, or no effect on their grades. Assume you have access to the data shown in the “Small Sample” table below while your neighbor has access to the data shown in the “Large Sample” table. Look at the data carefully and note that the cell frequencies differ by a constant factor, that is the only difference. TABLE 9.12: A Cautionary Tale about Sample Sizes (Sample 1) Positive None Negative Total 1-15 hours 26 50 14 90 16-24 hours 16 27 17 60 25-34 hours 11 19 20 50 Total 53 96 51 200 TABLE 9.12: A Cautionary Tale about Sample Sizes (Sample 2) Positive None Negative Total 1-15 hours 260 500 140 900 16-24 hours 160 270 170 600 25-34 hours 110 190 200 500 Total 530 960 510 2000 Now, if you calculated the \\(\\chi^2\\) for both sample sizes, the Small Sample would yield \\(\\chi^2_{df=4} = 10.59\\) and with \\(\\alpha = 0.01\\) you would be unable to reject the null hypothesis because the \\(p-value = 0.03157959\\). In contrast, the large sample would yield \\(\\chi^2_{df=4} = 106.03\\) and this would have a \\(p-value = 5.109665e-22\\), which is practically \\(0\\), easily allowing you to reject the null hypothesis of no association. How come such contrasting conclusions? Strictly because of a difference in sample size. What should we do then? For the moment, nothing, since the solutions go beyond the purview of this course. I do, however, want you to recognize the issue of large samples being preordained to more often than not yield statistically significant results. This recognition will come in handy should you ever have large samples to work with. 9.5 Chapter 9 Practice Problems For all problems involving a hypothesis test, you must state the null and the alternative nhypotheses AND your conclusion, in words. Merely saying “we reject” or “we fail to reject” is insufficient. Problem 1 Revisit the Titanic data-set and answer the following questions with a suitable test: Was survival independent of a passenger’s class of travel? Was survival independent of whether the passenger’s sex? Problem 2 The Youth Risk Behavior Surveillance System (YRBSS) was developed in 1990 to monitor priority health risk behaviors that contribute markedly to the leading causes of death, disability, and social problems among youth and adults in the United States. These behaviors, often established during childhood and early adolescence, include (i) Behaviors that contribute to unintentional injuries and violence; (ii) Sexual behaviors related to unintended pregnancy and sexually transmitted infections, including HIV infection; (iii) Alcohol and other drug use; (iv) Tobacco use; (v) Unhealthy dietary behaviors; and (vi) Inadequate physical activity. In addition, the YRBSS monitors the prevalence of obesity and asthma and other priority health-related behaviors plus sexual identity and sex of sexual contacts. From 1991 through 2015, the YRBSS has collected data from more than 3.8 million high school students in 1,700+ separate surveys. The problems that follow rely upon the YRBSS 2015 data and the documentation for the data-set can be found here. Read the documentation carefully, in particular, the details of the survey questions. Then answer the following questions: Were males or females (Q2) more likely to ride often with a driver who had been drinking (Q10)? Is a youth’s frequency of riding with a driver who had been drinking (Q10) independent of sex (Q2)? Is physical dating violence (Q22) associated with sex (Q2)? Who was more likely to be subjected to physical dating violence? Is sex (Q2) and ever considered suicide (Q27) related? Who is less likely to have considered suicide? Problem 3 Gallup has long measured Partisanship, essentially splitting the electorate into Republicans, Independents and Democrats. In 1988, the distribution was 31% Republicans, 33% Independents, and 36% Democrats. A recent poll conducted by a different polling firm found 1,746 likely voters breaking down their party identification as 468 Republicans, 522 Democrats and 756 Independents. Has the distribution of partisanship changed since 1988? Problem 4 Why does America vote as it does on Election Day? The mission of the American National Election Studies (ANES) is to inform explanations of election outcomes by providing data that support rich hypothesis testing, maximize methodological excellence, measure many variables, and promote comparisons across people, contexts, and time. The ANES serves this mission by providing researchers with a view of the political world through the eyes of ordinary citizens. Use these data to answer the following questions: Is a respondent’s perception of whether things are generally on the right/wrong track in the country (V161081) independent of whether she/he and her/his family are better/worse off than a year ago (V161110)? Is a respondent’s view on marijuana legalization (V162179) independent of his/her opinion on government regulation of business (V162186)? Were older people (V161267x) more likely to favor legalizing marijuana (V162179)? Problem 5 The World Values Survey (WVS) is a global network of social scientists studying changing values and their impact on social and political life, led by an international team of scholars, with the WVS Association and WVSA Secretariat headquartered in Vienna, Austria. The survey, which started in 1981, seeks to use the most rigorous, high-quality research designs in each country. The WVS consists of nationally representative surveys conducted in almost 100 countries which contain almost 90 percent of the world’s population, using a common questionnaire. The WVS is the largest non-commercial, cross-national, time series investigation of human beliefs and values ever executed, currently including interviews with almost 400,000 respondents. Moreover the WVS is the only academic study covering the full range of global variations, from very poor to very rich countries, in all of the world’s major cultural zones. One of the WVS data-sets is available here in SPSS format with the Excel version available here, and the documentation is available here. Use the 2014 data to answer the following questions: questions: Is religiosity (V147) associated with agreement/disagreement with the statement that “When jobs are scarce, men should have more right to a job than women” (V45)? What patterns are evident in the data? What about the possible relationship between an individual’s sex (V240) and V45? What patterns are evident in the data? Problem 6 There is a common perception that hate crimes on university/college campuses tend to occur on the basis of sexual orientation (60%), then on the basis of race (30%), and the remaining hate crimes occur on the basis of religion. In 2015, the following number of hate crimes were recorded by the Federal Bureau of Investigation (FBI): Race = 108, Religion = 51, Sexual Orientation = 32. Does this suggest that the common perception should be rejected? Problem 7 Windows kill more birds than any other human-related factor. Somewhere between 100 million and a billion birds die each year by crashing into windows on buildings in North America alone, which represents up to 5% of the total number of birds in the area. One possible mechanism to prevent this mortality is angling windows downwards slightly, which would then reflect the ground rather than an image of the sky to the flying bird. An experiment was done to look at the number of birds that died as a result of vertical windows, windows angled 20 degrees off vertical and 40 degrees off vertical (Klem Jr et al. (2004)). The angles were randomly assigned to six identical windows, and the assignments were randomly varied daily for four months. The amount of time that windows were at each of the three angles was the same. Over the course of the experiment, 30 birds were killed by windows in the vertical orientation, 15 were killed by windows set at 20 degrees off vertical, and 8 were killed by windows set at 40 degrees. Use an appropriate test to determine if bird deaths are independent of window angles. Problem 8 To the casual observer, any profession must have as many highly rated workers as there are poorly rated workers. Most of us assume the same should be true for education. In the recent past, New York had the following distribution of the 35,752 teachers for whom the state had calculated a teacher effectiveness rating: 7% Highly Effective; 77% Effective; 11% Developing, and 5% Ineffective. Use an appropriate test to determine if the casual observer’s assumption of evenly distributed teacher effectiveness has any merit to it. Problem 9 American municipal governments are organized in different ways, ranging from the Mayor-Council form of government – Elected council or board serves as the legislative body. The chief elected official is the head of government, with significant administrative authority, generally elected separately from the council. Council-Manager form of government – Elected council or board and chief elected official (e.g., mayor) are responsible for making policy with advice of the chief appointed official. A professional administrator appointed by the board or council has full responsibility for the day-to-day operations of the government. Commission form of government – Members of a board of elected commissioners serve as heads of specific departments and collectively sit as the legislative body of the government. Town Meeting form of government – Qualified voters convene to make basic policy and to choose a board of selectmen. The selectmen and elected officers carry out the policies established by the government. Representative Town Meeting form of government – Voters select citizens to represent them at the town meeting. All citizens may attend and participate in debate, but only representatives may vote. Are these forms of government evenly distributed? Survey data indicate the following distribution, based on a total of 4,000 responses: Mayor-Council (33%), Council-Manager (59%), Commission (1%), Town Meeting (6%), and Representative Town Meeting (1%). Problem 10 Hurricanes hit the US often and hard, causing some loss of life and many economic costs. They are ranked in severity by the Saffir-Simpson scale, which ranges from category 1 to category 5, with 5 being the worst. In some years, as many as 3 hurricanes that rate a category 3 or higher hit the US coastline, while in other years no hurricane of this severity hits the US. The following table gives the number of years that had 0, 1, 2, 3 or more hurricanes of at least category 3 in severity, over the 100 years of the twentieth century Blake et al. 2007: TABLE 9.13: Number of hurricanes Category 3 or higher Number of Years 0 50 1 39 2 7 3 4 4 or more 0 Expected frequencies are 52, 34, 11, 2, and 1. Use an appropriate test to determine whether the observed distribution of hurricanes follows the expected distribution. Problem 11 Given that Halloween is all that and more in some parts of the country, believe it or not we have annual surveys of candy hierarchy. Download the data for 2017 and answer the following questions. Are opinions about Snickers (Q6 | Snickers) independent of whether the respondent is a male or female (Q2 | Gender)? Are opinions about Toblerone (Q6 | Tolberone) (sic!) independent of opinions about Snickers (Q6 | Snickers)? Problem 12 This data-set provides information on a large number of loan applications. Using these data, answer the following questions: What is the most likely “purpose” of a loan? If you focus only on whether the applicant owns or rents their home, is there any relationship between owning (versus renting) a home and the purpose of the loan application? Create a five-group version of loan_amount. Given the resulting distribution, what is the modal loan amount group? Is there any relationship between the grouped loan amount variable and the purpose of the loan? Problem 13 Why are our best and most experienced employees leaving prematurely? The data available here includes information on several current and former employees of an anonymous organization. Fields in the data-set include: satisfaction_level = Level of satisfaction (0-1) last_evaluation = Evaluation of employee performance (0-1) number_project = Number of projects completed while at work average_monthly_hours = Average monthly hours at workplace time_spend_company = Number of years spent in the company Work_accident = Whether the employee had a workplace accident left = Whether the employee left the workplace or not (1 or 0) promotion_last_5years = Whether the employee was promoted in the last five years sales = Department in which they work for salary = Relative level of salary (low med high) Using these data, answer the following questions: Is whether the employee was promoted in the last five years related to a work accident? Is whether the employee was promoted in the last five years related to whether they stayed or left? Is whether an employee left or stayed related to the relative level of their salary? Problem 14 Since 1972, the General Social Survey (GSS) has provided politicians, policymakers, and scholars with a clear and unbiased perspective on what Americans think and feel about such issues as national spending priorities, crime and punishment, intergroup relations, and confidence in institutions. Use the 2016 GSS data available here in SPSS format or the [Excel data here]((https://aniruhil.github.io/avsr/teaching/dataviz/GSS2016.xlsx) to answer the following questions: Is there any relationship between an individual’s sex (sex) and their view of their own spirituality (sprtprsn)? Is there any relationship between an individual’s sex (sex) and whether they oppose or support capital punishment (cappun)? What about between an individual’s highest degree (degree) and whether they oppose or support capital punishment (cappun)? Problem 15 Using the fatal police shootings data-set, answer the questions that follow. Be sure to read the following details of the data-set. The Washington Post is compiling a database of every fatal shooting in the United States by a police officer in the line of duty since Jan. 1, 2015. In 2015, The Post began tracking more than a dozen details about each killing - including the race of the deceased, the circumstances of the shooting, whether the person was armed and whether the victim was experiencing a mental-health crisis - by culling local news reports, law enforcement websites and social media and by monitoring independent databases such as Killed by Police and Fatal Encounters. The Post conducted additional reporting in many cases. In 2016, The Post is gathering additional information about each fatal shooting that occurs this year and is filing open-records requests with departments. More than a dozen additional details are being collected about officers in each shooting. The Post is documenting only those shootings in which a police officer, in the line of duty, shot and killed a civilian - the circumstances that most closely parallel the 2014 killing of Michael Brown in Ferguson, Mo., which began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide. The Post is not tracking deaths of people in police custody, fatal shootings by off-duty officers or non-shooting deaths. The FBI and the Centers for Disease Control and Prevention log fatal shootings by police, but officials acknowledge that their data is incomplete. In 2015, The Post documented more than two times more fatal shootings by police than had been recorded by the FBI. Last year, the FBI announced plans to overhaul how it tracks fatal police encounters. Most of the variables are self-explanatory but the following information may be helpful for two: Race = W: White, non-Hispanic; B: Black, non-Hispanic; A: Asian; N: Native American; H: Hispanic; O: Other; None: unknown Gender = M: Male; F: Female; None: unknown The threat level column was used to flag incidents for the story by Amy Brittain in October 2015. http://www.washingtonpost.com/sf/investigative/2015/10/24/on-duty-under-fire/ As described in the story, the general criteria for the attack label was that there was the most direct and immediate threat to life. That would include incidents where officers or others were shot at, threatened with a gun, attacked with other weapons or physical force, etc. The attack category is meant to flag the highest level of threat. The other and undetermined categories represent all remaining cases. Other includes many incidents where officers or others faced significant threats. The threat column and the fleeing column are not necessarily related. For example, there is an incident in which the suspect is moving away from officers (fleeing) and at the same time turns to fire at gun at the officer. Also, attacks represent a status immediately before fatal shots by police; while fleeing could begin slightly earlier and involve a chase. Is there a relationship between the victim’s race/ethnicity and if the office was wearing a body camera or not? Is there a relationship between the victim’s race/ethnicity and the victim’s sex? References "],["props.html", "Chapter 10 Comparing Proportions 10.1 Specifyng Hypotheses for One-group Tests 10.2 Two-group Tests 10.3 Measuring the Strength of the Association 10.4 Chapter 10 Practice Problems", " Chapter 10 Comparing Proportions Thus far we have worked with continuous and multinomial outcomes but the more common measures you are likely to encounter in the social and behavioral sciences happen to be dichotomous – did the job-training participant get employment \\((y=1)\\) or not \\((y=0)\\)?; did the patient improve post-surgery because he/she received pre-operative physical therapy \\((y=1)\\) or not \\((y=0)\\)? Does the community support this initiative \\((y=1)\\) or not \\((y=0)\\)? Did someone volunteer for the food drive \\((y=1)\\) or not \\((y=0)\\)? The dichotomy may also arise in other ways as, for example, with the questions of whether women are more likely to vote \\((y=1)\\) than men \\((y=0)\\), whether women are more likely to vote Democrat \\((y=1)\\) than men \\((y=0)\\), and so on. With data such as these, while the basic logic of hypothesis testing continues to guide analysis, the manner in which we analyze such data differs from the \\(t-tests\\) used in the preceding chapter. The need for hypothesis testing remains the same: We have to determine whether the proportions we see in our sample are very likely to reflect what is true in the population before we state a claim. Take, for instance, the question of blacks killed by police officers. If you read the story you see statements such as “30% of black victims were unarmed compared to 21% of white victims in 2015”. The Washington Post releases a continually updated data-set of all individuals killed by a police officer and so we could analyze the statement for ourselves. Similarly, we could shatter the myth of the chivalry of the seas with the specific example of the sinking of the Titanic and asking whether women were more likely to survive than men by studying the details of the passenger manifest. These are but two examples of the ways in which categorical data tied to a pressing issue or to a lingering romantic historical artifact can be used to judge claims for ourselves, and to correct the record if necessary. To begin to do so we could recognize that dichotomous (or binary) outcomes where only one of two events is likely to occur at a time per person – a passenger either survives or does not survive – and that the binomial distribution characterizes such outcomes. We covered this distribution in Chapter 6, section 6.2.1.1 to be precise. How could the binomial distribution help us here? Let us start with the naive assumption that in any shipwreck involving passengers of both sexes, the probability of survival is \\(0.50\\) for both men and women. Recall that the probability of observing \\(x\\) successes in \\(n\\) trials of a binomial process is given by \\[\\begin{eqnarray*} P\\left[x \\text{ successes}\\right] = \\binom{n}{x}p^{x}\\left(1 - p\\right)^{n-x} \\\\ \\text{where } \\binom{n}{x} = \\dfrac{n!}{x!(n-x)!} \\text{ and } n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 \\end{eqnarray*}\\] How many female passengers (aka trials) do we have? 466. Let us take this piece of data, together with \\(p=0.50\\) to generate the distribution of female passengers’ survival via the binomial distribution. The resulting number of female passengers surviving the shipwreck is shown below. The distribution peaks at 233, which is \\(=0.5 \\times 466\\) … i.e., the most likely outcome is that one-half of the female passengers survive. FIGURE 10.1: Binomial Distribution of Female Passenger’s Survival with p = 0.5 How many of the female passengers survived the Titanic sinking? Some 339 did, which makes the actual probability of survival approximately \\(0.7275\\). If this were the true probability of a female passenger surviving a shipwreck, the distribution of survivals should have looked as follows: FIGURE 10.2: Binomial Distribution of Female Passenger’s Survival with p = 0.5 vs. p = 0.7275 Note how the actual distribution of survival (in blue) differs from the expected distribution of survival (in red), and this difference is stark. But this is no surprise given that we had seen a massive gap between the expected probability of \\(0.50\\) and the actual probability of \\(0.7275\\). Assuming no complications with the data and factors shaping survival, this simple analysis would lead us to conclude that survival probabilities, at least on the Titanic, were quite a bit higher than \\(0.5\\) for female passengers. What about for the 843 male passengers? Their survival probability was about \\(0.1910\\), leading to two very different distributions of survival for the sexes. FIGURE 10.3: Binomial Distribution of Female vs. Male Passengers This side-by-side comparison would also lead us to suspect that males were less likely to survive than females. But how could we draw these sorts of conclusions via hypothesis tests? As it turns out, in a fairly straightforward manner, and I demonstrate the approach below. 10.1 Specifyng Hypotheses for One-group Tests Now we no longer refer to the null population mean \\(mu\\) but instead to the null population proportion \\(p_0\\) when constructing hypotheses, with the sample proportion represented by \\(\\bar{p}\\). In particular, the hypotheses will look as follows: \\[\\begin{eqnarray*} \\text{Lower Tail Test } \\ldots H_{0}: p \\geq p_{0}; H_{1}: p &lt; p_{0} \\\\ \\text{Upper Tail Test } \\ldots H_{0}: p \\leq p_{0}; H_{1}: p &gt; p_{0} \\\\ \\text{Two Tailed Test } \\ldots H_{0}: p = p_{0}; H_{1}: p \\neq p_{0} \\end{eqnarray*}\\] The sample standard deviation \\((s)\\) is calculated as \\(s = \\sqrt{p_0 \\times ( 1 - p_0)}\\). Note that we are using \\(p_0\\) and not \\(\\bar{p}\\). The standard error \\((s_{\\bar{p}})\\) is then calculated as \\(s_{\\bar{p}} = \\dfrac{s}{ \\sqrt{n} } = \\dfrac{ \\sqrt{ p_0 \\times \\left(1 - p_0 \\right) } }{ \\sqrt{n} }\\). The test statistic is: \\(z = \\dfrac{ \\bar{p}-p_{0} }{ s_{\\bar{p}} }\\). Again, note that we are using the \\(z\\) distribution and not the \\(t\\). Confidence intervals are calculated as before except the standard error is calculated with the sample proportion \\(\\bar{p}\\), i.e., \\[\\bar{s}_{\\bar{p}} = \\sqrt{ \\dfrac{ \\left( \\bar{p} \\times (1-\\bar{p}) \\right) }{n} }\\] leading to the confidence interval defined as \\(\\bar{p} \\pm z_{\\alpha/2} (\\bar{s}_{\\bar{p}})\\). We also make a coninuity correction used to adjust for the fact that we are approximating a discrete distribution with a continuous distribution, the correction being the addition and subtraction of \\(\\dfrac{0.5}{n}\\) from the upper and lower confidence interval limits, respectively. The decision rules stay the same: Specify the hypotheses, set \\(\\alpha = 0.05\\) or \\(\\alpha=0.01\\), reject the null hypothesis if the \\(p-value \\leq \\alpha\\), state the conclusion in words, and then report and interpret the confidence interval for the estimate. 10.1.1 The Normal Approximation to One-group Tests We will start with the normal approximation tests before switching to the more precise tests, if only because you will see the normal approximation tests in use in other texts. 10.1.1.1 Example 1 The probability of surviving a shipwreck is assumed to be \\(0.5\\). When the Titanic went down, only some 38.1971% of its 1,309 passengers survived. Did the Titanic have a significantly lower survival rate than the assumed probability of \\(0.5\\)? \\[\\begin{array}{l} H_0: \\text{ Probability of surviving the Titanic } (p \\geq 0.50) \\\\ H_1: \\text{ Probability of surviving the Titanic } (p &lt; 0.50) \\end{array}\\] Let us use \\(\\alpha = 0.01\\) The standard deviation is \\(s = \\sqrt{p_0 \\times \\left( 1 - p_0 \\right)} = \\sqrt{0.5 \\times \\left( 1 - 0.5\\right)} = \\sqrt{0.25} = 0.5\\) The standard error is \\(s_{\\bar{p}} = \\dfrac{s}{\\sqrt{n}} = \\dfrac{0.5}{\\sqrt{1309}} = \\dfrac{0.5}{36.18011} = 0.01381975\\) The test statistic is \\(z=\\dfrac{\\bar{p}-p_{0}}{s_{\\bar{p}}} = \\dfrac{0.381971 - 0.5}{0.01381975} = \\dfrac{-0.118029}{0.01381975} = -8.540603\\). The \\(p-value\\) is practically \\(0\\) (to be precise, it is \\(6.676161e-18\\)) so we can easily reject the null hypothesis. The data suggest that the Titanic had a significantly lower survival rate than \\(0.5\\). The 99% confidence interval is \\(\\bar{p} \\pm z_{\\alpha/2}(\\bar{s}_{\\bar{p}}) = 0.381971 \\pm 2.327 \\left(\\sqrt{\\dfrac{0.381971 \\times (1 - 0.381971)}{1309}}\\right) = (0.3507213, 0.4132207)\\). With the continuity correction we have \\((0.3507213 - 0.000381971)\\) and \\((0.4132207 + 0.000381971)\\), respectively. That is, we can be about 99% confident that the true survival rate for shipwrecks lies in the interval given by \\(0.3503\\) and \\(0.4136\\). 10.1.1.2 Example 2 Your public school district carried out a drug-use survey and found the proportion of the 497 students reporting “occasional recreational use of opioids” to be \\(0.11\\). The national average is reported to be \\(0.06\\); is your school district’s rate significantly different from the national average? Use \\(\\alpha = 0.05\\). \\[\\begin{array}{l} H_0: \\text{ The district&#39;s rate is not different from the national average } (p = 0.06) \\\\ H_1: \\text{ The district&#39;s rate is different from the national average } (p \\neq 0.06) \\end{array}\\] The standard deviation is \\(s = \\sqrt{p_0 \\times \\left( 1 - p_0 \\right)} = \\sqrt{0.06 \\times \\left( 1 - 0.06\\right)} = \\sqrt{0.0564} = 0.2374868\\) The standard error is \\(s_{\\bar{p}} = \\dfrac{s}{\\sqrt{n}} = \\dfrac{0.2374868}{\\sqrt{497}} = \\dfrac{0.2374868}{22.2935} = 0.01065274\\) The test statistic is \\(z=\\dfrac{\\bar{p}-p_{0}}{s_{\\bar{p}}} = \\dfrac{0.11 - 0.06}{0.01065274} = \\dfrac{0.05}{0.01065274} = 4.693628\\). This has a \\(p-value\\) that is practically \\(0\\) and so we can reject the null hypothesis; these data suggest that the school district’s rate is significantly different from the national average. The 95% confidence interval is \\(\\bar{p} \\pm z_{\\alpha/2} (\\bar{s}_{\\bar{p}}) = 0.11 \\pm 1.96 (0.01403502) = (0.08249136, 0.1375086)\\), and with the continuity correction we have \\((0.08249136 - 0.001006036, 0.1375086 + 0.001006036)\\) indicating that we can be about 95% confident the true rate lies in the interval of 8.14% and 13.85%. 10.1.2 The Binomial Test The normal approximation we used above works well in “large samples” though defining “large” is tricky. Some folks are perfectly content to rely on the central limit theorem to use the normal approximation so long as they have 30 or more cases but others proceed with more caution. The latter group will focus on the known or, if unknown, the suspected population proportion \\(p\\). If \\(p = 0.5\\), then one can get away with the normal approximation when the sample size is 9. As \\(p\\) gets closer to \\(01\\) or \\(1\\), the sample size needed increases. If \\(p=0.8\\) or \\(p=0.2\\), you need \\(n=36\\) and if \\(p=0.9\\) or \\(p=0.1\\) you need \\(n=81\\). Others will eschew the normal approximation in favor of more precise binomial tests. I follow this route because with the availability of computers there is really no longer any need to use the normal approximation with proportions. In fact, you don’t even need a statistics package, all you need is a calculator and a browser and you can run the tests via, for example, this applet or this applet. 10.1.2.1 Example 1 Revisited Using the first applet, enter the values asked for and click Calculate. The first line of results will have a row called “Binomial Probability: P(X = 500)” and the corresponding \\(p-value\\) will be shown as \\(&lt; 0.000001\\). This \\(p-value\\) is clearly smaller than \\(\\alpha = 0.05\\) so we can reject the null hypothesis. This is the same conclusion we arrived at earlier. The 99% confidence interval is \\(0.3480\\) and \\(0.4170\\), respectively. 10.1.2.2 Example 2 Revisited Once again, if you enter the values asked for and calculate the \\(p-value\\) you will see “Cumulative Probability: P(X &gt; 55)” with a corresponding value of \\(5.74906923800356E-06\\). Since it is a two-tailed hypothesis test, the \\(p-value\\) becomes \\(0.00001149814\\), again allowing us to reject the null hypothesis. The 95% confidence interval is \\(0.0858\\) and \\(0.1414\\), respectively. 10.2 Two-group Tests What if we have two groups instead of one as, for example, in the question of survival rates of males versus females on the Titanic, or even drug use among male versus female students in the school district? How can we carry out hypothesis tests for these designs? 10.2.1 Example 1 Assume, for example, that we are studying the Titanic disaster and interested in how survival rates differed by the passenger’s sex. A simple cross-tabulation of sex by survival status would look like the following: FIGURE 10.4: Titanic Passengers’ Survival by Sex TABLE 10.1: Survival Status by Sex Died Survived Total female 127 339 466 male 682 161 843 Total 809 500 1309 The bar-chart shows a marked difference in the chance of survival for male versus female passengers, with female passengers more likely to survive. Specifically, only 19.1% of males survived versus 72.75% of females. The same pattern is visible in the contingency table as well. On the basis of these visualizations is is very likely that if we test for an association between sex and survival we are likely to reject the null hypothesis of no association between the two variables. Let us now turn to the hypothesis test. In essence, we have a \\(2 \\times 2\\) contingency table and could test whether the proportion of female deaths differs from the proportion of male deaths via a normal approximation but instead we will rely upon two other tests – (i) the \\(\\chi^2\\) test, and (ii) the more precise Fisher’s Exact test. 10.2.2 The \\(\\chi^2\\) Test We saw this test in action in the preceding chapter so the mechanics are familiar to us. Let us run through them. \\[\\begin{array}{l} H_0: \\text{ Survival was independent of a passenger&#39;s sex } (p_{male} = p_{female}) \\\\ H_1: \\text{ Survival was NOT independent of a passenger&#39;s sex } (p_{male} \\neq p_{female}) \\end{array}\\] Using the online calculator we obtain \\(\\chi^2_{1} = 363.62\\) with a \\(p-value &lt; 0.0001\\) so we can easily reject the null hypothesis; these data suggest that survival was not independent of a passenger’s sex. 10.2.3 Fisher’s Exact Test There is an online calculator for \\(2 \\times 2\\) tables; see here, although you won’t see the exact \\(p-value\\) reported but it is the one calculator we currently have. As it turns out, the \\(p-value\\) is indeed very small (almost \\(0\\)) and so this too allows us to reject the null hypothesis. 10.2.4 Example 2 Suicidal deaths tend to be very high in India, with the causes ranging from indebtedness to a sense of shame for having failed an exam, divorce, and other life course events. Farmers tend to be one of the more vulnerable groups with, by some account, some 60,000 farmers having committed suicide over the past three decades due to rising temperatures and the resultant stress on India’s agricultural sector. We have access to data on suicides over the 2001-2012 period, and this data-set contains information on 3,193 of 19,799 suicides that occurred in 2012. The question of interest is whether men and women are just as likely to commit suicide for the following two causes: (i) Fall in Social Reputation, and (ii) Dowry Dispute. \\[\\begin{array}{l} H_0: \\text{ There is no association between the cause and the suicide victim&#39;s sex} \\\\ H_1: \\text{ There is an association between the cause and the suicide victim&#39;s sex} \\end{array}\\] The contingency table and bar-chart are shown below: TABLE 10.2: Suicide Cause by Sex (India, 2012) Dowry Dispute Fall in Social Reputation Total Female 47 50 97 Male 14 79 93 Total 61 129 190 FIGURE 10.5: Cause of Suicide by Sex Running the \\(\\chi^2\\) test yields \\(\\chi^2_{df = 1} = 22.79\\) with a \\(p-value = 0.000001807\\) and so we can reject the null hypothesis; these data suggest that cause of suicide and the victim’s sex are not independent. 10.3 Measuring the Strength of the Association Merely rejecting or failing to reject a null hypothesis is rarely the ultimate goal of any analysis. Say you reject the null hypothesis of a passenger’s survival being independent of their sex. Wouldn’t you at least want to know how strong is the association between these two variables? You often do, and we can answers questions about the strength of the association between categorical variables (be they nominal or ordinal) by way of some statistics. In this section we will not only look at our options but also at what measure of association should be used and when. Say our contingency table is made up of two nominal variables, one being support for abortion and the other being the respondent’s educational attainment level. The data are mapped for you in the table that follows: TABLE 10.3: Educational Attainment and Support for Abortion Less than High School High School or More Total No Support (n) NA NA 555.0 No Support (%) NA NA 57.1 Yes Support (n) NA NA 417.0 Yes Support (%) NA NA 42.9 Total (n) NA NA 972.0 Total (%) NA NA 100.0 Having rejected the null hypothesis our interest now is in being able to predict support for abortion from educational attainment levels because we suspect that support increases with education. In the language of data analysis, support for adoption would be called the dependent variable while educational attainment would be labeled the independent variable. You only see row totals of 555 and 417; i.e., total for and against abortion, respectively. You pick one person at random. Is this person most likely to support abortion or most likely not to support abortion? Well, the only thing you can do is look at the modal response, which was 555 individuals indicating no support for abortion. So your best bet would be to expect the individual you drew at random to echo no support for abortion. Now, what if you were provided the missing information, the missing cell frequencies? TABLE 10.4: Educational Attainment and Support for Abortion Less than High School High School or More Total No Support (n) 399.0 156.0 555.0 No Support (%) 64.3 44.4 57.1 Yes Support (n) 222.0 195.0 417.0 Yes Support (%) 35.7 55.6 42.9 Total (n) 621.0 351.0 972.0 Total (%) 100.0 100.0 100.0 Now, a few things would become readily apparent. First, working with only the modal response you would have incorrectly predicted no support for abortion for 417 individuals. The percent correctly predicted would thus have been \\(\\dfrac{555}{972} \\times 100 = 57.1\\). Second, if you took educational attainment into account, how many errors would you make? You might have thought everyone with \\(&gt;\\) High School supports abortion but only \\(195\\) do so, leading you to make \\(156\\) errors here. Similarly, you expected everyone with \\(\\leq\\) High School to oppose abortion but only \\(399\\) do so, leading you to make \\(222\\) errors here. Total errors when taking education (the independent variable) into account would then sum to \\(156 + 222 = 378\\). These errors are fewer in number – \\(378\\) versus \\(417\\) – than when you lacked any information the breakdown of support by education. In a way, then, you have reduced prediction error by folding in information on an individual’s educational attainment. This predictive leverage is what lies behind the notion of proportional reduction in error (PRE), with \\(0 \\leq PRE \\leq 1\\) and the closer is PRE to \\(1\\) the more the reduction in prediction error. Technically, PRE is calculated as follows: Set \\(E_1 = n -\\) Modal frequency \\(= 972 - 555 = 417\\). This is the number of prediction errors made by ignoring an independent variable. \\(E_2 = (n_{column_i} - mode_{column_i}) + (n_{column_j} - mode_{column_j})\\), \\(\\therefore E_2 = (621 - 399) + (351 - 195) =156 + 222 = 378\\). This is the number of prediction errors made by taking an independent variable into account. Now calculate \\(PRE = \\dfrac{E_1 - E_2}{E_1} = \\dfrac{417 - 378}{417} = \\dfrac{39}{417} = 0.09\\). We improved our predictive ability by 9% when using educational attainment as compared to if we ignored or did not have access to information about an individual’s educational attainment. There are several PRE measures that could be used, depending upon the nature of the variables that comprise the contingency table. Let us see each of these in turn. 10.3.1 Goodman-Kruskal Lambda \\((\\lambda)\\) \\(\\lambda = \\dfrac{E_1 - E_2}{E_1}\\) was used in the preceding example that explained the concept of proportional reduction in error. It is an asymmetrical measure of association in that its value will differ depending upon which variable is used as the dependent variable. For example, consider the following two tables, built with the same data except that one uses violence as the dependent variable while the other uses assailant’s status as the dependent variable. In the first table, \\[\\begin{array}{l} \\lambda = \\dfrac{E_1 - E_2}{E_1} \\\\ E_1 = n - \\text{ Modal frequency of the dependent variable} \\\\ \\therefore E_1 = 9,898,980 - 8,264,320 = 1,634,660 \\\\ E_2 = (5,045,040 - 3,992,090 = 1,052,950) + (4,853,940 - 4,272,230 = 581,710) = 1,634,660 \\\\ \\lambda = \\dfrac{E_1 - E_2}{E_1} = \\dfrac{1,634,660 - 1,634,660}{1,634,660} = 0 \\end{array}\\] In the second table, \\[\\begin{array}{l} \\lambda = \\dfrac{E_1 - E_2}{E_1} \\\\ E_1 = n - \\text{ Modal frequency of the dependent variable} \\\\ \\therefore E_1 = 9,898,980 - 5,045,040 = 4,853,940 \\\\ E_2 = (472,760 - 350,670) + (1,161,900 - 930,860) + (8,264,320 - 4,272,230) \\\\ \\therefore E_2 = (122,090 + 231,040 + 3,992,090) = 4,345,220 \\\\ \\lambda = \\dfrac{E_1 - E_2}{E_1} = \\dfrac{4,854,940 - 4,345,220}{4,853,940} = 0.1048 \\end{array}\\] Conventionally, the dependent variable is always the row variable and you should follow this rule. 10.3.2 Phi \\((\\phi)\\) Coefficient If both variables are nominal and you have a \\(2 \\times 2\\) contingency table, then we use the phi coefficient to measure the strength of the association between the two variables. \\(\\phi = \\sqrt{\\dfrac{\\chi^2}{n}}\\) Technically, if the table is as follows, TABLE 10.5: Calculating phi Alive Dead Drug A a b Drug B c d where \\(\\phi = \\dfrac{ad - bc}{\\sqrt{(a+b)(c+d)(a+c)(b+d)}}\\) with \\(df=(r-1)(c-1)\\). 10.3.3 Cramer’s \\(V\\) and Contingency \\(C\\) Cramer’s \\(V\\) is used when both variables are nominal but at least one has more than \\(2\\) categories. Cramer’s V \\(= \\sqrt{\\dfrac{\\chi^2}{n \\times m}}\\) where \\(m\\) is the smaller of \\((r-1)\\) or \\((c-1)\\). Contingency Coefficient \\(C = \\sqrt{ \\dfrac{\\chi^{2}}{\\chi^{2} \\times n} }\\) is recommended for \\(5 \\times 5\\) tables since it appears to underestimate the strength of the association in smaller tables. Both \\(c\\) and \\(V\\) will fall in the \\(\\left[0,1\\right]\\) range, i.e., \\(0 \\leq V \\leq 1\\) and \\(0 \\leq C \\leq 1\\). Reference tables are available to classify the strength of the association: For \\(df=1\\), Small effect if \\(0.10 &lt; V \\leq 0.30\\) Medium effect if \\(0.30 &lt; V \\leq 0.50\\) Large effect if \\(V &gt; 0.50\\) For \\(df=2\\), Small effect if \\(0.07 &lt; V \\leq 0.21\\) Medium effect if \\(0.21 &lt; V \\leq 0.35\\) Large effect if \\(V &gt; 0.35\\) For \\(df=3\\), Small effect if \\(0.06 &lt; V \\leq 0.17\\) Medium effect if \\(0.17 &lt; V \\leq 0.29\\) Large effect if \\(V &gt; 0.29\\) 10.3.4 Goodman-Kruskal Gamma \\((\\gamma)\\) What if we have two ordinal variables, as is the case in the contingency table shown below? TABLE 10.6: Two Ordinal Variables High School or More Less than High School Total High Financial Satisfaction 1194 193 1387 Low Financial Satisfaction 477 147 624 Total 1671 340 2011 There are four pairs in the table – High education and High financial satisfaction, High education and Low financial satisfaction, Low education and High financial satisfaction, and Low education and Low financial satisfaction. Let us call these pairs High-High, High-Low, Low-High, and Low-Low. The research question in play is whether education has any impact on financial satisfaction. If it does so perfectly, without any error, then we should see all those high on education to also be high on financial satisfaction, and likewise all those low on education to also be low on financial satisfaction. However, that is not the case; we do see High-Low and Low-High pairs with non-zero frequencies! One way to evaluate the association between the two variables might be to how many concordant pairs there are (High-High and Low-Low) versus disordant pairs (High-Low, Low-High). Let us label the concordant pairs \\(N_s\\) and the discordant pairs \\(N_d\\). Then, the total number of concordant pairs possible would be given by the number High-High \\(\\times\\) the number Low-Low. Similarly, the total number of discordant pairs would be given by the number High-Low \\(\\times\\) the number Low-High. That is, for the given table, we would calculate \\[\\begin{array}{l} N_s = 1194 \\times 147 = 175,518 \\\\ N_d = 193 \\times 477 = 92,061 \\end{array}\\] If most of the pairs possible are concordant, then we could calculate a ratio of the difference between the number of concordant and discordant pairs to the total number of pairs (concordant + discordant). This is the measure of association called Goodman-Kruskal’s gamma \\((\\gamma)\\) where \\[\\gamma = \\dfrac{N_s - N_d}{N_s + N_d}\\] such that \\[\\begin{array}{l} N_s &gt; N_d, \\gamma \\to +1 \\\\ N_s &lt; N_d, \\gamma \\to -1 \\\\ N_s = N_d, \\gamma = 0 \\end{array}\\] In the present example, we have \\(\\gamma = \\dfrac{N_s - N_d}{N_s + N_d} = \\dfrac{175,518 - 92,061}{175,518 + 92,061}=0.31\\), indicating a moderate impact of education on financial satisfaction. Note, before we move on, that if \\(N_s = 100, N_d = 0, \\gamma = \\dfrac{N_s - N_d}{N_s + N_d} = \\dfrac{100 - 0}{100 + 0} = 1\\) and if \\(N_s = 0, N_d = 100, \\gamma = \\dfrac{N_s - N_d}{N_s + N_d} = \\dfrac{0 - 100}{0 + 100} = -1\\) 10.3.5 Kendall’s \\((\\tau_b)\\) Unfortunately, however, \\(\\gamma\\) ignores what are called tied pairs. Say what? TABLE 10.7: Tied Dependent and Independent Pairs High School or More Less than High School Total High Financial Satisfaction 1194 193 1387 Low Financial Satisfaction 477 147 624 Total 1671 340 2011 With tied dependent pairs \\((T_y)\\), we have individuals with the same value of the dependent variable but different values of the independent variable. Here, 1,194 and 193 are tied on the dependent variable value of High while 477 and 147 are tied on the dependent variable value of Low. With tied independent pairs \\((T_x)\\), we have individuals with the same value of the independent variable but different values of the dependent variable. Here, 1194 and 477 are tied on the independent variable value of High while 193 and 147 are tied on the independent variable value of Low. \\[\\begin{array}{l} T_y = (1,194 \\times 193) + (477 \\times 147) = 230,442 + 70,119= 300,561 \\\\ T_x = (1,194 \\times 477) + (193 \\times 147) = 569,538 + 28,371 = 597,909 \\end{array}\\] Then, we can calculate \\(\\gamma&#39;s\\) replacement:\\(\\tau_b = \\dfrac{N_s - N_d}{\\sqrt{(N_s + N_d + T_y)(N_s + N_d + T_x)}} = \\dfrac{83,457}{701,226} =0.12\\), indicative of a weak positive association. This estimate is much smaller than what we had for \\(\\gamma\\) but that should be no surprise given that \\(\\tau_b\\) will, as a rule, be \\(&lt; \\gamma\\) because \\(\\tau_b\\) takes all tied pairs into account whereas \\(\\gamma\\) does not. \\(\\tau_b\\) is best used with square tables – \\(2 \\times 2\\), \\(3 \\times 3\\), and so on. 10.3.6 Kendall’s \\((\\tau_c)\\) This measure can be used instead of \\(\\tau_b\\) when you have tables that are not square – \\(2 \\times 3\\), \\(3 \\times 4\\), and so on. Specifically, \\(tau_c = \\left(N_s - N_d\\right) \\times \\left[2m / (n^{2} (m - 1) )\\right]\\), where \\(m\\) is the number of rows or columns, whichever is smaller, and \\(n\\) is sample size. All properties that hold for \\(\\tau_b\\) apply to \\(\\tau_c\\) as well. 10.3.7 Somer’s \\(D\\) \\(\\gamma, tau_b, tau_c\\) are all symmetric measures in that it does not matter what variable is an independent variable and what variable is dependent. Indeed, you may have no a priori idea of what is your dependent variable when using these and the resulting estimates would be fine. However, when you have a priori hypotheses about a particular variable being the dependent variable of interest, then you should use Somer’s \\(D\\). \\[D_{yx} = \\dfrac{N_s - N_d}{N_s + N_d + T_y}\\] Again, because this measure adjusts for tied dependent pairs its value will be less than \\(\\gamma\\). In the ongoing example of education and financial satisfaction, \\[D_{yx} = \\dfrac{N_s - N_d}{N_s + N_d + T_y} = \\dfrac{175,518 - 92,061}{175,518 + 92,061 + 300,561} = \\dfrac{83,457}{568,140} = 0.1468951\\] Somer’s \\(D\\) will work with square and non-square tables of any size. 10.4 Chapter 10 Practice Problems Problem 1 Radioactive waste, John Wayne, and Hollywood. Yup, not making that up, read it for yourself! Turns out that John Wayne starred in a movie (The Conqueror) that was shot in St. George (Utah) in 1954, on a site 137 miles downwind of A nuclear testing site in Yucca Flat (Nevada). As it happens, by the early 1980s some 91 of the 220 cast and crew had been diagnosed with cancer of one form or another, and . According to epidemiological data available to us, only 14% of the population this group of 220 represented should have been diagnosed with cancer between 1954 and the early 1980s. Was a cast or crew member more likely to get cancer because of exposure to radioactive waste while working on the film? Problem 2 Suicides and the holiday season, a common myth implies, go hand in hand. Similarly, “articles in the medical literature and lay press have supported a belief that individuals, including those dying of cancer, can temporarily postpone their death to survive a major holiday or other significant event, but results and effects have been variable” (Young and Hade 2004). To study this latter question, whether death does indeed take a holiday or not, the authors looked at 12,028 cancer deaths that occurred in Ohio between 1989 and 2000. Of these deaths, 6,052 occurred on the week before Christmas while the rest occurred in the week after Christmas. Do these data suggest that people are really able to postpone their passing until after the holiday season? Problem 3 Young and Hade (2004) also looked at the breakdown of these cancer deaths by the deceased’s sex, finding that of the 6,252 men, 3,192 died in the week before Christmas while of the 5,776 women, 2,858 died in the week before Christmas. Do these data suggest a difference in the ability of men and women to control the timing of their passing? Problem 4 Does a driver’s race have any impact on the probability that they will be stopped more often than a driver of another race? When stopped, what is the “hit rate” for each group – the probability that the stop actually results in some contraband being found on the driver or in the vehicle? If the hit rate is lower for minority drivers, that is typically used as an “outcome test” to argue that minority drivers are pulled over without good cause more often than are non-minority drivers. This isn’t foolproof evidence though because of the problem of “infra-margionality”, as has been argued here. Let us look at Rhode Island’s data, provided to us courtesy of the Stanford Open Policing Project 2017. You can download the full data-set from here. Once you download the data I want you to create a simple dummy variable that groups the drivers into one of two mutually exclusive groups – Minority \\(=1\\) if the driver’s race is “Black” or “Hispanic” and Minority \\(=0\\) if the driver’s race is recorded as “White”. Then calculate the search rates and hit rates for each group. The result should be a table such as the following: Group Stops Searches Hits Search Rate Hit Rate Minority 69883 3533 1475 0.05055593 0.4174922 Non-Minority 159808 3759 1828 0.02352198 0.4862995 Now answer the following questions: Are search rates (i.e., number of searches per number of stops) independent of a driver’s minority status? Are hit rates (i.e., number of hits per number of searches) independent of a driver’s minority status? When you combine the information about search rates with information on hit rates, what story emerges? Does this suggest a possible pattern of discrimination? Problem 5 Some 15.9% of all fiurst time enrollees in a two- or four-year program at a college/university tend to be first-generation students (students who are the first in their family to study beyond high school). A major public university in the Midwest claims that of the 6,000 students who enrolled in the Fall of 2017, some 22.7% were first-generation students. Is this university’s rate significantly different from the national average? Problem 6 The Youth Risk Behavior Surveillance System (YRBSS) was developed in 1990 to monitor priority health risk behaviors that contribute markedly to the leading causes of death, disability, and social problems among youth and adults in the United States. These behaviors, often established during childhood and early adolescence, include (i) Behaviors that contribute to unintentional injuries and violence; (ii) Sexual behaviors related to unintended pregnancy and sexually transmitted infections, including HIV infection; (iii) Alcohol and other drug use; (iv) Tobacco use; (v) Unhealthy dietary behaviors; and (vi) Inadequate physical activity. In addition, the YRBSS monitors the prevalence of obesity and asthma and other priority health-related behaviors plus sexual identity and sex of sexual contacts. From 1991 through 2015, the YRBSS has collected data from more than 3.8 million high school students in 1,700+ separate surveys. The problems that follow rely upon the YRBSS 2015 data and the documentation for the data-set can be found here. Read the documentation carefully, in particular, the details of the survey questions. Then answer the following questions: Is there an association between drinking and driving (Q11) and texting and driving (Q12)? How strong is the relationship between these two variables? Problem 7 The General Social Survey (GSS) gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes, behaviors, and attributes. Hundreds of trends have been tracked since 1972. In addition, since the GSS adopted questions from earlier surveys, trends can be followed for up to 70 years. The GSS contains a standard core of demographic, behavioral, and attitudinal questions, plus topics of special interest. Among the topics covered are civil liberties, crime and violence, intergroup tolerance, morality, national spending priorities, psychological well-being, social mobility, and stress and traumatic events. Altogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. Using these 2016 GSS data, test (a) whether educational attainment (coldeg1) is related to confidence in the scientific community (consci), and (b) the strength of this relationship. Problem 8 In 1984, the Centers for Disease Control and Prevention (CDC) initiated the state-based Behavioral Risk Factor Surveillance System (BRFSS) –- a cross-sectional telephone survey that state health departments conduct monthly over landline telephones and cellular telephones with a standardized questionnaire and technical and methodologic assistance from CDC. BRFSS is used to collect prevalence data among adult U.S. residents regarding their risk behaviors and preventive health practices that can affect their health status. Respondent data are forwarded to CDC to be aggregated for each state, returned with standard tabulations, and published at year’s end by each state. In 2011, more than 500,000 interviews were conducted in the states, the District of Columbia, and participating U.S. territories and other geographic areas. The 2016 BRFSS data-set for Ohio is available here and the accopmpanying codebook is here. Using these data, answer the following questions: Test for a possible relationship between level of education completed (v_educag) and the four categories of body mass index (v_bmi5ca). How strong is this relatinship? Test for a possible relationship between income (v_incomg) and the four categories of body mass index (v_bmi5ca). How strong is this relatinship? Use an appropriate chart to reflect the relationships you tested in (a) and (b). "],["linreg.html", "Chapter 11 Linear Regression 11.1 Correlations 11.2 Bivariate Regression 11.3 Multiple Regression 11.4 Assumptions of Linear Regression 11.5 Chapter 11 Practice Problems", " Chapter 11 Linear Regression In this chapter we close the book with an introduction to regression models – the cornerstone of almost all statistical techniques you see being used in this age of “big data”, “health analytics” and “predictive modeling”. The goal of regression models is to help us understand what variables have a statistically significant impact on some outcome of interest, and how well the regression model we have built can predict outcomes in the future. Imagine, if you will, that you could understand and predict major hurricanes that have hit the United States. If you could accomplish this predictive feat, why then we would know when a hurricane will hit and where, the path it will take, likely damage, the recovery costs, and more. Or perhaps it is a question of delivering better health care and if we can now predict how a patient will respond to total hip or knee surgery, what factors help or hinder speedier recovery, and so on, then patients’ lives will be improved even as health care delivery costs plummet. Indeed, regression models have been used for well over a century now and are employed daily in academia and in the real-world. 11.1 Correlations The ability to understand how some outcome varies with another event is not rocket science to any of us, even those who have never taken a statistics class. We all understand when two things are correlated because when we see broken windows in a neighborhood we suspect that it is an unsafe neighborhood. Or if we see clouds when we wake up we expect some rainfall before the day is done. What are we doing? Why, recognizing that there seems to be a pattern in that when we see \\(x\\) we often see \\(y\\). Now, just because two things seem to occur at the same time does not mean that one causes another, to claim as such would be disregarding the long-standing adage that correlation is not causation. That is, just because two things are correlated does not mean one causes the other. Look hard enough and you will find ridiculous correlations, a feature of our world that has led to what started as a hilarious website and is now a book: Spurious Correlations. As I write this chapter, the leading spurious correlation Tyler Vigen is displaying is of an almost perfect correlation between U.S. spending on science, technology and space and the number of suicides by hanging, strangulation and suffocation \\((r = 0.9978)\\). FIGURE 11.1: A Ridiculous Spurious Correlation So one thing rapidly becomes crystal clear: Correlations do not explain anything, they just reflect a pattern in the data. It is up to us to come up with a plausible explanation for why two things covary. As we allow this point to sink in, we should also recognize that not all correlations are statistically significant. Simply put, we may see two things correlated at \\(0.5\\) but that does not mean the correlation is statistically different from \\(0\\). Let us appreciate these statements in some detail. 11.1.1 Pearson Correlation Coefficient Given two numeric variables, the degree to which they are correlated is measured via the Pearson Product-Moment Correlation Coefficient, denoted by the symbol \\(r\\). Mathematically, the Pearson correlation coefficient is calculated as \\[r = \\dfrac{\\sum \\left( x_i - \\bar{x} \\right) \\left(y_i - \\bar{y} \\right)}{\\sqrt{ \\sum \\left( x_i - \\bar{x} \\right)^2} \\sqrt{ \\sum \\left( y_i - \\bar{y} \\right)^2 }}\\] Note that \\(\\bar{x}\\) is the mean of \\(x\\) and \\(\\bar{y}\\) is the mean of \\(y\\). So the numerator is just the sum of the product of the deviations of \\(x_i\\) from its mean and of \\(y_i\\) from its mean. The denominator is just the product of the square root of the sum of squared deviations of \\(x\\) and \\(y\\) from their respective means. Assume that \\(x = y\\), i.e., the two variables are mirror images of each other. In that case, \\(r\\) will be \\[\\begin{array}{l} r = \\dfrac{\\sum \\left( x_i - \\bar{x} \\right) \\left(x_i - \\bar{x} \\right)}{\\sqrt{ \\sum \\left( x_i - \\bar{x} \\right)^2} \\sqrt{ \\sum \\left( x_i - \\bar{x} \\right)^2 }} \\\\ = \\dfrac{\\sum {\\left(x_i - \\bar{x} \\right)^2}}{\\sqrt{ \\sum \\left( x_i - \\bar{x} \\right)^2} \\sqrt{ \\sum \\left( x_i - \\bar{x} \\right)^2 }} \\\\ = \\dfrac{\\sum {\\left(x_i - \\bar{x} \\right)^2}}{\\sum {\\left(x_i - \\bar{x} \\right)^2}} \\\\ = 1 \\end{array}\\] In other words, if \\(x\\) and \\(y\\) have a one-to-one correspondence, then the two will be perfectly correlated. That is not the whole story since technically speaking the two will be perfectly positively correlated since \\(r = +1\\). If \\(x\\) and \\(y\\) are perfectly but inversely related, then \\(r = -1\\), and we refer to this is a case of a perfect negative correlation. Finally, if the two are not at all correlated, then \\(r = 0\\). In brief, the correlation coefficient will have a specific range: \\(-1 \\leq r \\leq +1\\) and cannot exceed these limits. The graph below shows you stylized correlations by way of a scatter-plot. Notice how these plots show the pattern of the relationship between \\(x\\) and \\(y\\). In (a), we see the cloud of points tilted upwards and to the right; as the variable on the \\(x\\) axis increases, so does the variable on the \\(y\\) axis. In (b), we see the opposite; as the variable on the \\(x\\) axis increases the variable on the \\(y\\) axis decreases. The relationship isn’t that strong here so the pattern of the cloud tilted down and to the right is less obvious than in (c). In (c) it looks like a swarm of bees, all over the place, huddling in the middle, and with no pattern evident. This is no surprise because the two variables have an almost \\(0\\) correlation. But scatter-plots alone don’t tell us whether a correlation is significant; a hypothesis test does. Given that \\(r\\) is based on a sample it is estimating the true correlation between \\(x\\) and \\(y\\) in the population, denoted by \\(\\rho\\). One then needs to conduct a hypothesis test that will tell us whether in the population \\(\\rho=0\\) or \\(\\rho \\neq 0\\). \\[\\begin{array}{l} H_0: \\rho=0 \\\\ H_A: \\rho \\neq 0 \\end{array}\\] The test statistic is \\(t = \\dfrac{r}{SE_r}\\); where \\(SE_r = \\sqrt{\\dfrac{1-r^2}{n-2}}\\) and as usual we reject \\(H_0\\) if \\(p-value \\leq \\alpha\\). We can also calculate asymptotic approximate confidence intervals for \\(\\rho\\) as \\[z - 1.96\\sigma_z &lt; \\zeta &lt; z + 1.96\\sigma_z \\text{ where } z=0.5ln\\left( \\dfrac{1+r}{1-r} \\right)\\] where \\(\\sigma_z = \\sqrt{\\dfrac{1}{n-3}}\\) and \\(\\zeta\\) is the population analogue of the \\(z\\) used to calculate confidence intervals. Because the \\(z\\) involves the natural logarithm we back-transform via taking the antilog of the lower and upper bounds of the confidence interval. Let us see scatter-plots and correlations in action with some real-world data to get a feel for patterns we might encounter. Example 1 The Robert Wood Johnson Foundation’s County Health Rankings measure the health of nearly all counties in the nation and rank them within states. The Rankings are compiled using county-level measures from a variety of national and state data sources. The rankings employ a number of measures but those of interest and in use below are: (a) Premature Death (Years of potential life lost before age 75 per 100,000 population), (b) Adult Obesity (Percentage of adults that report a BMI of 30 or more), (c) Physical Inactivity (Percentage of adults aged 20 and over reporting no leisure-time physical activity ), (d) Uninsured (Percentage of population under age 65 without health insurance), (e) High School Graduation (Percentage of ninth-grade cohort that graduates in four years), and (f) Unemployment (Percentage of population ages 16 and older unemployed but seeking work). If we look at all unique pairs of these variables, what should we expect? Well, I would expect premature death to be positively correlated with obesity, physical inactivity, uninsured, unemployment, and negatively correlated with high school graduation (under the belief that if you have \\(\\geq\\) high school education you are more likely to be employed, insured, etc). FIGURE 11.2: Correlations in the County Health Rankings I’ll setup a single pair of hypotheses so that you get the basic idea. \\[\\begin{array}{l} H_0: \\text{ Adult Obesity and Premature Death are not correlated } (\\rho = 0) \\\\ H_1: \\text{ Adult Obesity and Premature Death ARE correlated } (\\rho \\neq 0) \\end{array}\\] As usual, we reject the null hypothesis if \\(p-value \\leq \\alpha\\) In (a) and (b) we see a positive relationship with premature death increasing as adult obesity increases and uninsured rates increase, respectively. In (c), however, we see a negative relationship with premature death decreasing as high school graduation rates increase. The estimated correlation coefficients and \\(p-values\\) are: (a) \\(r = 0.4966, p-value &lt; 2.2e-16\\), (b) \\(r = 0.3784, p-value &lt; 2.2e-16\\), and (c) \\(-0.1477, p-value = 1.055e-14\\), with 95% confidence intervals of \\(0.4697, 0.5226\\), \\(0.3480, 0.4081\\), and \\(-0.1843, -0.1107\\), respectively. Example 2 Are care insurance premiums correlated with the percentage of drivers who were involved in fatal collisions and were speeding? \\[\\begin{array}{l} H_0: \\text{ Insurance premiums are not correlated with the percentage of drivers who were involved in fatal collisions and were speeding } (\\rho = 0) \\\\ H_1: \\text{ Insurance premiums ARE correlated with the percentage of drivers who were involved in fatal collisions and were speeding } (\\rho \\neq 0) \\end{array}\\] FIGURE 11.3: Correlation between fatal collisions involving speeding and insurance premiums The estimate correlation coefficient is \\(r=0.0425\\), the \\(p-value = 0.7669\\), and the 95% confidence interval is \\((-0.2358, 0.3144\\)). Quite clearly we cannot reject the null; there is no correlation between the two metrics. You may have been surprised by these estimates since the plot may have lead you to expect a positive correlation. Well, a picture (and our a priori expectations) can always be at odds with the truth. Stereotypes, anyone?? 11.1.1.1 Assumptions of the Pearson Correlation Coefficient The Correlation Coefficient is based on the assumption of bivariate normality: That \\(x\\) and \\(y\\) are each normally distributed – this assumption can be tested via the usual approaches to testing for normality. For our purposes, however, we will assume that normality holds. That \\(x\\) and \\(y\\) are linearly related – the Pearson correlation coefficient applies to linear relationships so if \\(x\\) and \\(y\\) are non-linearly related the Pearson correlation coefficient should not be used. That the cloud of points characterized by pairs of \\(x\\) and \\(y\\) has a circular or elliptical shape – this assumption can be visually checked. If these assumptions are violated, we have some fallback solutions but those go beyond the purview of this course. We also will not worry about testing these assumptions for now. 11.1.2 Spearman Correlation Coefficient While the Pearson correlation coefficient is designed for numeric variables, what if the measures are ordinal, such as states ranked by how healthy their populations are, counties ranked by the number of opioid deaths, or students ranked on the basis of their cumulative GPAs? Well, in these instances, where we have ordinal data, the Spearman correlation coefficient comes into use. Technically, the Spearman correlation coefficient measures the strength and association between the ranks of two variables assumed to be (i) randomly sampled, and (ii) with linearly related ranks. Again, we will not test these assumptions (since (i) is assumed to be true and testing (ii) is beyond our current scope). The mechanics are fairly simple: Rank the scores of each variable separately, from low to high Average the ranks in the presence of ties Calculate \\(r_s=\\dfrac{\\sum \\left(R - \\bar{R} \\right)\\left( S - \\bar{S}\\right) }{\\sum \\left(R - \\bar{R} \\right)^2 \\sum \\left(S - \\bar{S} \\right)^2}\\) \\(H_0\\): \\(\\rho_s = 0\\); \\(H_1\\): \\(\\rho_s \\neq 0\\) Set \\(\\alpha\\) Reject \\(H_0\\) if \\(P-value \\leq \\alpha\\); Do not reject \\(H_0\\) otherwise Let us see a particularly interesting example. 11.1.2.1 Example 1 How reliable are witness accounts of “miracles”? One means of testing this is by comparing different accounts of extraordinary magic tricks. Of the many illusions performed by magicians, none is more renowned than the Indian rope trick. In brief, a magician tosses the end of a rope into the air and the rope forms a rigid pole. A boy climbs up the rope and disappears at the top. The magicians scolds the boy and asks him to return but with no response, and so climbs the rope himself, with a knife in hand, and does not return. The boy’s body falls in pieces from the sky into a basket on the ground. The magician then drops back to the ground and retrieves the boy from the basket, revealing him to be unharmed and in one piece. Researchers tracked down 21 first-hand accounts and scored each narrative according to how impressive it was, on a scale of 1 to 5. The researchers also recorded the number of years that had lapsed between the date that the trick was witnessed and the date when the memory of the trick being performed was written down. Is there any association between the impressiveness of eyewitness accounts and the time lapsed since the account was penned? FIGURE 11.4: The Indian rope trick The figure is hard to read but it looks as if the more the years that have lapsed the higher the impressiveness score. The estimated correlation is \\(0.7843\\) and has a \\(p-value = 2.571e-05\\). The null hypothesis, of no correlation, is soundly rejected. 11.1.2.2 Example 2 You may have heard about the PISA Report every now and then when there are moans and groans about how the United States is performing academically compared to other Organization for Economic Cooperation and Development (OECD) nations. The report ranks countries in terms of students’ academic achievements and while it has its detractors (largely because of how the data are generated), PISA remains one of those obelisks policymakers and politicians can’t help but stare at even if they wished it would crumble into a mound of stand before next Monday. Well, if we look at how countries rank in terms of reading scores versus mathematics scores, what do we see? Are country ranks on these subjects correlated? Here is the scatter-plot: FIGURE 11.5: PISA ranks on Reading and Mathematics \\[\\begin{array}{l} H_0: \\text{ Countries&#39; ranks on reading and mathematics are not correlated } (\\rho_s = 0) \\\\ H_1: \\text{ Countries&#39; ranks on reading and mathematics ARE correlated } (\\rho_s \\neq 0) \\end{array}\\] Quite clearly, countries’ ranks on reading and mathematics are highly correlated. The estimated correlation is \\(r_s = 0.9374\\) with a \\(p-value &lt; 2.2e-16\\), allowing us to easily reject the null hypothesis of no correlation. 11.2 Bivariate Regression Do tall parents beget tall children? That was the question Sir Francis Galton grappled with questions of heredity, asking, among other things, whether parents pass on their traits to their offspring. What began as an inquiry with seeds turned into a study of the heights of children. What Galton realized was that regardless of parents’ average heights, children tended towards the average height of children of parents with a given height. In his own words, “It appeared from these experiments that the offspring did not tend to resemble their parents in size, but always to be more mediocre than they – to be smaller than the parents, if the parents were large; to be larger than the parents, if the parents were small.”. This finding, since enshrined as the concept of regression to the mean, is the foundation of what we call regression analysis. We can explore Sir Galton’s point by examining the very data he worked with. FIGURE 11.6: Sir Francis Galton’s data: Heights of parents and children The scatter-plot emphasizes a few features of the data. First, there does seem to be an upward tilt to the cloud of data points, suggesting that on average as parents’ mid-height increases, so does the children’s height. In fact, the correlation is \\(r=0.4587\\) with \\(p-value &lt; 2.2e-16\\), and so we can be confident that the two are indeed significantly and positively correlated. Second, every mid-height of the parents has children of multiple heights, rendering quite clearly the fact that there isn’t a \\(1:1\\) relationship between parents’ average heights and children’s heights. Sometimes children are taller and other times they are shorter than their parents. TABLE 11.1: Mean Child height per Parent height Parent’s height Mean Child height 64.0 65.3 64.5 65.4 65.5 66.7 66.5 67.1 67.5 67.6 68.5 68.0 69.5 68.7 70.5 69.6 71.5 70.1 72.5 71.9 73.0 73.0 What is the average height of the children for each parent mid-height? This is easily calculated, and has been tabulated for you below. The key point of this table is to illustrate the increasing average height of the child per increase in the parent mid-height. So when we encounter parents who have an average height of say 64 inches our best bet of their child’s height would be 65.3 inches, and so on. These mean heights of the children are plotted for you in the figure below. Notice that the line connecting these means is not a straight line. FIGURE 11.7: Sir Francis Galton’s data: Heights of parents and children What does any of this have to do with regression analysis? Everything. How? Because regression analysis revolves around trying to fit a straight line through a cloud of points representing pairs of values of two variables \\(x\\) and \\(y\\). The variable \\(y\\) is what we will call our dependent variable and will be numeric and continuous while \\(x\\) is out independent variable and can be either numeric or categorical. To aid our grasp of regression analysis we will restrict \\(x\\) to be a numeric variable for the moment but relax this requirement down the road. How could we fit a straight line through Galton’s data? By recalling the equation for a straight line: \\(y = m(x) + c\\) where \\(m\\) is the slope of the line and \\(c\\) is in the intercept. Given two sets of coordinates \\((x_1,y_1), (x_2,y_2)\\), say \\((-1,-1)\\) and \\((5, 1)\\) respectively, we can calculate the slope of the straight line connecting these two points as \\(b = \\dfrac{y_2 - y_1}{x_2 - x_1} = \\dfrac{1 - (-1)}{5 - (-1)} = \\dfrac{2}{6}=0.333\\). This works for more than two data points as well, as shown below with a simple data-set. TABLE 11.2: A small data-set for regression x y -1 -1.000 0 -0.667 1 -0.333 2 0.000 3 0.333 4 0.667 5 1.000 Given these data, we can calculate the intercept \\((a)\\) by finding the value of \\(y\\) when \\(x=0\\). So one set of coordinates we want are \\((0, y_0)\\), and a second set picked arbitrarily could be \\((5,1)\\). Note that \\(y_0\\) is the unknown value of the intercept \\(a\\). Now, the slope is calculated for these pairs of points as \\[b = \\dfrac{1 - y_0}{5 - 0}=\\dfrac{1-y_0}{5}\\] Since we calculated the slope before we know the slope is 0.333, i.e., \\(b=0.333\\). Substitute the value of the slope in the equation above and solve for the intercept \\(y_0\\). \\[\\begin{array}{l} b = \\dfrac{1-y_0}{5} \\\\ 0.333 = \\dfrac{1-y_0}{5} \\\\ \\therefore 0.333 \\times 5 = 1 - y_0 \\\\ \\therefore (0.333 \\times 5) - 1 = - y_0 \\\\ \\therefore 1.665 - 1 = - y_0 \\\\ \\text{multiply both sides of the equation to get rid of } -1 \\text{ and rearrange}\\\\ \\therefore y_0 = 1 - 1.665 = -0.665 \\end{array}\\] We have both our slope and the intercept so we can write the equation for the straight line connecting the pairs of \\((x,y)\\) points as: \\(y = m(x) + c = 0.333(x) - 0.665\\). Given this equation we can find the value of \\(y\\) for any given value of \\(x\\). When \\(x=0\\) \\(y = 0.333(0) - 0.665 = -0.665\\) When \\(x=1\\) \\(y = 0.333(1) - 0.665 = -0.332\\) When \\(x=2\\) \\(y = 0.333(2) - 0.665 = 0.001\\) When \\(x=3\\) \\(y = 0.333(3) - 0.665 = 0.334\\) When \\(x=4\\) \\(y = 0.333(4) - 0.665 = 0.667\\) When \\(x=5\\) \\(y = 0.333(5) - 0.665 = 1.000\\) As \\(x\\) increases by \\(1\\), \\(y\\) increases by \\(0.333\\). That is the very definition for the slope of a straight line: The slope indicates how much \\(y\\) changes by for a unit change in \\(x\\). Unit change, in our case, will be treated as an increase of exactly \\(1\\), so bear that in mind. Let us now plot these data points and draw the straight line we calculated through the cloud of points. FIGURE 11.8: The regression line and cloud of points In this case we have a single value of \\(y\\) for a single value of \\(x\\), unlike the Galton data where for each value of \\(x\\) (parents’ average height) we had children of different heights \\((y)\\). What would the straight line look like in that case? Let us redraw the plot of parents’ and children’s heights with the straight line superimposed on the cloud of points. FIGURE 11.9: Sir Francis Galton’s data: Heights of parents and children Given the multiple \\(y\\) values for each \\(x\\) value we knew the straight line could not touch every point. Instead, it runs through the middle of the range of \\(x\\) values for each \\(y\\), except for the maximum value of \\(x\\) where both points are well above the line. How close does this line come to the average of children’s heights calculated for each parent mid-height? Quite well, it turns out, except for the two highest values of parents’ mid-heights. FIGURE 11.10: Sir Francis Galton’s data: Heights of parents and children This demonstrates an important principle of the regression line: It will try to fit itself such that it minimizes the distance between itself and all the data points. Once it is fit, there is no way to rotate it up or down without making at least one data point farther away from the resulting line than was the case before. Hence the regression line is often also referred to as the line of best fit and the method of obtaining the estimates of the regression line as the method of ordinary least squares (OLS). What if we used this line to predict a child’s height? Well, your prediction would be close to reality but the child could be taller or shorter than your predicted height. This drift we refer to as errors, and because these errors will occur, no matter how much we try to minimize them, we start writing the equation for the regression line as \\(y = \\alpha + \\beta(x) + \\epsilon\\) where \\(\\alpha = intercept\\), \\(\\beta = slope\\) and \\(\\epsilon = errors\\). In Galton’s data, the slope is estimated to be \\(0.6463\\) and the intercept is \\(23.9415\\), rendering the equation to be \\[\\begin{array}{l} y = 23.9415 + 0.6463(x) \\\\ i.e., \\text{ child&#39;s height } = 23.9415 + 0.6463(\\text{ parents&#39; mid-height }) \\end{array}\\] The errors \\((\\epsilon)\\) represent the drift between the actual \\(y_i\\) value and the predicted value of y that is denoted as \\(\\hat{y}_i\\), i.e., \\(\\epsilon_i = y_i - \\hat{y}_i\\). These errors reflect the fact that our regression line is either underestimating or overestimating actual \\(y\\) values and so the amount of overestimation/underestimation is called the residuals – something left over by the regression line. For the estimated regression line, here are the \\(\\hat{y}_i\\) (the predicted values of \\((y_i)\\) and the residuals \\((\\hat{e}_i)\\) for a snippet of the data. TABLE 11.3: Predicted values and Residuals Parent’s mid-height Child’s height Predicted Child’s height Residual 64.0 61.7 65.3 -3.6 64.0 63.2 65.3 -2.1 64.0 63.2 65.3 -2.1 64.0 64.2 65.3 -1.1 64.0 64.2 65.3 -1.1 64.0 64.2 65.3 -1.1 64.0 64.2 65.3 -1.1 64.0 65.2 65.3 -0.1 64.0 66.2 65.3 0.9 64.0 66.2 65.3 0.9 64.0 67.2 65.3 1.9 64.0 67.2 65.3 1.9 64.0 68.2 65.3 2.9 64.0 69.2 65.3 3.9 64.5 61.7 65.6 -3.9 64.5 62.2 65.6 -3.4 64.5 63.2 65.6 -2.4 64.5 63.2 65.6 -2.4 64.5 63.2 65.6 -2.4 64.5 63.2 65.6 -2.4 64.5 64.2 65.6 -1.4 64.5 64.2 65.6 -1.4 64.5 64.2 65.6 -1.4 64.5 64.2 65.6 -1.4 64.5 65.2 65.6 -0.4 64.5 66.2 65.6 0.6 64.5 66.2 65.6 0.6 64.5 66.2 65.6 0.6 64.5 66.2 65.6 0.6 64.5 66.2 65.6 0.6 When \\(x = 64.0, \\hat{y} = 65.3, y = 61.7\\) and hence \\(\\epsilon = 61.7 - 65.3 = -3.6\\) When \\(x = 64.0, \\hat{y} = 65.3, y = 63.2\\) and hence \\(\\epsilon = 63.2 - 65.3 = -2.1\\) \\(\\ldots\\) When \\(x = 64.0, \\hat{y} = 65.3, y = 65.2\\) and hence \\(\\epsilon = 65.2 - 65.3 = -0.1\\) When \\(x = 64.0, \\hat{y} = 65.3, y = 66.2\\) and hence \\(\\epsilon = 66.2 - 65.3 = 0.9\\) 8 \\(\\ldots\\) The residual is the smallest when predicted \\(y\\) and actual \\(y\\) are close. If they were the same, the residual would be \\(0\\). Now, what is the mean of \\(y\\) when \\(x = 64.0\\)? This was \\(65.3\\). So the predicted value of \\(y | x = 64.0\\) is being calculated as the average height of all children born to parents with mid-heights of \\(64.0\\) inches. This why the predicted value of a child’s height is always \\(65.3\\) for these parents. In this sense, a predicted value from a regression is really a conditional mean prediction, conditional, that is, on the value of \\(x\\). If you average all the residuals, they will be zero, i.e., \\(\\bar{\\epsilon = 0}\\) and this is always true for any regression line. Before we move on, note that the variance of the residuals is calculated as \\[\\begin{array}{l} var(e_i) = \\dfrac{\\sum\\left(e_i - \\bar{e}\\right)^2}{n-2} \\\\ = \\dfrac{\\sum\\left(e_i - 0\\right)^2}{n-2} \\\\ = \\dfrac{\\sum\\left(e_i\\right)^2}{n-2} \\\\ = \\dfrac{\\sum\\left(y_i - \\hat{y}_i\\right)^2}{n-2} \\end{array}\\] This is the \\(\\dfrac{\\text{ Sum of Squares of the Residuals}}{n-2} = \\text{Mean of the Sum of Squares of the Residuals} = MS_{residual}\\). This is the average prediction error in squared units so if we take the square-root, we get the average prediction error \\(RMSE = \\sqrt{\\text{Mean of the Sum of Squares of the Residuals}}\\). The smaller is this value, the better is the regression line fitting the data. 11.2.1 The Method of Ordinary Least Squares The method of ordinary least squares estimates the slope and the intercept by looking to minimize the sum of squared errors (SSE), i.e., \\(\\sum(e_i)^2 = \\sum\\left(y_i - \\hat{y}_i\\right)^2\\) with the slope estimated as \\(\\hat{\\beta} = \\dfrac{\\sum\\left(x_i - \\bar{x}\\right)\\left(y_i - \\bar{y}\\right)}{\\sum\\left(x_i - \\bar{x}\\right)^2}\\). Once \\(\\hat{\\beta}\\) is estimated, the intercept can be calculated via \\(\\hat{\\alpha} = \\bar{y} - \\hat{\\beta}\\left(\\bar{x}\\right)\\). The standard error of the slope is \\(s.e._{\\hat{\\beta}} = \\sqrt{ \\dfrac{\\text{Mean of the Sum of Squares of the Residuals}}{\\sum\\left(x_i - \\bar{x}\\right)^2} }\\). 11.2.2 Population Regression Function vs. Sample Regression Function Now, assume that the Galton data represent the population and instead of working with the population data you have a random sample to work with. Below I have drawn four random samples, each with \\(100\\) data points randomly selected from the Galton data-set, estimated the regression line and then plotted the cloud of points plus the regression line. What do you see as you scan the four plots? FIGURE 11.11: Four random samples from Galton’s data Note that each sample yields different estimates as compared to the population regression line where the regression equation was \\(23.94 + 0.64(x)\\). This is to be expected since a sample will approximate the population but rarely be identical. Because we are dealing with a sample, we distinguish between the population regression line and the sample regression line by writing the latter as \\(y = a + b(x) + e\\). 11.2.3 Hypotheses Of course, we need to assess both the statistical significance of our sample regression line, and how well the line fits the data. Let us look at the hypothesis tests first. We have estimated two quantities – the intercept and the slope – and hence will have two hypothesis tests. \\[\\begin{array}{l} H_0: \\alpha = 0 \\\\ H_1: \\alpha \\neq 0 \\end{array}\\] and \\[\\begin{array}{l} H_0: \\beta = 0 \\\\ H_1: \\beta \\neq 0 \\end{array}\\] The test statistic for each relies on the standard errors of \\(a\\) and \\(b\\). In particular, the test statistic for the slope is \\(t_{\\hat{b}} = \\dfrac{\\hat{b}}{s.e._{\\hat{b}}}\\) and if the \\(p-value \\leq \\alpha\\) we reject the null that \\(\\beta = 0\\). Likewise, for the intercept we have \\(s.e._{\\hat{a}} = \\sqrt{MS_{residual}}\\left(\\sqrt{\\dfrac{\\sum x_i^2}{n\\sum\\left(x_i - \\bar{x}\\right)^2}} \\right)\\), leading to \\(t_{\\hat{a}} = \\dfrac{\\hat{a}}{s.e._{\\hat{a}}}\\). We can also estimate the confidence interval estimates for \\(\\hat{a}\\) and \\(\\hat{b}\\) to get a sense of the intervals within which \\(\\alpha\\) and \\(\\beta\\) are likely to fall (in the population). 11.2.4 The \\(R^2\\) In addition to the RMSE, the average prediction error, we also rely on the \\(R^2\\), which tells us what percent of the variation in \\(y\\) can be explained by the regression model. How is this calculated? Well, we already calculated the sum of squares for the residuals (SSE), which was \\(\\sum \\left(y_i - \\bar{y}\\right)^2\\). Now, we do know that the total variation in \\(y\\) can be calculated as \\(\\sum\\left(y_i - \\bar{y}\\right)^2\\), so let us label this SST. The total variation in \\(y\\) has to be made up of the SSE and the amount of variation being captured by the regression. Let us label this latter quantity SSR, and note that it can be calculated as \\(SSR = \\sum\\left(\\hat{y}_i - \\bar{y}\\right)^2\\). Since \\(SST = SSR + SSE\\), we can assess what proportion of variance in \\(y\\) is explained by the regression by calculating \\(\\dfrac{SSR}{SST} \\ldots\\) which is the \\(R^2\\). If the regression model is terrible, \\(R^2 \\to 0\\) and if the regression model is excellent, \\(R^2 \\to 1\\). That is, \\(0 \\leq R^2 \\leq 1\\) … the \\(R^2\\) will lie in the \\(\\left[0, 1\\right]\\) interval. In practice, we adjust the \\(R^2\\) for the number of independent variables used and rely on this adjusted \\(R^2\\), denoted as \\(\\bar{R}^2\\), and calculated as \\(\\bar{R}^2 = 1 - \\left(1 - R^2\\right)\\left(\\dfrac{n-1}{n-k-1}\\right)\\) where \\(k =\\) the number of independent variables being used. 11.2.5 Confidence Intervals vs. Prediction Intervals Technically speaking, \\(\\hat{y}_{i}\\) is a point estimate for \\(y_{i}\\) because it is a single estimate and tells us nothing about the interval within which the predicted value might fall. Confidence intervals and Prediction intervals, however, do, but they are not the same thing. Specifically, while the confidence interval is an interval estimate of the mean value of y for a specific value of \\(x\\), the prediction interval is an interval estimate of the predicted value of y for a specific value of \\(x\\). Given \\(x_{p} =\\) specific value of \\(x\\) and \\(y_{p} =\\) specific value of \\(y\\) for \\(x = x_{p}\\), \\(E(y_{p}) =\\) expected value of \\(y\\) given \\(x = x_{p}\\) is defined as \\(\\hat{y}_{p} = \\hat{a} + \\hat{b}(x_{p})\\). The variance is, in turn, given by var(\\(\\hat{y}_{p}\\)) \\(= s^{2}_{\\hat{y}_{p}} = s^{2}\\left[\\dfrac{1}{n} + \\dfrac{(x_{p} - \\bar{x})^{2}}{\\sum(x_{i} - \\bar{x})^{2}}\\right]\\), which leads to s(\\(\\hat{y}_{p}\\)) \\(= s_{\\hat{y}_{p}}=s\\sqrt{\\left[\\dfrac{1}{n} + \\dfrac{(x_{p} - \\bar{x})^{2}}{\\sum(x_{i} - \\bar{x})^{2}} \\right]}\\) Now, the confidence interval is given by \\(\\hat{y}_{p} \\pm t_{\\alpha/2}\\left(s_{\\hat{y}_p}\\right)\\). Every time we calculate this interval we know we could be wrong on two counts – \\(b_{0}; b_{1}\\). Fair enough, since both have been estimated from the data and could be wrong. However, if we want to predict \\(y\\) for some \\(x\\) value not in the sample, we know here we could be wrong on three counts – \\(b_{0}; b_{1}; \\text{ and } e\\). This forces an adjustment in the variance calculated earlier to now be defined as \\(s^{2}_{ind} = s^{2} + s^{2}_{\\hat{y}_{p}}\\), where \\(s^{2}_{ind} = s^{2} + s^{2}\\left[\\dfrac{1}{n} + \\dfrac{(x_{p} - \\bar{x})^{2}}{\\sum(x_{i} - \\bar{x})^{2}}\\right] = s^{2} \\left[1 + \\dfrac{1}{n} + \\dfrac{(x_{p} - \\bar{x})^{2}}{\\sum(x_{i} - \\bar{x})^{2}}\\right]\\) and hence \\(s_{ind} = s\\sqrt{\\left[1 + \\dfrac{1}{n} + \\dfrac{(x_{p} - \\bar{x})^{2}}{\\sum(x_{i} - \\bar{x})^{2}}\\right]}\\). The prediction interval is then calculated as \\(\\hat{y}_{p} \\pm t_{\\alpha/2}\\left(s_{ind}\\right)\\) With the Galton data, this is what the scatter-plot would look like with the confidence intervals and predictions intervals added to the regression line. Focus on these intervals. FIGURE 11.12: Confidence vs. Prediction Intervals Notice how much wider the prediction intervals are (the red dashed lines); this is because we have made an adjustment for the additional uncertainty surrounding an individual prediction. Notice also that the confidence intervals flex inward, toward the regression line, in the center of the distribution but then flew outward at either extreme of the regression line. This is a result of the fact that the regression estimates are built around \\(\\bar{x}\\) and \\(\\bar{y}\\) and hence the greatest precision we have is when we are predicting \\(y\\) for \\(\\bar{x}\\), i.e., when predicting \\(\\hat{y}_{\\bar{x}} = \\hat{a} + \\hat{b}\\left(\\bar{x}\\right)\\). To sum up our discussion of interval estimates, remember that the confidence interval is in use when predicting the average value of \\(y\\) for a specific value of \\(x\\). However, when we are predicting a specific value of \\(y\\) for a given \\(x\\), then the prediction interval comes into use. Further, confidence intervals will hug the regression line most closely around \\(\\bar{x}\\), and widen as you progressively move towards \\(x_{min}\\) and \\(x_{max}\\). 11.2.5.1 Example 1   child Predictors Estimates CI p (Intercept) 30.50 13.42 – 47.59 0.001 parent 0.55 0.30 – 0.80 &lt;0.001 Observations 100 R2 / R2 adjusted 0.161 / 0.153 With a random sample drawn from the Galton data-set, can we claim that parents’ mid-heights predict the child’s height? If they do, how good is the fit of the model? It turns out that the intercept was estimated as \\(\\hat{a} = 21.1143\\) and has a \\(p-value = 0.00921\\) while the slope was estimated as \\(0.6913\\) and has a \\(p-value = 4.49e-08\\). So both the intercept and the slope are statistically significant. The \\(RMSE = 2.216\\) and the \\(\\bar{R}^2 = 0.2568\\). When the parents’ mid-height is \\(=0\\) the child’s height is predicted to be \\(21.11\\) inches As parents’ mid-height increases by \\(1\\), the child’s height increases by 0.6913 inches If we used this regression model to predict a child’s height, on average our prediction error would be \\(2.216\\) inches, i.e., we would overpredict or underpredict the child’s actual height by \\(\\pm 2.216\\) inches The \\(\\bar{R}^2 = 0.2568\\), indicating that about \\(25.68\\%\\) of the variation in the child’s height can be explained by this regression model (i.e., by using the parents’ mid-height as an independent variable) The 95% confidence interval around \\(\\hat{a}\\) is \\(\\left[5.34, 36.88\\right]\\), indicating that we can be about 95% confident that the population intercept lies in the \\(\\left[5.34, 36.88\\right]\\) range The 95% confidence interval around \\(\\hat{b}\\) is \\(\\left[0.46, 0.92\\right]\\), indicating that we can be about 95% confident that the population slope lies in the \\(\\left[0.46, 0.92\\right]\\) range Note that the intercept will not always make sense from a real-world perspective, i.e., you can’t have parents’ mid-height be \\(0\\) since that would mean there are no parents! However, we retain the intercept for mathematical necessity but accommodate for the often nonsensical meaning of the intercept by not interpreting it all. It is needed for the mathematics to work out but need not be interpreted. Note also that since we are only explaining about \\(25.68\\%\\) of the variation in children’s heights, there is \\(100 - 25.68 = 74.32\\%\\) of the variation left to be explained. Maybe there are other independent variables we could have used that would do a better job of predicting the child’s height as in, for example, nutrition, physical activity, good health care during the child’s early years, and so on. 11.2.5.2 Example 2 Let us revisit the question of premature death and adult obesity. How well can we predict premature death from adult obesity? Since the original data we looked at included all counties in the U.S., I will draw a random sample of 100 counties to fit the regression model.   Premature Death Predictors Estimates CI p (Intercept) 0.94 0.79 – 1.10 &lt;0.001 Adult Obesity 0.00 -0.01 – 0.01 0.626 Observations 99 R2 / R2 adjusted 0.002 / -0.008 The intercept is \\(-28.83\\) but is not statistically significant since the \\(p-value = 0.988\\) The slope is \\(264.78\\) and is statistically significant given the \\(p-value = -1.77e-05\\), indicating that as the percent of obese adults increases by \\(1\\), the number of premature deaths increases by about 265. The \\(\\bar{R}^2 = 0.1652\\), indicating that we can explain about \\(16.52\\%\\) of the variation in premature deaths with this regression model. The \\(RMSE = 2355.516\\), indicating that average prediction error would be \\(\\pm 2355.516\\) if this model were used. Note the high value here, indicative of a sizable prediction error. Being off the truth by \\(2355\\) premature deaths is a lot! Here are the data with the scatter-plot, regression line, and confidence intervals. Note that the regression equation superimposed on the plot has estimates rounded up; hence the marginal difference between these and the estimates interpreted above. Note also that the \\(R^2\\) is being reported, not \\(\\bar{R}^2\\). FIGURE 11.13: Predicting Premature Death from Adult Obesity 11.2.6 Dummy Variables In the examples so far we have worked with numeric variables – parents’ mid-height, adult obesity, and so on. However, what if we had categorical variables and wanted to examine, for example, if premature deaths vary across males and females? If public school district performance differs between Appalachian and non-Appalachian Ohio? Do carbon dioxide \\(CO_2\\) emissions vary between the developed and the developing countries? These types on interesting questions lend themselves to regression analysis rather easily. To see how these models are fit and interpreted, let us work with a specific example. The data I will utilize span the 50 states and Washington DC for the 1977-1999 period, with information on the following variables: state = factor indicating state. year = factor indicating year. violent = violent crime rate (incidents per 100,000 members of the population). murder = murder rate (incidents per 100,000). robbery = robbery rate (incidents per 100,000). prisoners = incarceration rate in the state in the previous year (sentenced prisoners per 100,000 residents; value for the previous year). afam = percent of state population that is African-American, ages 10 to 64. cauc = percent of state population that is Caucasian, ages 10 to 64. male = percent of state population that is male, ages 10 to 29. population = state population, in millions of people. income = real per capita personal income in the state (US dollars). density = population per square mile of land area, divided by 1,000. law = factor. Does the state have a shall carry law in effect in that year? If the value is “yes”, then the state allows individuals to carry a concealed handgun provided they meet certain criteria. To keep things simple let us shrink this data-set to the year 1999. The substantive question of interest is: Do concealed carry permits reduce robberies? \\[\\begin{array}{l} H_0: \\text{ Concealed carry laws have no impact on robbery rates } (\\beta = 0) \\\\ H_1: \\text{ Concealed carry laws have an impact on robbery rates } (\\beta \\neq 0) \\end{array}\\] The regression model that could be used would assume the following structure: \\(\\hat{y} = \\hat{a} + \\hat{b}\\left(law\\right) + \\hat{e}\\). The variable “law” is a categorical variable that assumes two values – \\(x = 1\\) indicates the state allows concealed carry and \\(x = 0\\) indicates that the state does not allow concealed carry. What will the regression equation look like if \\(x=1\\) versus \\(x=0\\)? \\[\\begin{array}{l} \\hat{y} = \\hat{a} + \\hat{b}\\left(x \\right) \\\\ \\text{When } x = 1: \\hat{y} = \\hat{a} + \\hat{b}\\left(1\\right) = \\hat{a} + \\hat{b} \\\\ \\text{When } x = 0: \\hat{y} = \\hat{a} + \\hat{b}\\left(0\\right) = \\hat{a} \\end{array}\\] Aha! The predicted value of \\(y\\) for states with no concealed carry laws is just the estimated intercept while the predicted value of \\(y\\) for states with concealed carry laws is the estimated intercept \\(+\\) the estimated slope. This tells us that difference in robbery rates between states with and without concealed carry laws is just the estimated slope \\(\\hat{b}\\). Now on to visualizing the data and then to fitting the regression model. FIGURE 11.14: Box-plots of Robbery rates by Law   robbery Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 155.09 20.33 0.34 0.21 114.23 – 195.94 -0.07 – 0.75 7.63 &lt;0.001 law [yes] -59.59 26.96 -0.60 0.27 -113.77 – -5.40 -1.15 – -0.05 -2.21 0.032 Observations 51 R2 / R2 adjusted 0.091 / 0.072 The box-plots show an outlier for law = “no” and a higher median robbery rate for the “no” group versus the “yes” group; seemingly, states without concealed carry laws have on average higher robbery rates. The estimated regression line is \\(\\hat{y} = 155.09 -59.59\\left(law\\right)\\). That is, states without a law are predicted to have about \\(114\\) robberies per 100,000 persons. States with a law, on the other hand, are predicted to have almost 114 fewer robberies per 100,000 persons. Note that both the intercept and the slope are statistically significant, the \\(RMSE = 95.36\\) and the \\(\\bar{R}^2 = 0.0720 = 7.20\\%\\). Wait a minute. What about that outlier we see in the box-plot? Couldn’t that be influencing our regression estimates? Let us check by dropping that data point and re-estimating the regression model. FIGURE 11.15: Box-plots of Robbery rates by Law (no outlier)   robbery Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 131.79 13.90 0.32 0.21 103.83 – 159.75 -0.11 – 0.75 9.48 &lt;0.001 law [yes] -36.29 18.26 -0.55 0.28 -73.00 – 0.42 -1.11 – 0.01 -1.99 0.053 Observations 50 R2 / R2 adjusted 0.076 / 0.057 Now, the estimated intercept is \\(131.79\\) and statistically significant. The estimated slope is \\(-36.29\\) and has a \\(p-value = 0.0526\\), and so it is not statistically significant! In brief, without the outlier, average robbery rates per 100,000 persons do not appear to differ between states with/without concealed carry laws. What is the moral of the story? Regression estimates will be influenced by outliers, the more extreme the outlier, the more the influence. The preceding example involved a two-category variable. What if the variable had three or more categories? What would the regression model look like and how would we interpret the results? Assume, for example, that we are interested in looking at the wages of men who had an annual income greater than USD 50 in 1992 and were neither self-employed nor working without pay. The sample spans men aged 18 to 70 and is drawn from the March 1988 Current Population Survey. The data-set includes the following variables: wage = Wage (in dollars per week). education = Number of years of education. experience = Number of years of potential work experience. ethnicity = Factor with levels “cauc” and “afam” (African-American). smsa = Factor. Does the individual reside in a Standard Metropolitan Statistical Area (SMSA)? region = Factor with levels “northeast”, “midwest”, “south”, “west”. parttime = Factor. Does the individual work part-time? The question of interest here will be whether wages differ by Census regions. \\[\\begin{array}{l} H_0: \\text{ Wages do not differ by Census regions } \\\\ H_1: \\text{ Wages do differ by Census regions } \\end{array}\\] The variable “region” is coded as follows: \\(\\text{Northeast}: x=1\\), \\(\\text{Midwest}: x=2\\), \\(\\text{South}: x=3\\), \\(\\text{West}: x=4\\). The usual regression equation would be \\(\\hat{y} = \\hat{a} + \\hat{b}\\left(x\\right)\\). However, with categorical variables representing some attribute that has more than 2 categories we write the regression equation in a different manner. Specifically, we create a dummy variable for each of mutually exclusive categories as follows: \\[\\begin{array}{l} \\text{Northeast}: = 1 \\text{ if in the Northeast }; 0 \\text{ otherwise (i.e., if in the Midwest, South, West)} \\\\ \\text{Midwest}: = 1 \\text{ if in the Midwest }; 0 \\text{ otherwise (i.e., if in the Northeast, South, West)} \\\\ \\text{South}: = 1 \\text{ if in the South }; 0 \\text{ otherwise (i.e., if in the Northeast, Midwest, West)} \\\\ \\text{West}: = 1 \\text{ if in the West }; 0 \\text{ otherwise (i.e., if in the Northeast, Midwest, South)} \\end{array}\\] This leads to the regression equation being \\(\\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(South\\right) + \\hat{b}_4\\left(West\\right)\\) but in estimating the regression model we only use three of the dummy variables. This is a general rule: With a categorical variable that results in \\(k\\) dummy variables only \\(k-1\\) dummy variables should be included in the model unless the intercept is excluded. How do we decide which region to leave out? The rule of thumb is to leave out the modal category (i.e., the category that has the highest frequency). TABLE 11.4: Frequency Table for Census Regions Census Region Frequency northeast 6441 midwest 6863 south 8760 west 6091 In this data-set, the modal category happens to be the South and so we will estimate the model as \\(\\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right)\\). Now we break this down so we know how to interpret the estimates of \\(\\hat{y}\\): \\[\\begin{array}{l} \\text{In the Northeast}: \\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right) = \\hat{a} + \\hat{b}_1\\left(1\\right) + \\hat{b}_2\\left(0\\right) + \\hat{b}_3\\left(0\\right) = \\hat{a} + \\hat{b}_1 \\\\ \\text{In the Midwest}: \\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right) = \\hat{a} + \\hat{b}_1\\left(0\\right) + \\hat{b}_2\\left(1\\right) + \\hat{b}_3\\left(0\\right) = \\hat{a} + \\hat{b}_2 \\\\ \\text{In the West}: \\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right) = \\hat{a} + \\hat{b}_1\\left(0\\right) + \\hat{b}_2\\left(0\\right) + \\hat{b}_3\\left(1\\right) = \\hat{a} + \\hat{b}_3 \\\\ \\text{In the South}: \\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right) = \\hat{a} + \\hat{b}_1\\left(0\\right) + \\hat{b}_2\\left(0\\right) + \\hat{b}_3\\left(0\\right) = \\hat{a} \\end{array}\\] Aha! Since we excluded South the, intercept \\((\\hat{a})\\) reflects the impact of being in the South. Keeping this in mind, let us see how to interpret the rest of the regression estimates shows in the regression equation below:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 558.31 4.83 -0.10 0.01 548.84 – 567.78 -0.12 – -0.08 115.56 &lt;0.001 region [northeast] 95.73 7.42 0.21 0.02 81.18 – 110.28 0.18 – 0.24 12.90 &lt;0.001 region [midwest] 46.37 7.29 0.10 0.02 32.08 – 60.66 0.07 – 0.13 6.36 &lt;0.001 region [west] 56.46 7.54 0.12 0.02 41.68 – 71.25 0.09 – 0.16 7.48 &lt;0.001 Observations 28155 R2 / R2 adjusted 0.006 / 0.006 \\[\\begin{array}{l} \\hat{y} = \\hat{a} + \\hat{b}_1\\left(Northeast\\right) + \\hat{b}_2\\left(Midwest\\right) + \\hat{b}_3\\left(West\\right) \\\\ \\hat{y} = 558.308 + 95.731\\left(Northeast\\right) + 46.371\\left(Midwest\\right) + 56.463\\left(West\\right) \\end{array}\\] Predicted average wage in the Northeast is \\(558.308 + 95.731 = 654.039\\) Predicted average wage in the Midwest is \\(558.308 + 46.371 = 604.679\\) Predicted average wage in the Northeast is \\(558.308 + 56.463 = 614.771\\) Predicted average wage in the South is \\(558.308\\) The \\(p-values\\) are basically \\(0\\) for the intercept and the the three dummy variables and hence these results suggest that region does matter; wages differ by Census region, with those in the Northeast having the highest average wage, followed by the Northeast, then the Midwest, and finally the West. In closing, the \\(RMSE = 452.2\\) and the \\(\\bar{R}^2 = 0.005959 = 0.5959\\%\\). Clearly this is a terrible regression model because we are hardly explaining any of the variation in wages. 11.3 Multiple Regression Thus far we have restricted out attention to a single independent variable, largely because it is easier to understand the logic of regression analysis with a single independent variable. However, few things, if any, can be explained by looking at a single independent variable. Instead, we often have to include multiple independent variables because the phenomenon the dependent variable represents is extremely complex. For example, think about trying to predict stock prices, climate change, election results, students’ academic performance, health outcomes, and so on. None of these are easy phenomenon to comprehend, leave alone capture via a single independent variable. Consequently, we now turn to the more interesting world of multiple regression analysis We will start with at least two independent variables, each of which may be numeric or categorical. For ease of comprehension I’ll stick with the wage data-set. Say we have two independent variables, \\(x_1\\) and \\(x_2\\). Now the regression equation is specified as \\[\\hat{y} = \\hat{a} + \\hat{b}_1\\left( x_1 \\right) + \\hat{b}_2\\left( x_2 \\right)\\] With three independent variables the equation becomes \\[\\hat{y} = \\hat{a} + \\hat{b}_1\\left( x_1 \\right) + \\hat{b}_2\\left( x_2 \\right) + \\hat{b}_3\\left( x_3 \\right)\\] and so on. Now the way we interpret \\(\\hat{a}, \\hat{b}_1, \\hat{b}_2, \\text{ and } \\hat{b}_3\\) changes. Specifically, \\(\\hat{a}\\) is the predicted value of \\(y\\) when all independent variables are \\(=0\\) \\(\\hat{b}_1\\) is the partial slope coefficient on \\(x_1\\) and indicates the change in \\(y\\) for a unit change in \\(x_1\\) holding all other independent variables constant \\(\\hat{b}_2\\) is the partial slope coefficient on \\(x_2\\) and indicates the change in \\(y\\) for a unit change in \\(x_2\\) holding all other independent variables constant \\(\\hat{b}_3\\) is the partial slope coefficient on \\(x_3\\) and indicates the change in \\(y\\) for a unit change in \\(x_3\\) holding all other independent variables constant and so on The interpretation of the \\(RMSE\\) and \\(\\bar{R}^2\\) remains the same. Hypotheses are setup for each independent variable as usual. Generating predicted values requires some work though because you have to set each independent variable to a specific value when generating \\(\\hat{y}\\). We typically start by setting all independent variables to their mean (or median) value (if the independent variable is numeric) and to their modal values (if the independent variable is categorical). Let us anchor our grasp of these in the context of a specific problem. 11.3.1 Two Numeric (i.e., Continuous) Independent Variables Say we are interested in studying the impact of two continuous independent variables, education and experience, on wage, i.e., \\(\\hat{y} = \\hat{a} + \\hat{b}_1\\left( education \\right) + \\hat{b}_2\\left( experience \\right)\\). When you fit this regression model the estimates will be as follows:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) -385.08 13.24 0.00 0.01 -411.04 – -359.13 -0.01 – 0.01 -29.08 &lt;0.001 education 60.90 0.88 0.39 0.01 59.17 – 62.63 0.38 – 0.40 68.98 &lt;0.001 experience 10.61 0.20 0.31 0.01 10.22 – 10.99 0.29 – 0.32 54.19 &lt;0.001 Observations 28155 R2 / R2 adjusted 0.177 / 0.177 The intercept is statistically significant and is \\(\\hat{a} = -385.0834\\), indicating that when both education and experience are \\(=0\\), predicted wage is \\(-385.0834\\). Note that this makes no substantive sense but we are not interested in interpreting the intercept per se. The partial slope on education is statistically significant and is \\(\\hat{b}_1 = 60.8964\\), indicating that holding experience constant, for every one more year of education wage increases by $60.8964. The partial slope on experience is statistically significant and is \\(\\hat{b}_2 = 10.6057\\), indicating that holding education constant, for every one more year of experience wage increases by $10.6057. \\(RMSE = 411.5\\), indicating that average prediction error from this regression model would be \\(\\pm 411.5\\) The \\(\\bar{R}^2 = 0.1768\\) indicating that about \\(17.68\\%\\) of the variation in wage is explained by this regression model. What about predicted values? Let us calculate a few. Predict wage when education is at its mean and experience is at its mean: \\[\\begin{array}{l} = -385.0834 + 60.8964(education) + 10.6057(experience) \\\\ = -385.0834 + 60.8964(13.06787) + 10.6057(18.19993) \\\\ = -385.0834 + 795.7862 + 193.023 = 603.7258 \\end{array}\\] Predict wage when education is at its mean and experience is (i) \\(8\\), and (ii) \\(27\\): \\[\\begin{array}{l} = -385.0834 + 60.8964(13.06787) + 10.6057(8) \\\\ = -385.0834 + 795.7862 + 84.8456 = 495.5484 \\end{array}\\] \\[\\begin{array}{l} = -385.0834 + 60.8964(13.06787) + 10.6057(27) \\\\ = -385.0834 + 795.7862 + 286.3539 = 697.0567 \\end{array}\\] Predict wage when experience is at its mean and education is (i) \\(12\\), and (ii) \\(15\\): \\[\\begin{array}{l} = -385.0834 + 60.8964(12) + 10.6057(18.19993) \\\\ = -385.0834 + 730.7568 + 193.023 = 538.6964 \\end{array}\\] \\[\\begin{array}{l} = -385.0834 + 60.8964(15) + 10.6057(18.19993) \\\\ = -385.0834 + 913.446 + 193.023 = 721.3856 \\end{array}\\] 11.3.2 One Numeric and One Categorical Independent Variable Let us stay with education as the continuous independent variable but add a categorical independent variable – parttime – that tells us whether the individual was working full-time \\((parttime = 0)\\) or not \\((parttime = 1)\\). The regression model is: \\[wage = \\hat{\\alpha} + \\hat{\\beta_1}(education) + \\hat{\\beta_2}(parttime) + e\\]   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 27.99 11.50 0.08 0.01 5.44 – 50.54 0.07 – 0.09 2.43 0.015 education 46.82 0.86 0.30 0.01 45.14 – 48.49 0.29 – 0.31 54.63 &lt;0.001 parttime [yes] -402.05 8.70 -0.89 0.02 -419.10 – -385.00 -0.92 – -0.85 -46.22 &lt;0.001 Observations 28155 R2 / R2 adjusted 0.155 / 0.155 As estimated, the model turns out to be: \\[wage = 27.9930 + 46.8153(education) - 402.0493(parttime)\\] and this regression equation makes clear that the estimate of \\(-402.0493\\) applies to the case of a part-time employee. What would predicted values look like for part-time versus full-time employees, for specific values of education. For a part-time employee, predicted wage is \\(= 27.9930 + 46.8153(education) - 402.0493(1) = -374.0563 + 46.8153(education)\\) For a full-time employee, predicted wage is \\(= 27.9930 + 46.8153(education) - 402.0493(0) = 27.9930 + 46.8153(education)\\) So quite clearly, holding all else constant, on average, part-time employees make \\(402.0493\\) less than full-time employees. For every one more year of education, wage increases by \\(46.8153\\). When \\(education = 0\\) and the employee is a full-time employee, predicted wage is \\(27.9930\\). Now we switch things up by fitting a regression model that predicts wage on the basis of two categorical variables. We have already worked with parttime so let us add the second categorical variable, region. The resulting regression model is: \\[wage = \\hat{\\alpha} + \\hat{\\beta_1}(parttime) + \\hat{\\beta_2}(region) + e\\] When you estimate the model you will see the following results:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 593.92 4.74 -0.02 0.01 584.63 – 603.21 -0.04 – -0.00 125.34 &lt;0.001 parttime [yes] -405.68 9.12 -0.89 0.02 -423.57 – -387.80 -0.93 – -0.86 -44.47 &lt;0.001 region [northeast] 91.11 7.18 0.20 0.02 77.04 – 105.17 0.17 – 0.23 12.70 &lt;0.001 region [midwest] 48.41 7.05 0.11 0.02 34.60 – 62.22 0.08 – 0.14 6.87 &lt;0.001 region [west] 62.54 7.29 0.14 0.02 48.25 – 76.84 0.11 – 0.17 8.58 &lt;0.001 Observations 28155 R2 / R2 adjusted 0.071 / 0.071 that then lead to the following regression equation: \\[wage = 593.921 - 405.684(parttime = yes) + 91.106(region = northeast) + 48.412(region = midwest) + 62.544(region = west)\\] The correct starting point would be to see what the Intercept represents. Since we see a partial slope for parttime = yes the Intercept must be absorbing parttime = no, and the omitted region = south. That is, the Intercept is yielding the predicted wage \\((593.921)\\) when the individual is a full-time employee who lives in the South. Let us spell out this situation and all other possibilities as well. Full-time living in the South: \\(wage = 593.921\\) Full-time living in the Northeast: \\(wage = 593.921 + 91.106 = 685.027\\) Full-time living in the Midwest: \\(wage = 593.921 + 48.412 = 642.333\\) Full-time living in the West: \\(wage = 593.921 + 62.544 = 656.465\\) Part-time living in the South: \\(wage = 593.921 - 405.684 = 188.237\\) Full-time living in the Northeast: \\(wage = 593.921 + 91.106 - 405.684 = 279.343\\) Full-time living in the Midwest: \\(wage = 593.921 + 48.412 - 405.684 = 236.649\\) Full-time living in the West: \\(wage = 593.921 + 62.544 - 405.684 = 250.781\\) 11.3.3 Interaction Effects Say we are back in the world of the regression model with education and parttime. When we fit this model before, we saw a constant difference in predicted wage for the employee’s full-time/part-time status. In particular, we saw predicted wage being lower by a constant amount for every level of education. However, what if part-time/full-time status has more of an impact on wage at low education levels than it does at high education levels? This is quite likely to be true if, once you have sufficient education, part-time status does not depress your wage as much as it does when you have low educational attainment. Situations such as these are referred to as interaction effects. In general, An interaction effect exists when the magnitude of the effect of one independent variable \\((x_1)\\) on a dependent variable \\((y)\\) varies as a function of a second independent variable \\((x_2)\\). Models such as these are written as follows: \\[wage = \\hat{\\alpha} + \\hat{\\beta_1}(education) + \\hat{\\beta_2}(parttime) + \\hat{\\beta_3}(education \\times parttime) + e\\] Now, we may either have a priori expectations of an interaction effect or just be looking to see if an interaction effect exists. Regardless, if we were to fit such a model to the data, we would see the following:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic std. Statistic p std. p (Intercept) -19.07 11.97 0.08 0.01 -42.53 – 4.39 0.07 – 0.09 -1.59 13.87 0.111 &lt;0.001 education 50.41 0.89 0.32 0.01 48.66 – 52.17 0.31 – 0.33 56.41 56.41 &lt;0.001 &lt;0.001 parttime [yes] 137.35 40.38 -0.89 0.02 58.21 – 216.49 -0.93 – -0.86 3.40 -46.73 0.001 &lt;0.001 education * parttime[yes] -41.52 3.04 -0.27 0.02 -47.47 – -35.57 -0.30 – -0.23 -13.68 -13.68 &lt;0.001 &lt;0.001 Observations 28155 R2 / R2 adjusted 0.161 / 0.161 Note the estimated regression line: \\[wage = - 19.0698 + 50.4144(education) + 137.3511(parttime) - 41.5221(education \\times parttime)\\] For a model such as this, we speak of two types of effects – (a) a main effect of an independent variable, and an (b) interaction effect for each variable. These effects are best understood visually, and then through a series of calculations that will underline what is going on in the model. FIGURE 11.16: Interaction between parttime and education You see two regression lines, one for part-time employees and the other for full-time employees, each drawn for specific values of education. At low education levels, full-time employees \\((parttime = no)\\) have a lower average wage than do part-time employees \\((parttime = yes)\\). At a certain education level there seems to be no difference at all, roughly around \\(education = 3\\). Once the employee has 4 or more years of education, however, the gap in wages increases in favor of full-time employees for each additional year of education. This widening gap is clearly visible in the steepening regression line for \\(parttime = no\\) at higher education levels. We could also calculate predicted wage when \\(education = 0\\), and then for \\(education = 3\\), and then for \\(education = 18\\) to see the gap in action. Let us do so now. For a part-time employee with \\(0\\) years of education, the predicted wage is \\[\\begin{array}{l} wage = - 19.0698 + 50.4144(education = 0) + 137.3511(parttime = 1) - 41.5221(0 \\times 1) \\\\ = - 19.0698 + 50.4144(0) + 137.3511(1) - 41.5221(0) \\\\ = - 19.0698 + 137.3511 \\\\ = 118.2813 \\end{array}\\] For a full-time employee with \\(0\\) years of education, the predicted wage is \\[\\begin{array}{l} wage = - 19.0698 + 50.4144(education = 0) + 137.3511(parttime = 0) - 41.5221(0 \\times 0) \\\\ = - 19.0698 \\end{array}\\] Repeating the preceding calculations for \\(education = 3\\) will yield predicted wages of \\(144.9582\\) for part-time employees and \\(132.1733\\) for full-time employees. When education = \\(4\\), full-time employees are predicted to have a wage of \\(182.5877\\) and part-time employees a wage of \\(153.8505\\). When education is at it’s maximum in-sample value of \\(18\\), predicted wages are \\(278.3428\\) for part-time employees and \\(888.3890\\) for full-time employees. Let us toss these into a table to ease the narrative. TABLE 11.5: Predicted wages from the interaction model Education Part-time Wage Full-time Wage Wage Differential 0 118.28 -19.07 137.35 3 144.96 132.17 12.78 4 153.85 182.59 -28.74 18 278.34 888.39 -610.05 Evidently, then, the impact of part-time versus full-time status is not constant across education levels but in fact varies by education level: Education and employment status interact to influence an employee’s wage. I mentioned this earlier but it bears repeating. First, either we expect an interaction between some variables because theory or past research tells us an interaction effect should exist, or if there is nothing to go by we ourselves might wish to test for an interaction. Second, whether an estimated coefficient is statistically significant or not has consequences for how we interpret the model and how we generate predicted values. For predicted values we use all estimated coefficients, even if some of them are not statistically significant. However, for interpretation we only focus on statistically significant estimates. The example below walks you through these conditions. The data we will work with is an extract from the Current Population Survey (1985). Several attributes are measured for individual; the details follow. wage = wage (US dollars per hour) educ = number of years of education race = a factor with levels NW (nonwhite) or W (white) sex = a factor with levels F M hispanic = a factor with levels Hisp NH south = a factor with levels NS S married = a factor with levels Married Single exper = number of years of work experience (inferred from age and educ) union = a factor with levels Not Union age = age in years sector = a factor with levels clerical const manag manuf other prof sales service Assume we know nothing about what influences wage but we suspect the wage is influenced by experience. Let us also say that we suspect women earn less than men. We do not know if there is an interaction between education and sex. In fact, our research team is split where one group thinks there is no interaction. The other groups feels strongly that an interaction might exist. So we fit two models, one to appease the no interaction group and the other to appease the interaction group. The results are shown below:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p (Intercept) 7.07 0.46 -0.23 0.06 6.17 – 7.98 -0.35 – -0.11 15.36 &lt;0.001 exper 0.04 0.02 0.10 0.04 0.01 – 0.08 0.02 – 0.19 2.43 0.015 sex [M] 2.20 0.44 0.43 0.08 1.34 – 3.05 0.26 – 0.59 5.03 &lt;0.001 Observations 534 R2 / R2 adjusted 0.053 / 0.049 These results are for the no interaction model. Note the essential message here: average wage increases with experience, and males earn, on average, more than women, for the same level of experience. So what is being inferred here is that the slope is the same for both men and women; they differ only in the sense of a constant wage differential. We then re-estimate the model to soothe the with interaction group. When we do so, we see the following results:   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic std. Statistic p std. p (Intercept) 7.86 0.57 -0.22 0.06 6.73 – 8.98 -0.35 – -0.10 13.69 -3.58 &lt;0.001 &lt;0.001 exper 0.00 0.03 0.00 0.06 -0.05 – 0.05 -0.12 – 0.12 0.05 0.05 0.964 0.964 sex [M] 0.76 0.76 0.43 0.08 -0.74 – 2.27 0.26 – 0.59 1.00 5.03 0.318 &lt;0.001 exper * sex [M] 0.08 0.04 0.19 0.08 0.01 – 0.15 0.03 – 0.36 2.27 2.27 0.023 0.023 Observations 534 R2 / R2 adjusted 0.062 / 0.057 Note what these results tell us. First, neither experience nor sex matter on their own; the p-values are way above \\(0.05\\) so on it’s own neither variable influences wage. However, the interaction is significant. What this is telling us is that while neither variable shapes wage on its own, the two interact to do so! The interaction is clearly visible in the plot below: FIGURE 11.17: No Interaction Model Note the differing slopes, with the regression line for Males being steeper than that for the Females. So which set of results do we believe? Well, you are in a bind. If you started out on the side of the no interaction group and then saw the results for the interaction group, you would second-guess yourself and perhaps admit that you were wrong; there is an interaction. That would be the appropriate response. What would be unacceptable is for you to brush off the results from the model with an interaction and instead stick with your no interaction model. But what if the interaction were not significant, as in the case below? Which model would be preferable then?   wage wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic p Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic std. Statistic p std. p (Intercept) -1.91 1.04 -0.22 0.06 -3.96 – 0.14 -0.34 – -0.11 -1.83 0.068 -3.27 1.62 -0.22 0.06 -6.45 – -0.09 -0.34 – -0.11 -2.02 -3.88 0.044 &lt;0.001 educ 0.75 0.08 0.38 0.04 0.60 – 0.90 0.31 – 0.46 9.78 &lt;0.001 0.86 0.12 0.44 0.06 0.62 – 1.10 0.31 – 0.56 7.00 7.00 &lt;0.001 &lt;0.001 sex [M] 2.12 0.40 0.41 0.08 1.33 – 2.92 0.26 – 0.57 5.27 &lt;0.001 4.37 2.09 0.41 0.08 0.27 – 8.47 0.26 – 0.57 2.10 5.27 0.037 &lt;0.001 educ * sex [M] -0.17 0.16 -0.09 0.08 -0.48 – 0.14 -0.24 – 0.07 -1.10 -1.10 0.273 0.273 Observations 534 534 R2 / R2 adjusted 0.188 / 0.185 0.190 / 0.186 Now, note that the interaction is not statistically significant; the p-value is \\(0.2727\\), way above \\(0.05\\). If the interaction is not statistically significant, then you should opt for the model without an interaction because it is the simpler model – the priceless value of the principle of Occam's Razor: “When you have two competing theories that make exactly the same predictions, the simpler one is the better.” In the present situation that would mean defaulting to the model without an interaction. What about interactions between two continuous variables? Those types of models require more effort, as shown below.   wage Predictors Estimates std. Error std. Beta standardized std. Error CI standardized CI Statistic std. Statistic p std. p (Intercept) -12.19 1.79 0.11 0.05 -15.71 – -8.68 0.00 – 0.21 -6.82 2.04 &lt;0.001 0.042 exper -0.61 0.10 -1.82 0.19 -0.82 – -0.41 -2.20 – -1.44 -5.84 -9.51 &lt;0.001 &lt;0.001 age 0.96 0.08 2.02 0.19 0.80 – 1.12 1.66 – 2.39 11.72 10.84 &lt;0.001 &lt;0.001 exper * age -0.00 0.00 -0.11 0.04 -0.01 – -0.00 -0.19 – -0.04 -2.94 -2.94 0.003 0.003 Observations 534 R2 / R2 adjusted 0.214 / 0.209 Note that both experience and age each influence wage, but they also interact for added influence. The only way to figure out what this model is telling us would be by way of predicted values, allowing one variable to change by a specific amount while the other variable is held fixed at a specific values. Common practice is to set the independent variable to be held constant to the mean, and then to 1 standard deviation above the mean and 1 standard deviation below the mean. Alternatively, some analysts will set it to the median value, and then to the first quartile \\((Q_1)\\) and then to the third quartile \\((Q_3)\\). Here is a simple table that lists these values for each independent variable. TABLE 11.6: Summary Statistics of the Two Independent Variables Variable Mean SD Median Q1 Q3 exper 17.82 12.38 15 8 26 age 36.83 11.73 35 28 44 Now, let us calculate and plot the predicted values holding exper fixed to each value and allowing age to vary. FIGURE 11.18: Holding exper at Median and Quartiles vs. Mean and unit Standard Deviations and then allowing exper to vary while setting age to specific values. FIGURE 11.19: Holding age at Median and Quartiles vs. Mean and unit Standard Deviations 11.4 Assumptions of Linear Regression The Classical Linear Regression Model (CLRM) is built upon the following assumptions, though some of these are assumed to be true prior to any data analysis being initiated. The regression model is linear in terms of \\(y\\) and \\(x\\) The values of \\(x\\) are fixed in repeated sampling Zero mean value of the residuals, i.e., \\(E(e_i | x_i) = 0\\) Homoscedasticity, i.e., the residuals have a constant variance for each \\(x_i\\), that is \\(var(e_i|x_i = \\sigma^2)\\) No autocorrelation or serial correlation between the disturbances, i.e., \\(cov(e_i e_j | x_i x_j = 0)\\) Zero covariance between \\(e_i\\) and \\(x_i\\) i.e., \\(E(e_i x_i = 0)\\) The number of observations exceeds the number of parameters to be estimated Variability in \\(x\\) values The regression model is correctly specified There is no perfect multicollinearity 11.4.1 Assumption 1: Linear Regression Model The regression model is assumedly linear in parameters. That is, a function is said to be linear in \\(x\\) if the value of the change in \\(y\\) does not depend upon the value of \\(x\\) when \\(x\\) changes by a unit. Thus if \\(y = 4x\\), \\(y\\) changes by \\(4\\) for every unit change in \\(x\\), regardless of whether \\(x\\) changes from \\(1\\) to \\(2\\) or from \\(19\\) to \\(20\\). But if \\(y=4x^2\\), then when \\(x=1\\), \\(y=4(1^1) = 4\\) and when \\(x\\) increases by a unit to \\(x=2\\), we have \\(y=4(2^2) = 16\\). When \\(x\\) increase by another unit, \\(y=4(3^2) = 36\\) \\(\\ldots\\) note that the value \\(y\\) assumes is now dependent upon what \\(x\\) was when it increased (or decreased) by \\(1\\). In sum, a function \\(y = \\beta(x)\\) is linear in parameters if the coefficient \\(\\beta\\) appears with a power of \\(1\\). In this sense, \\(y=\\beta(x^2)\\) is linear in parameters, as is \\(y=\\beta(\\sqrt{x})\\). In the regression framework, then, \\(y = \\alpha + \\beta_1x + \\beta_2x^2\\), \\(y=\\alpha + \\beta_1x + \\beta_2x^2 + \\beta_3x^3\\) and so on are all linear in parameters but not so regressions such as \\(y = \\alpha + \\beta_1^2x + \\beta_2x^2\\) or \\(y = \\alpha + \\beta_1^{1/3}x + \\beta_2x^2\\), etc. 11.4.2 Assumption 2: \\(x\\) values fixed in repeated sampling For every value of \\(x\\) in the population, we have a range of \\(y\\) values that are likely to be observed. We are assuming that if we hold \\(x\\) at a specific value and keep drawing repeated samples, we will end up with all possible values of \\(y\\) for the fixed \\(x\\) value. This will allow us to estimate \\(E(y)\\) given that \\(x\\) is a specific value. But if we cannot be at all confident that we know the conditional distribution of $y$ given $x$, then we cannot predict what \\(y\\) may be given \\(x\\). A technically accurate explanation is that in an experiment, the values of the independent variable would be fixed by the experimenter and repeated samples would be drawn with the independent variables fixed at the same values in each sample, resulting in the independent variable being uncorrelated with the residuals. Of course, we work with non-experimental data more often than not and hence we have to assume that the independent variables are fixed in repeated samples. 11.4.3 Assumption 3: Zero conditional mean value of Residuals Formally, this assumption implies that \\(E(e_i | x_i) = 0\\), i.e., that given a specific \\(x\\) value, the expected value of \\(e\\) is \\(0\\). Recall that \\(e\\) refers to the difference between actual and predicted \\(y\\). If you see the PRF in the plot, notice that the PRF passes through the middle of the \\(y\\) values for each \\(x\\) … or in other words, \\(E(y | x_i)\\). Look at the figure and table below and note that the regression line passes through the “middle” of the \\(y\\) values for each \\(x_i\\), and that the residuals sum to zero for each \\(x_i\\). FIGURE 11.20: Regressing Expenditure on Income TABLE 11.7: Regressing Expenditure on Income Income Actual Expenditure Predicted Expenditure Residuals 80 55 65 -10 80 60 65 -5 80 65 65 0 80 70 65 5 80 75 65 10 100 65 77 -12 100 70 77 -7 100 74 77 -3 100 80 77 3 100 85 77 8 100 88 77 11 120 79 89 -10 120 84 89 -5 120 90 89 1 120 94 89 5 120 98 89 9 140 80 101 -21 140 93 101 -8 140 95 101 -6 140 103 101 2 140 108 101 7 140 113 101 12 140 115 101 14 160 102 113 -11 160 107 113 -6 160 110 113 -3 160 116 113 3 160 118 113 5 160 125 113 12 180 110 125 -15 180 115 125 -10 180 120 125 -5 180 130 125 5 180 135 125 10 180 140 125 15 200 120 137 -17 200 136 137 -1 200 140 137 3 200 144 137 7 200 145 137 8 220 135 149 -14 220 137 149 -12 220 140 149 -9 220 152 149 3 220 157 149 8 220 160 149 11 220 162 149 13 240 137 161 -24 240 145 161 -16 240 155 161 -6 240 165 161 4 240 175 161 14 240 189 161 28 260 150 173 -23 260 152 173 -21 260 175 173 2 260 178 173 5 260 180 173 7 260 185 173 12 260 191 173 18 Look at the values of \\(y\\) when \\(x=80\\). The mean of \\(y\\) for \\(x-80\\) is \\(\\bar{y}=65\\). The gap between \\(y_i\\) and \\(\\bar{y}\\) is \\(-10, -5, 0, +5, +10\\) and when you add them, you get \\(\\sum (y_i - \\bar{y}) | x_i=80 = 0\\). Repeat this for \\(x=260\\) … Note that the errors are \\(-23, -21, 2, 5, 7, 12, 18\\) … and these add up to \\(0\\). So it must be that the regression line passes through \\(\\bar{y}\\) for each \\(x\\) value. 11.4.4 Assumption 4: Homoscedasticity Formally, homoscedasticity means that the conditional variance of the residuals \\((e_i)\\) is constant across \\(x_i\\). That is, no matter what \\(x\\) value we select, it should be that \\(y\\) varies the same. A violation would be evident if, say, at \\(x= 80\\) we see a lot of variance in \\(y\\) but at \\(x=160\\) we see minimal variance in \\(y\\). When might this happen? Imagine \\(x\\) is income and \\(y\\) is consumption expenditure. For low levels of income you are likely to see consumption varying narrowly. But as income rises, some people with the same income may consume quite a bit more or less than others with the same income. In such a case \\(var(e_i|x_i \\neq \\sigma^2)\\) but rather \\(var(e_i|x_i = \\sigma^2_i)\\). If homoscedasticity is violated we heteroscedasticity, and as a consequence the standard errors of the regression coefficients will be unreliable, which in turn means the significance tests and confidence intervals that use these standard errors will be unreliable as well. 11.4.5 Assumption 5: No Autocorrelation Autocorrelation (also referred to as serial correlation) occurs in time series data and with autocorrelated data \\(cov(e_i e_j | x_i x_j \\neq 0)\\). This is a problem because now \\(y\\) is also a function of \\(e\\), as shown below: FIGURE 11.21: Autocorrelation In a cross-sectional data-set, this assumption becomes one of saying no two observations \\(i\\) and \\(j\\) are correlated by design. I emphasize “by design” because in a genuine random sample, each unit has been selected into the sample independently of all other units and consequently this assumption should, in theory, be met. However, some units may end up being correlated for reasons unknown to us. 11.4.6 Assumptions 6, 7, and 8 Assumption 6 requires zero Covariance between \\(e\\) and \\(x\\). Formally, we need to have \\(E(e_i x_i = 0)\\) and this is important because it allows us to reliably estimate the impact of a unit change in \\(x\\) holding all else constant. If \\(E(e_i x_i \\neq 0)\\), then whenever \\(x\\) changes, so does \\(e\\). Think about this assumption as follows. In brief, the model we are fitting has been built by recognizing and measuring every independent variable known to influence \\(y\\). If we have forgotten to include some variables that do influence \\(y\\), these omitted variables will leak into the residuals (since the residuals represent all the variance of \\(y\\) left unexplained by the model). Assumption 7 requires that \\(n\\) exceed the number of parameters to be estimated. Assume \\(n=2\\), can you estimate \\(\\hat{a}\\) and \\(\\hat{b}\\)? No you cannot because you will run into the degrees of freedom problem. So we always want to have a sufficiently large sample, and the more complex the regression model, the bigger the sample needed because each parameter that is estimated will use up a degree of freedom. Assumption 8 requires variability in the \\(x\\) values. Imagine if in a sample, all \\(x_i = x\\). Now \\(\\bar{x}=x\\). What will happen to \\(SS_x\\)? It will be \\(0\\), and no estimates will be calculable. Thus we require that the variance of \\(x\\) be \\(&gt; 0\\). In other words, to calculate the effect of changes in \\(x\\) on \\(y\\), the sample values \\(x_i\\) of must vary across observations in any given sample. 11.4.7 Assumptions 9 and 10 Assumption 9 requires that the model is correctly specified. Modeling always begins with theory, and by drawing a good sample, measuring the right variables correctly, and so on. Assume the TRUE MODEL is \\(y = a + b\\left (\\dfrac{1}{x} \\right) + e\\) but we don’t know this and estimate \\(y = a + bx + e\\). Doing so will generate biased regression estimates! You can see this in the plot that follows, where for low values of \\(x\\) we end up under-predicting \\(y\\), then over-predicting \\(y\\) in the middle-range of \\(x\\) values, and again under-predicting \\(y\\) for high \\(x\\) values. FIGURE 11.22: Model Misspecification Of course, we rarely know the true model and hence it is all the more important that you have a sound theory to work with and the best data that you can gather to operationalize the theory. Assumption 10 requires that there be no perfect Multicollinearity. This assumption comes into play when we go beyond a single \\(x\\) variable to using multiple independent variables in a model such as \\(y = a + b_1x_1 + b_2x_2 + \\ldots + b_kx_k + e\\). In such an instance, multicollinearity implies that one or more of the \\(x\\) variables are highly correlated. This becomes a problem because you cannot calculate the impact of a unit increase in \\(x_i\\) by holding \\(x_j\\) constant if \\(x_i\\) and \\(x_j\\) are highly correlated! There will always be some correlation between pairs of variables; that is not an issue. What is an issue is severe multicollinearity because it can increase the variance of the coefficient estimates and make the estimates very sensitive to minor changes in the model. The result is unstable regression coefficients that may even switch signs, making it difficult to specify the correct model. A classic sign of severe multicollinearity is when a model generates a high \\(R^2\\) but very few of the independent variables are statistically significant. One way to minimize severe multicollinearity is to retain only one of a pair of highly correlated variables, combine them into some index via scaling techniques, or then try to increase the sample size since correlations tend to weaken in strength (not necessarily in statistical significance) with large samples. There are several tests one could carry out to check for violations of these assumptions, and should some violations be discovered, there exist a vast array of techniques to adjust the model as well. Covering these techniques is beyond the scope of this introductory text but is addressed in the next text in this series. In that text we also extend the regression modeling framework to situations where the dependent variable is limited in some ways as, for example, a binary variable \\((yes/no)\\) or a count variable that measures the number of trips patients make in a year to the emergency room, the number of highway fatalities, and so on. We also cover regression techniques for panel data, and some interesting applications of regression discontinuity designs, propensity scores, mixed models, small area estimation techniques, sample selection models, and ordinal and multinomial models. For now, recognize the fact that two things are your best friend: (1) A good, substantive grasp of the factors that influence the dependent variable, and (2) access to a large random sample with accurate measurement of the independent and dependent variables. Without these bases covered, there is little value in fitting any sort of a statistical model. 11.5 Chapter 11 Practice Problems Problem 1 Open up EPA’s Fuel Economy data. These data are the result of vehicle testing done at the Environmental Protection Agency’s National Vehicle and Fuel Emissions Laboratory in Ann Arbor, Michigan, and by vehicle manufacturers with oversight by EPA. Read the accompanying document available here to understand how some attribute is being measured. fuelType1 charge120 cylinders displ drive year ghgScore highway08 city08 model trany youSaveSpend The data span several years so make sure you filter the file to retain only 2018 records. Now answer the following questions: How are city miles per gallon (city08) correlated with highway miles per gallon (highway08)? Is the correlation significant? Graph the relationship between these two variables. As an intern at the EPA, your job is to determine if engine displacement (displ) has any impact on miles per gallon, and if it does, how much of an impact is it? Fit two regression models, one with city and the other with highway miles per gallon as the outcome of interest. Then, for each model, Report and interpret the slope, the \\(\\bar{R}^2\\) and the standard error of the prediction. Generate predicted values of miles per gallon both for the city and for the highway when displ = 1.5 versus when displ = 4. Based on the two regressions you estimated, does a unit increase in displ have a larger impact on city miles per gallon or on highway miles per gallon? Add the variable drive to both regression models. Now, Report and interpret the impact of this new variable on miles per gallon Evaluate whether the model fit has improved in terms of the \\(\\bar{R}^2\\) and the standard error of the prediction Does drive have a larger impact on miles per gallon in the city or on the highway? What drive axle type has the largest versus smallest impact on city versus highway? Problem 2 The next set of questions revolve around the 2016 Boston Marathon race results available here. The data-set contains the following variables: Bib = the runner’s bib number Name = the runner’s name Age = the runner’s age (in years) M/F = the runner’s gender (M = Male; F = Female) City = the runner’s home city State = the runner’s home state Country = the runner’s home country finishtime = the runner’s time (in seconds) to the finish line Can we predict a runner’s finishing time on the basis of his/her age and gender? In order to answer this question, Fit the appropriate regression model Interpret the partial slopes, the \\(\\bar{R}^2\\) and the standard error of the prediction Interpret the 95% confidence intervals around the partial slope of age Draw a scatter-plot with the regression line superimposed along with the 95% confidence intervals Based on your results, does it look like Age has a larger impact for a particular gender? Which one and what sort of an impact differential is there? (Hint: In order to answer this question you will need to suitably modify your regression model and review the results.) Problem 3 Download the 2017 County Health Rankings data SPSS format from here, Excel format from here and the accompanying codebook. You should see the following measures (measurename) Adult obesity Children in poverty High school graduation Preventable hospital stays Unemployment rate Test for a pairwise correlation between all of these variables and note which ones are significant. Identify the pairs of variables that seem to be most strongly versus most weakly correlated with each other. Problem 4 The data for this analysis come from the University of Michigan Panel Study of Income Dynamics (hereafter PSID) for the year 1975 (interview year 1976). Although this year was atypical of most of the 1970’s, only in 1976 did PSID directly interview the wives in the households. During all other years the head of the household’s interview supplied information about the wife’s labor market experiences during the previous year. One suspects that the own reporting is more accurate, and it is for this reason that many recent studies of married women’s labor supply have used these data. This sample consists of 753 married white women between the ages of 30 and 60 in 1975, with 428 working at some time during the year. This sample size is smaller than most used in the studies reported in Table I. The dependent variable is hours = the wife’s annual hours of work in 1975, and measured as the product of the number of weeks the wife worked for money in 1975 and the average number of hours of work per week during the weeks she worked. The measure of the wage rate is the average hourly earnings, defined by dividing the total labor income of the wife in 1975 by the above measure of her hours of work. inlf = labor force participation in 1975 = 1 hours = wife’s hours of work in 1975 kidslt6 = number of children less than 6 years old in household kidsge6 = number of children between ages 6 and 18 in household age = wife’s age educ = wife’s educational attainment, in years wage = wife’s average hourly earnings, in 1975 dollars repwage = wife’s wage reported at the time of the 1976 interview (not= 1975 estimated wage) hushrs = husband’s hours worked in 1975 husage = husband’s age huseduc = husband’s educational attainment, in years huswage = husband’s wage, in 1975 dollars faminc = family income, in 1975 dollars mtr = Federal marginal tax rate facing women mothereduc = wife’s mother’s educational attainment, in years fathereduc = wife’s father’s educational attainment, in years unemprate = unemployment rate in county of residence, in percentage points city = lives in large city (SMSA) = 1 exper = actual years of wife’s previous labor market experience nwifeinc = \\(faminc - (wage \\times hours)\\) in thousand dollars lwage = log(wage) expersq = \\(exper^2\\) The dependent variable of interest is wage, the wife’s average hourly earnings. Bearing this in mind, Fit a regression model with the wife’s educational attainment as the independent variable. Interpret the partial slopes, as appropriate, and the fit of the model. Now fit a regression model with the wife’s age, educational attainment, hours worked, and whether the household is in a large city or not. Interpret the partial slopes, as appropriate, and the fit of the model. Does adding the husband’s wage to the model improve the fit? Explain with reference to appropriate estimates from the revised model. Problem 5 “The Student/Teacher Achievement Ratio (STAR) was a four-year longitudinal class-size study funded by the Tennessee General Assembly and conducted by the State Department of Education. Over 7,000 students in 79 schools were randomly assigned into one of three interventions: small class (13 to 17 students per teacher), regular class (22 to 25 students per teacher), and regular-with-aide class (22 to 25 students with a full-time teacher’s aide). Classroom teachers were also randomly assigned to the classes they would teach. The interventions were initiated as the students entered school in kindergarten and continued through third grade. The Project STAR public access data set contains data on test scores, treatment groups, and student and teacher characteristics for the four years of the experiment, from academic year 1985–1986 to academic year 1988–1989. The test score data analyzed in this chapter are the sum of the scores on the math and reading portion of the Stanford Achievement Test.” Source These data can be downloaded from here. Note the key variables as defined below: gender = student’s gender ethnicity = student’s ethnicity stark, star1, star2, star3 = whether the student was in a small classroom, a regular classroom, or a regular classroom with a teaching aide readk, read1, read2, read3 = reading test scores in kindergarten/grade 1/grade 2/grade 3 mathk, math1, math2, math3 = mathematics test scores in kindergarten/grade 1/grade 2/grade 3 lunchk, lunch1, lunch2, lunch3 = student eligible for free/reduced lunch in kindergarten/grade 1/grade 2/grade 3 schoolk, school1, school2, school3 = school type student attended in kindergarten/grade 1/grade 2/grade 3 degreek, degree1, degree2, degree3 = teacher’s highest degree earned in kindergarten/grade 1/grade 2/grade 3 ladderk, ladder1, ladder2, ladder3 = teacher’s career ladder status in kindergarten/grade 1/grade 2/grade 3 experiencek, experience1, experience2, experience3 = teacher’s years of teaching experience in kindergarten/grade 1/grade 2/grade 3 tethnicityk, tethnicity1, tethnicity2, tethnicity3 = teacher’s ethnicity in kindergarten/grade 1/grade 2/grade 3 Fit a regression model to predict a student’s third-grade reading score on the basis of his/her kindergarten score. Interpret the \\(\\bar{R}^2\\) and the standard error of the prediction, as well as the slope. Fit a regression model to predict a student’s third-grade reading score on the basis of his/her kindergarten score, gender, ethnicity, and the type of classroom he/she was in during the third-grade. Interpret, where appropriate, partial slopes Interpret the \\(\\bar{R}^2\\) and the standard error of the prediction Does adding information on the third-grade teacher’s years of experience improve the model fit? Explain. What if you also added whether the student was eligible for free/reduced lunch? Does the fit improve? What, if anything, changes in the partial slopes? Problem 6 The Ohio Department of Education gathers and releases a wealth of data on public schools and community schools; see, for instance, here and here. A snippet of these data are provided for you in this SPSS format file. An Excel version of the same data are available here Note that you have data for community schools and public school districts; private and parochial schools are excluded. Check for correlations between the piscore (a measure used in the state’s accountability system with a higher score indicative of better performance), pctdisabled (percent of students attending the school who have a learning disability), pctecon (the percent of students who are deemed to be economically disadvantaged via measuring how many are eligible for free/reduced lunch), pctlep (the percent of students who have limited English proficiency), and pctminority (percent of students that are non-White). Report which correlations are/are not significant and the strength of these correlations as well. Construct scatter-plots for all correlations tested above in part (a) How well can you predict a school’s piscore? In order to answer this question, start by relying on the other variables listed in part (a) and folding them into the model as independent variables. Interpret all appropriate results, including the adjusted \\(R^2\\) and the standard error of the prediction. Does the prediction improve if you add school size (i.e., enrollment)? Note and report what changes and how. What about if you add information on whether a school is a Community school (i.e., a charter school) or the traditional Public school? Note and report what changes and how. All else being the same, does the model suggest that the average charter school performs better or worse than the average traditional public school? Explain. Problem 7 These data reflect 392 observations on the following 9 variables. mpg = miles per gallon cylinders = Number of cylinders between 4 and 8 displacement = Engine displacement (cu. inches) horsepower = Engine horsepower weight = Vehicle weight (lbs.) acceleration = Time to accelerate from 0 to 60 mph (sec.) year = Model year origin = Origin of car (1. American, 2. European, 3. Japanese) name = Vehicle name Note that the original data contained 408 observations but 16 observations with missing values have been removed. Regress mpg on weight and interpret the slope, adjusted R-squared, and the standard error of the estimate. Now estimate the following regression model: \\[mpg = \\alpha + \\beta_1(cylinders) + \\beta_2(acceleration) + \\beta_3(weight) + \\beta_4(origin) + \\epsilon\\] Be careful; some of the independent variables will have to be coded or even recoded given some problematic distributions and the fact that they are stored in the data-set as numeric but are in fact categorical variables. Interpret partial slopes, where appropriate, as well as the model fit. revise the previous model by adding an interaction between acceleration and manufacturing origin of the automobile. What estimates have changed and how? Has the fit improved or worsened? Interpret the interaction effects, if appropriate. Visually represent the interaction effects, holding the number of cylinders at their modal value and weight at its median. Use appropriate graphics to examine relationships between the variables you have in the regression model and see if anything seems to be amiss. For example, are distributions of weight, acceleration, mpg, and the number of cylinders roughly similar across manufacturing origin? Or do you see substantial differences? Problem 8 These data are simulated data containing sales of child car seats at 400 different stores. The variables include Sales = Unit sales (in thousands) at each location CompPrice = Price charged by competitor at each location Income = Community income level (in thousands of dollars) Advertising = Local advertising budget for company at each location (in thousands of dollars) Population = Population size in region (in thousands) Price = Price company charges for car seats at each site ShelveLoc = A factor with levels Bad, Good and Medium indicating the quality of the shelving loca- tion for the car seats at each site Age = Average age of the local population Education = Education level at each location Urban = A factor with levels No and Yes to indicate whether the store is in an urban or rural location US = A factor with levels No and Yes to indicate whether the store is in the US or not The outcome of interest is the number of car seats sold (Sales) and the task given to you, as the chief data analyst for one of the largest retailers in the country is to figure out what features seem to best predict sales. Demonstrate your ability by fitting appropriate regression models and presenting the model that appears to perform the best. Problem 9 These data reflect statistics for a large number of US Colleges from the 1995 issue of US News and World Report. The variables include Apps Number of applications received Private = A factor with levels No and Yes indicating private or public university Apps Number of applications received Accept = Number of applications accepted Enroll = Number of new students enrolled Top10perc = Pct. new students from top 10% of H.S. class Top25perc = Pct. new students from top 25% of H.S. class F.Undergrad = Number of fulltime undergraduates P.Undergrad = Number of parttime undergraduates Outstate = Out-of-state tuition Room.Board = Room and board costs Books = Estimated book costs Personal = Estimated personal spending PhD = Pct. of faculty with Ph.D.’s Terminal = Pct. of faculty with terminal degree S.F.Ratio = Student/faculty ratio perc.alumni = Pct. alumni who donate Expend = Instructional expenditure per student Grad.Rate = Graduation rate You work for the Admissions office at your undergraduate alma mater and believe that you can use these data to figure out how to increase the number of applications accepted (Accept) at your university/college. Start by visually exploring the data. Note down any instances where you see high correlations or strange distributions (for example, very skewed or even sparse), if any of these features vary by private versus public, and so on. Fit and interpret the results from a regression model with Apps as the dependent variable and two independent variables – Private and Accept. Now add Room.Board, Grad.Rate and perc.alumni. Reinterpret this updated model and evaluate if this model has improved our ability to predict Apps. Remove any variable(s) that you thought, based on your work in (a), could be problematic given the usual assumptions that underlie regression analysis. Reinterpret the resulting model and compare with the previous models you fit to the data. Is your model performing better? Briefly explain your conclusion. Problem 10 These data reflect a simulated data-set containing information on ten thousand customers. The aim here is to predict how much balance an individual is likely to carry on their credit card. The variables are: ID = Identification Income = Income in $10,000’s Limit = Credit limit Rating = Credit rating Cards = Number of credit cards Age = Age in years Education = Number of years of education Gender = A factor with levels Male and Female Student = A factor with levels No and Yes indicating whether the individual was a student Married = A factor with levels No and Yes indicating whether the individual was married Ethnicity = A factor with levels African American, Asian, and Caucasian indicating the individual’s ethnicity Balance = Average credit card balance in dollars Visually explore the variables in the data-set (excluding the ID variable) and briefly write-up what you see in the data. Note any distributions that are of special concern. Now fit a regression model with Balance as the dependent variable and the following independent variables: Rating, Income, Gender, Student, __Married_ and Ethnicity. Interpret your results. Now eliminate all independent variables that are not statistically significant at \\(\\alpha = 0.05\\) and look at your estimates. Has the model improved or worsened? In what ways, if any? Problem 11 These data reflect wage and other data for a group of 3000 male workers in the Mid-Atlantic region. year = Year that wage information was recorded age = Age of worker maritl = A factor with levels 1. Never Married 2. Married 3. Widowed 4. Divorced and 5. Separated indicating marital status race = A factor with levels 1. White 2. Black 3. Asian and 4. Other indicating race education = A factor with levels 1. &lt; HS Grad 2. HS Grad 3. Some College 4. College Grad and 5. Advanced Degree indicating education level region = Region of the country (mid-Atlantic only) jobclass = A factor with levels 1. Industrial and 2. Information indicating type of job health = A factor with levels 1. &lt;=Good and 2. &gt;=Very Good indicating health level of worker health_ins = A factor with levels 1. Yes and 2. No indicating whether worker has health insurance logwage = Log of workers wage wage = Workers raw wage Subset the data-set to the year 2003 and fit the following regression model: \\[wage = \\alpha + \\beta_1(education) + \\beta_2(jobclass) + \\beta_3(race) + \\beta_4(age) + \\epsilon\\] (b) Interpret your regression results. (c) Subset the data to 2009 and generate predicted values of wage with the 2003 regression estimates you generated in (a) (d) Generate an interaction between age and education and refit the model you estimated in (a) to the 2003 data. Does the model fit improve or worsen? What estimates change and how? Problem 12 These data are from the 1971 Census of Canada and reflect the following variables: education = Average education of occupational incumbents, years, in 1971 income = Average income of incumbents, dollars, in 1971 women = Percentage of incumbents who are women prestige = Pineo-Porter prestige score for occupation, from a social survey conducted in the mid-1960s census = Canadian Census occupational code type = Type of occupation. A factor with levels: bc, Blue Collar; prof, Professional, Managerial, and Technical; wc, White Collar The goal is to see if we can predict an occupation’s prestige on the basis of specific independent variables. Fit a simple regression model with a lone independent variable – type and interpret the estimates. Now add women to the model and see how the estimates change, if at all Now add income and education and interpret the new estimates Check to see if income and education are highly correlated, as should be expected. If they are, and the correlation is significant, exclude one of these from the model fit in (c) and see how the results change. Has there been any significant reduction in predictive ability or a substantial rise in our average prediction error? Problem 13 Here are Andrew Gelman’s data on the IQ scores of some children and, for each child, information about their mother: kid_score = child’s IQ score mom_hs = whether the mother has a high school degree (=1) or not (=0) mom_iq = mother’s IQ score mom_age = mother’s age, in years mom_work = mother’s work history, coded as 1 = did not work in first three years of child’s life, 2 = worked in 2nd or 3rd year of child’s life; 3 = worked part-time in first year of child’s life, and; 4 = worked full-time in first year of child’s life Fit a simple regression model of the child’s IQ score as a function of the mother’s IQ score and whether the mother has a high school degree (or not) Is there a significant interaction between the mother’s IQ and whether the mother has a high school degree (or not)? Test in a regression setting and state your conclusion. Can you improve the model by adding the mother’s work history? Be sure to test for possible interactions, as appropriate and defensible. Problem 14 These data come from the Institute for Democracy and Electoral Assistance (IDEA) and contain country-level information on voter turnout. Read all about the variables in this particular database here. I have added other variables, including Income Group that slots each country into the current World Bank classification system. The data-set thus includes the following variables: Country = country name Region = World region Income group = World Bank classification Year = Year turnout data refer to Compulsory voting = whether voting is compusive or not, codes as “Yes” or “No” turnout = Voter turnout (in percent) vapturnout = Voting age population turnout (in percent) popn = Population vappopn = Voting age population regdvoters = Number of registered voters civilliberties = Freedom House’s rating on civil liberties (low = bad; high = good) politicalrights = Freedom House’s rating on political rights (low = bad; high = good) Fit a regression model with turnout as the dependent variable and popn as the independent variable. Interpret the results, including the model fit. Now add whether voting is compulsory or not. How do the estimates change? How good is the model fit? What if you add civilliberties and politicalrights? How do the results and the model fit change? Add yet another variable, Income group. How do the results and the model fit change? Given all the models you have fit, which model has the best overall fit? Focus on \\(\\bar{R}^2\\) and on the average prediction error. "],["summation.html", "Chapter 12 The Summation Operator 12.1 The Single Summation Operator 12.2 The Double Subscript 12.3 The Constant Rule 12.4 The Distributive Rule 12.5 The Dot Notation 12.6 The Basic Rules of Summation", " Chapter 12 The Summation Operator The summation operator, \\(\\sigma\\), is an indispensable element of statistics because it is used in almost all formulas. Most students will encounter it first when they see the formula for the arithmetic mean. In this chapter I outline the basic rules we use to work with the summation operator. Make sure you work through them to understand when and how a rule might ease computation. 12.1 The Single Summation Operator \\[\\sum^{n}_{i=1}x_i\\] is how most single summation operators will be shown in a textbook. This is basically telling you to each value \\(i\\) of the variable \\(x\\). For example, if \\(x = 1, 2, 3, 5, 6\\), then \\[\\sum^{n}_{i=1}x_i = 1 + 2 + 3 + 5 + 6 = 17\\] For the same values of \\(x\\), if the expression is \\[\\sum^{n}_{i=1}x_i^{2}\\] we calculate \\[\\sum^{n}_{i=1}x_i^{2} = (1)^2 + (2)^2 + (3)^2 + (5)^2 + (6)^2 = 1 + 4 + 9 + 25 + 36 = 75\\] On the other hand, if the expression were \\[\\left(\\sum^{n}_{i=1}x_i\\right)^2\\] then the answer would be \\((75)^2 = 289\\). Obviously then, \\[\\sum^{n}_{i=1}x_i^{2} \\neq \\left(\\sum^{n}_{i=1} x_i\\right)^2\\] 12.2 The Double Subscript In some of the chapters you will encounter the double subscript \\(x_{ij}\\). For example, when we build contingency tables with one categorical variable as the rows and the other categorical variable as the columns, the table might look as follows: This might be represented as follows: The important thing to note is that one subscript \\((i)\\) indexes the row being referenced in the calculation while the other \\((j)\\) indexes the column being referenced in the calculation. 12.3 The Constant Rule This rule says that \\[\\sum^{n}_{i=1}ax_i = a\\sum^{n}_{i=1}x_i\\] For example, with \\(x = 1, 2, 3, 5, 6\\), \\[\\sum^{n}_{i=1}3x_i\\] would be \\[\\sum^{n}_{i=1}3x_i = 3 \\sum^{n}_{i=1}x_i = 3 (1 + 2 + 3 + 5 + 6) = 3(17) = 51\\] Note that this is the same thing as doing \\[3(1) + 3(2) + 3(3) + 3(5) + 3(6) = 3 + 6 + 9 + 15 + 18 = 51\\] but this would be the more laborious way of calculating the answer. Similarly, \\[\\sum^{n}_{i=1}\\dfrac{x_i}{2} = \\dfrac{1}{2}\\sum^{n}_{i=1}x_i\\] Say for the same values of \\(x\\), \\[\\sum^{n}_{i=1}\\dfrac{x_i}{2}\\] would be \\[\\dfrac{1}{2}\\sum^{n}_{i=1}x_i = \\dfrac{1}{2}17 = 8.5\\] The same answer would result if you calculated \\[\\sum^{n}_{i=1}\\dfrac{x_i}{2} = \\dfrac{1}{2} + \\dfrac{2}{2} + \\dfrac{3}{2} + \\dfrac{5}{2} + \\dfrac{6}{2} = 0.5 + 1 + 1.5 + 2.5 + 3 = 8.5\\] 12.4 The Distributive Rule \\[\\sum^{n}_{i=1}(x_i + y_i) = \\sum^{n}_{i=1}x_i + \\sum^{n}_{i=1}y_i\\]. Say \\(x = 1, 2, 3, 5, 6\\) and \\(y = 4, 8, 7, 9, 11\\), \\[\\sum^{n}_{i=1}(x_i + y_i) = \\sum^{n}_{i=1}x_i + \\sum^{n}_{i=1} y_i = (1 + 2 + 3 + 5 + 6) + (4 + 8 + 7 + 9 + 11) = (17) + (39) = 56\\] This is the same as \\[\\sum^{n}_{i=1}x_i + \\sum^{n}_{i=1}y_i = 17 + 39 = 56\\] Of course, \\[\\sum^{n}_{i=1}(x_i - y_i) = \\sum^{n}_{i=1}x_i - \\sum^{n}_{i=1}y_i\\] Try this for yourself with the given values of \\(x\\) and \\(y\\). 12.5 The Dot Notation Given the \\(x_{ij}\\) double subscripts, note that some times you may see the dot notation in action. For example, \\[x_{\\bullet j} = \\sum^{R}_{i=1}x_{ij}\\] and \\[x_{i \\bullet} = \\sum^{C}_{j=1}x_{ij}\\] What does this mean? Assume the data are as follows: What if I want the total of Column 1? I could represent this as \\[x_{\\bullet i} = \\sum^{n_j}_{i=1}x_{ij}\\] where \\(n_j\\) represents the total number of values we see in row \\(i\\). So for Column 1, this would be \\[x_{\\bullet 1} = \\sum^{n_1}_{i=1}x_{ij} = 1 + 4 + 6 = 11\\] What about Column 2? Here we have one blank cell but still, \\[x_{\\bullet 2} = \\sum^{n_2}_{i=1}x_{ij} = 2 + 7 = 9\\] Similarly \\[x_{\\bullet 3} = \\sum^{n_3}_{i=1}x_{ij} = 3 + 5 + 8 = 16\\] What about the row sums? \\[x_{i \\bullet} = \\sum^{C}_{j=1}x_{ij}\\] So if I want the sum of the third row I would have \\[x_{i \\bullet} = \\sum^{n_3}_{j=1}x_{ij} = 6 + 7 + 8 = 21\\] And so on for the sum of the second row and the sum of the first row, respectively. 12.6 The Basic Rules of Summation \\(\\sum x \\cdots\\) sum all the values of \\(x\\) \\(\\sum x^2 \\cdots\\) square each \\(x\\) and then sum the values of \\(x^2\\) \\(\\sum xy \\cdots\\) multiple each pair of \\(x\\) and \\(y\\) to obtain \\(xy\\) and then sum the values of \\(xy\\) \\(\\left(\\sum x\\right)^2 \\cdots\\) sum all the values of \\(x\\) and then square \\(\\sum x\\) \\(\\sum x \\sum y \\cdots\\) sum all the values of \\(x\\), sum all the values of \\(y\\), and then multiple the two sums \\(\\sum x\\) and \\(\\sum y\\). Note that \\(\\sum xy \\neq \\sum x \\sum y\\) \\(\\sum(x + 2) \\cdots\\) add 2 to each \\(x\\) and then sum \\(\\sum(x - y) \\cdots\\) subtract \\(y\\) from each \\(x\\) and then sum. This is the same as doing \\(\\sum x - \\sum y\\) \\(\\sum(x + y) \\cdots\\) add \\(y\\) to each \\(x\\) and then sum. This is the same as doing \\(\\sum x + \\sum y\\) \\(\\sum(2x) \\cdots\\) add each \\(x\\) and then multiply the sum by 2. Note that \\(\\sum(2x) = 2\\sum x\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
